{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Autonomier's Notebook!","text":"<p>\u8bb0\u5f55\u4e00\u4e9b\u81ea\u5df1\u7684\u5b66\u4e60\u7b14\u8bb0\u4e0e\u5fc3\u5f97</p> <p>Contact me with my ZJU email: 3220105831@zju.edu.cn.</p> <p> \u949f\u9f0e\u5c71\u6797\u90fd\u662f\u68a6, \u4eba\u95f4\u5ba0\u8fb1\u4f11\u60ca. \u53ea\u6d88\u95f2\u5904\u8fc7\u5e73\u751f.  \u9152\u676f\u79cb\u5438\u9732, \u8bd7\u53e5\u591c\u88c1\u51b0. \u8bb0\u53d6\u5c0f\u7a97\u98ce\u96e8\u591c, \u5bf9\u5e8a\u706f\u706b\u591a\u60c5. \u95ee\u8c01\u5343\u91cc\u4f34\u541b\u884c.  \u665a\u5c71\u7709\u6837\u7fe0, \u79cb\u6c34\u955c\u822c\u660e. </p> <p></p>"},{"location":"CS231n/","title":"CS231n: Deep Learning for Computer Vision","text":""},{"location":"CS231n/#stanford-spring-2024","title":"Stanford - Spring 2024","text":"<p>Courese Website</p> <p>\u7531\u4e8e\u53ea\u67092017\u5e74\u7684\u8bfe\u7a0b\u6709\u516c\u5f00\u89c6\u9891, \u6240\u4ee5\u53ea\u80fd\u5728\u5b66\u4e602017\u5e74\u8bfe\u7a0b\u7684\u57fa\u7840\u4e0a\u53c2\u80032024\u5e74\u7684notes\u548cslides\u6765\u5b66\u4e60, \u5b8c\u6210\u7684\u4f5c\u4e1a\u4e3a2024\u5e74\u7684\u7248\u672c.</p> <ul> <li>Python and Numpy Tutorial</li> <li>Pytorch Tutorial</li> <li>Transformer\u90e8\u5206\u542c\u4e86\u53f0\u6e7e\u5927\u5b66\u674e\u5b8f\u6bc5\u8001\u5e08\u7684\u673a\u5668\u5b66\u4e60(2021)\u8bfe\u7a0b.</li> </ul>"},{"location":"Transformer/","title":"Transformer","text":""},{"location":"Transformer/#self-attention","title":"Self-Attention","text":""},{"location":"Transformer/#input","title":"Input","text":"<p>\u8f93\u5165\u7684vector\u4e0d\u6b62\u4e00\u4e2a, \u800c\u4e14\u5411\u91cf\u4e2a\u6570\u4e5f\u4e0d\u786e\u5b9a, \u6240\u4ee5\u6211\u4eec\u9700\u8981\u5bf9\u5411\u91cf\u8fdb\u884c\u7f16\u7801.</p> <p></p> <p>One-hot encoding \u7f16\u7801\u5728vocabulary\u5e93\u5f88\u5927\u7684\u60c5\u51b5\u4e0b\u9700\u8981\u6781\u5927\u7684\u5b58\u50a8\u7a7a\u95f4, \u6027\u80fd\u8f83\u5dee, \u6240\u4ee5\u6211\u4eec\u4f7f\u7528word embedding\u8fdb\u884c\u7f16\u7801.</p> <p>\u5047\u8bbe\u6211\u4eec\u7684vocabulary\u603b\u5171\u6709\\(N\\)\u4e2aword, \u9700\u8981\u628a\u6bcf\u4e2aword\u7f16\u7801\u6210\\(M\\)\u7ef4\u7684\u5411\u91cf, \u90a3\u4e48\u6211\u4eec\u9700\u8981\u901a\u8fc7\u67d0\u4e9b\u673a\u5668\u5b66\u4e60\u7684\u624b\u6bb5\u5b66\u4e60\u4e00\u4e2a\\(W\\)\u77e9\u9635, \u6bcf\u4e00\u884c\u5bf9\u5e94\u4e00\u4e2aword\u7684\u7f16\u7801:</p> \\[ W = \\begin{pmatrix} w_{11} &amp; w_{12} &amp; \\cdots &amp; w_{1M} \\\\ w_{21} &amp; w_{22} &amp; \\cdots &amp; w_{2M} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ w_{N1} &amp; w_{N2} &amp; \\cdots &amp; w_{NM} \\end{pmatrix} \\] <p>\u6240\u4ee5\u5728\u4f7f\u7528\u6bcf\u4e2a\u8bcd\u7684\u65f6\u5019\u6211\u4eec\u53ea\u9700\u8981\u5728\u8fd9\u4e2a\u77e9\u9635\u4e2d\u627e\u5230\u8be5word\u5bf9\u5e94\u7684\u7f16\u7801\u5373\u53ef, \u4f7f\u7528\u8fd9\u4e2a\u7f16\u7801\u4f5c\u4e3aself-attention\u7684\u8f93\u5165.</p> <p>\u53e6\u5916, \u6211\u4eec\u4f7f\u7528PCA\u5c06word embedding\u4e4b\u540e\u7684\u5411\u91cf\u964d\u7ef4\u5230\u4e8c\u7ef4\u5e73\u9762\u4e0a, \u53d1\u73b0\u8bcd\u4e49\u76f8\u8fd1\u7684word\u9760\u7684\u66f4\u8fd1, \u6709\u4e00\u5b9a\u7684\u805a\u7c7b\u6548\u679c.</p>"},{"location":"Transformer/#output","title":"Output","text":"<p>\u5927\u81f4\u5c31\u662f\u4e09\u79cd\u53ef\u80fd\u6027, \u5206\u522b\u4e3a\u5bf9\u6bcf\u4e2a\u8f93\u5165\u90fd\u8f93\u51fa\u4e00\u4e2a\u6807\u7b7e, \u5bf9\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\u8f93\u51fa\u4e00\u4e2a\u6807\u7b7e, \u548c\u7531\u7f51\u7edc\u81ea\u884c\u51b3\u5b9a\u751f\u6210\u51e0\u4e2a\u6807\u7b7e.</p>"},{"location":"Transformer/#architecture","title":"Architecture","text":""},{"location":"Transformer/#overall-principle","title":"Overall Principle","text":"<p>\u4e0a\u9762\u8fd9\u4e2a\u7f51\u7edc\u5c55\u793a\u4e86\u53ea\u6709\u4e00\u5c42self-attention\u7684\u67b6\u6784, \u6211\u4eec\u53ef\u4ee5\u770b\u5230, self-attention\u5c42\u7efc\u5408\u8861\u91cf\u4e86\u56db\u4e2a\u8f93\u5165\u7684vector, \u7136\u540e\u4ea7\u751f\u4e86\u56db\u4e2a\u8f93\u51favector, \u6bcf\u4e2a\u8f93\u51fa\u7684vector\u90fd\u65e2\u5305\u542b\u4e86\u5f53\u524d\u4f4d\u7f6e\u7684\u8f93\u5165\u4fe1\u606f, \u53c8\u5305\u542b\u4e86\u6574\u4e2a\u5e8f\u5217\u7684\u4fe1\u606f. \u7136\u540e\u5bf9\u4e8eself-attention\u7684\u8f93\u51fa, \u6211\u4eec\u4f7f\u7528\u4e00\u4e2aFC layer\u6765\u9884\u6d4b\u7c7b\u522b.</p> <p></p> <ul> <li>\u53ef\u4ee5\u628a\u591a\u5c42self-attention\u53e0\u8d77\u6765, \u4e2d\u95f4\u52a0\u4e0aFC layers. \u6240\u4ee5\u4e00\u4e2aself-attention\u7684\u8f93\u5165\u4e0d\u4e00\u5b9a\u5c31\u662f\u539f\u59cb\u8f93\u5165, \u4e5f\u53ef\u80fd\u662fhidden layer\u4ea7\u751f\u7684\u8f93\u51fa.</li> <li>\u5728\u901a\u8fc7\\(a^i\\)\u8ba1\u7b97\\(b^i\\)\u7684\u65f6\u5019, \u5176\u5b9e\u6211\u4eec\u8003\u8651\u4e86\u5176\u4ed6\\(a^j(i\\neq j)\\)\u4e0e\\(a^i\\)\u4e4b\u95f4\u7684\u76f8\u5173\u6027, \u5177\u4f53\u5730\u8ba1\u7b97\u7ec6\u8282\u5728\u4e0b\u6587\u5c55\u5f00\u53d9\u8ff0.</li> </ul>"},{"location":"Transformer/#detailed-architecture","title":"Detailed Architecture","text":"<p>\u4e00\u822cself-attention layer\u7684\u7ec6\u8282\u67b6\u6784\u6709\u4e24\u79cd, \u4e00\u79cd\u662f\u5de6\u8fb9\u7684dot-product, \u4e00\u79cd\u662f\u53f3\u8fb9\u7684additive, \u4f46\u662f\u5de6\u4fa7\u7684dot-product\u66f4\u4e3a\u4e3b\u6d41, \u4e5f\u662f\u6211\u4eec\u4e3b\u8981\u8ba8\u8bba\u7684.</p> <p>\u5177\u4f53\u6765\u8bb2, \u8ba1\u7b97\u8fc7\u7a0b\u5206\u4e3a\u4e09\u6b65, \u6211\u4eec\u4ee5\u901a\u8fc7\\(a^1\\)\u8ba1\u7b97\\(b^1\\)\u4e3a\u4f8b:</p> <p></p> <p>\u6211\u4eec\u5b9e\u73b0\u901a\u8fc7\\(W^q\\)\u77e9\u9635\u8ba1\u7b97\u51fa\\(q^1\\), \u518d\u901a\u8fc7\\(W^k\\)\u77e9\u9635\u5206\u522b\u8ba1\u7b97\u51fa\\(a^2\\)\u5230\\(a^4\\), \u7136\u540e\u8ba1\u7b97\u51fa\u4ed6\u4eec\u4e4b\u95f4\u7684dot-product.</p> <p></p> <p>\u6ce8\u610f\u6211\u4eec\u4e5f\u9700\u8981\u8ba1\u7b97\\(q^1\\)\u548c\\(k^1\\)\u7684\u70b9\u79ef, \u5373\u81ea\u5df1\u7684query\u548c\u81ea\u5df1\u7684key\u4e4b\u95f4\u7684\u70b9\u79ef. \u8ba1\u7b97\u51fa\u8fd9\u4e9b\u70b9\u79ef\u4e4b\u540e, \u6211\u4eec\u5c06\u5176\u901a\u8fc7\u67d0\u4e2aactivation function, \u5982\u8fd9\u91cc\u7684softmax.</p> <p></p> <p>\u6211\u4eec\u8fd8\u9700\u8981\u901a\u8fc7\\(W^v\\)\u77e9\u9635\u8ba1\u7b97\u51fa\\(v^i\\)\u5411\u91cf, \u7136\u540eextract  information based on attention scores. \u5373\u4e0b\u5f0f:</p> \\[ b^i = \\sum_j \\alpha^{'}_{i, j} v^j \\] <p>\u8fd9\u6837\u6211\u4eec\u5c31\u5f97\u5230\u4e86self-attention\u7684\u8f93\u51fa.</p>"},{"location":"Transformer/#parallel","title":"Parallel","text":"<p>Self-Attention layer\u6bcf\u4e2a\u8f93\u51fa\u7684\u8ba1\u7b97\u90fd\u662f\u53ef\u4ee5\u5e76\u884c\u7684, \u6240\u4ee5\u6548\u7387\u8f83\u9ad8.</p> \\[ q^i = W^q a^i,\\ \\ (q^1, q^2, q^3, q^4) = W^q (a^1, a^2, a^3, a^4) \\] \\[ k^i = W^k a^i,\\ \\ (k^1, k^2, k^3, k^4) = W^k (a^1, a^2, a^3, a^4) \\] \\[ v^i = W^v a^i,\\ \\ (v^1, v^2, v^3, v^4) = W^v (a^1, a^2, a^3, a^4) \\] <p>where \\(a, q, k, v\\) are all column vactors.</p> <p></p> <p></p> \\[ (b^1, b^2, b^3, b^4) = (v^1, v^2, v^3, v^4) \\cdot A^{'} \\] <p></p> <p>Summary:</p> <p></p>"},{"location":"Transformer/#muti-head-self-attention","title":"Muti-head Self-Attention","text":"<p>In vanilla self-attention, we only considered one type of relevance, therefore there's only one \\(q^i, k^i, v^i\\). Now in the muti-head version, we need to consider different types of relevance. Take, 2-head for example.</p> <p></p>"},{"location":"Transformer/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>No position information in self-attention.</li> <li>Each position has unique positional vactor \\(e^i\\).</li> <li>hand-crafted.</li> <li>learned from data.</li> </ul> Some methods to perform positional encoding"},{"location":"Transformer/#application","title":"Application","text":""},{"location":"Transformer/#self-attention-for-speech","title":"Self-attention for Speech","text":"<p>\u56e0\u4e3aspeech\u7684\u4fe1\u606f\u6bcf10ms\u5c31\u4f1a\u751f\u6210\u4e00\u4e2avector, \u6240\u4ee5\u901a\u5e38\u8f93\u5165\u5411\u91cf\u90fd\u5f88\u591a, \u5982\u679c\u8ba1\u7b97\\(A^{'}\\)\u77e9\u9635\u7684\u8bdd\u5c31\u4f1a\u8fc7\u4e8e\u5e9e\u5927, \u6d88\u8017\u592a\u591a\u5185\u5b58, \u6240\u4ee5\u6211\u4eec\u4f7f\u7528Truncated Self-attention. \u5373\u9884\u6d4b\u6bcf\u4e2a\u8f93\u51fa\u7684\u65f6\u5019\u53ea\u770b\u5468\u56f4\u7684\u4e00\u90e8\u5206.</p>"},{"location":"Transformer/#self-attention-for-image","title":"Self-attention for Image","text":"<ul> <li>CNN: self-attention that can only attends in a receptive field.</li> <li>CNN is asimlified self-attention.</li> <li>Self-attention: CNN with learnable receptive field.</li> <li>Self-attention is the complex version of CNN.</li> </ul> <p>\u56e0\u4e3aCNN\u53ef\u4ee5\u770b\u505aself-attention\u7684\u4e00\u4e2a\u5b50\u96c6, \u6240\u4ee5self-attention\u7684\u53ef\u6269\u5c55\u6027\u548c\u89e3\u51b3\u95ee\u9898\u7684\u5e7f\u5ea6\u66f4\u597d, \u8fd9\u4e5f\u610f\u5473\u7740\u5bf9\u4e8eCNN\u64c5\u957f\u7684\u5de5\u4f5c(\u5982\u56fe\u50cf\u8bc6\u522b), CNN\u4f1a\u5728\u66f4\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d, \u800c\u5f53\u6570\u636e\u96c6\u975e\u5e38\u7684\u65f6\u5019, self-attention\u4f1a\u8868\u73b0\u66f4\u597d.</p>"},{"location":"Transformer/#self-attention-vs-rnn","title":"Self-attention v.s. RNN","text":"<p>RNN\u662f\u4e0d\u53ef\u5e76\u884c\u7684, \u6240\u4ee5self-attention\u57fa\u672c\u4e0a\u662f\u5b8c\u80dc\u7684.</p>"},{"location":"Transformer/#transformer_1","title":"Transformer","text":"<p>Transformer\u89e3\u51b3\u7684\u4e8bsequence-to-sequence(seq2seq)\u7684\u95ee\u9898.</p> <p></p>"},{"location":"Transformer/#encoder","title":"Encoder","text":"<p>\u4e00\u4e2aencoder\u7531\u591a\u4e2a\u76f8\u540c\u7684block\u7ec4\u6210, \u6bcf\u4e2ablock\u5185\u90e8\u5305\u542b\u4e86self-attention, residual\u548clayer normalization\u4e09\u79cd\u7ed3\u6784. \u5177\u4f53\u6765\u8bb2, \u8f93\u5165\u7684\u5411\u91cf\u5148\u7ecf\u8fc7self-attention\u4ea7\u751f\u8f93\u51fa, \u7136\u540e\u5c06\u8f93\u51fa\u901a\u8fc7\u6b8b\u5dee\u7ed3\u6784\u52a0\u4e0a\u8f93\u5165, \u6700\u540e\u505a\u4e00\u4e2alayer normalization,</p>"},{"location":"Transformer/#decoder","title":"Decoder","text":""},{"location":"Transformer/#autoregressiveat","title":"Autoregressive(AT)","text":"<p>Encoder\u7684\u8f93\u51fa\u901a\u8fc7\u67d0\u79cd\u65b9\u5f0f(\u540e\u9762\u4f1a\u8bb2)\u7ed9\u5230Decoder, \u7136\u540edecoder\u63a5\u53d7\u4e00\u4e2a\\(&lt;start&gt;\\) token\u5f00\u59cb\u8f93\u51fa\u9884\u6d4b\u7ed3\u679c, \u4e0b\u4e00\u6b65\u7684\u8f93\u5165\u8bbe\u7f6e\u4e3a\u4e0a\u4e00\u6b65\u9884\u6d4b\u7ed3\u679c\u4e2d\u6982\u7387\u6700\u9ad8\u7684\u7f16\u7801.</p> <p>Decoder\u4e2d\u7684attention module\u548cencoder\u4e0d\u592a\u4e00\u6837, \u591a\u4e86\u4e00\u4e2amask, \u79f0\u4e3aMasked Self-attention.</p> <p>\u73b0\u5728\u6bcf\u4e00\u4e2a\u8f93\u51fa\u90fd\u53ea\u8003\u8651\u5728\u8be5\u5f53\u524d\u8f93\u51fa\u4e4b\u524d\u7684\u5e8f\u5217\u4fe1\u606f\u4e0e\u5f53\u524d\u8f93\u5165\u7684relevance. \u8fd9\u5176\u5b9e\u975e\u5e38\u7b26\u5408\u6211\u4eec\u7684\u76f4\u89c9.</p> <p>\u6211\u4eec\u901a\u5e38\u8fd8\u4f1a\u6709\u4e2a\\(&lt;end\\) token, \u4f5c\u4e3astop token, \u5728decoder\u8f93\u51fa\u8fd9\u4e2atoken\u4e4b\u540e, \u6211\u4eec\u5c31\u7ed3\u675f\u751f\u6210.</p>"},{"location":"Transformer/#non-autoregressivenat","title":"Non-autoregressive(NAT)","text":"<ul> <li>How to decide the output length for NAT decoder?<ul> <li>Another predictor for output length.</li> <li>Output a very long sequence, ignore tokens after END.</li> </ul> </li> <li>Advantage: parallel, more stable generation.</li> <li>NAT is usually worse than AT.</li> </ul>"},{"location":"Transformer/#encoder-decoder","title":"Encoder - Decoder","text":"<p>Decoder\u4e2d\u6709\u4e2a\u90e8\u5206(Cross Attention)\u7528\u4e8e\u63a5\u53d7Encoder\u4f20\u6765\u7684\u4fe1\u606f.</p>"},{"location":"Transformer/#cross-attention","title":"Cross Attention","text":"<p>\u6211\u4eec\u5bf9masked self-Attention\u7684\u8f93\u51fa\u6c42\u51faquery, \u7136\u540e\u5bf9encoder\u7684\u8f93\u51favector\u8ba1\u7b97\u51fakey, \u518d\u5206\u522b\u8ba1\u7b97query\u4e0e\u6bcf\u4e2akey\u7684dot product, \u6700\u540e\u5408\u6210\\(v\\)\u5411\u91cf\u4f5c\u4e3aFC layers\u7684\u8f93\u5165.</p>"},{"location":"Transformer/#training","title":"Training","text":"<p>Teacher Forcing: using the ground truth as input.</p> <p></p> <p>\u4f46\u662f\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u6211\u4eec\u4f1a\u4f7f\u7528\u524d\u4e00\u6b65\u8f93\u51fa\u7684\u7ed3\u679c\u4f5c\u4e3a\u4e0b\u4e00\u6b65\u8f93\u51fa.</p> <p>\u4f46\u662f\u6d4b\u8bd5\u65f6\u53ef\u80fd\u4f1a\u51fa\u9519, \u600e\u4e48\u5c3d\u91cf\u907f\u514d\u524d\u4e00\u6b65\u9519\u5bfc\u81f4\u540e\u9762\u6b65\u6b65\u9519\u5462?</p>"},{"location":"Transformer/#scheduled-sampling","title":"Scheduled Sampling","text":"<p>Scheduled Sampling \u662f\u4e00\u79cd\u6280\u672f\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u5e8f\u5217\u751f\u6210\u6a21\u578b\uff08\u4f8b\u5982 RNNs \u548c Transformer\uff09\u65f6\u89e3\u51b3\u66b4\u9732\u504f\u5dee\uff08exposure bias\uff09\u7684\u95ee\u9898\u3002\u66b4\u9732\u504f\u5dee\u6307\u7684\u662f\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u603b\u662f\u63a5\u6536\u771f\u5b9e\u7684\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u800c\u5728\u6d4b\u8bd5\u65f6\u53ea\u80fd\u63a5\u6536\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u8fd9\u79cd\u5dee\u5f02\u4f1a\u5bfc\u81f4\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u4e0d\u4f73\u3002</p> <p>Scheduled Sampling \u901a\u8fc7\u9010\u6b65\u6539\u53d8\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7684\u8f93\u5165\u6570\u636e\u6765\u6e90\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002\u5177\u4f53\u505a\u6cd5\u5982\u4e0b\uff1a</p> <ol> <li> <p>\u521d\u59cb\u9636\u6bb5\uff1a\u5728\u8bad\u7ec3\u7684\u521d\u671f\uff0c\u6a21\u578b\u5b8c\u5168\u4f7f\u7528\u771f\u5b9e\u7684\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u5c31\u50cf\u4f20\u7edf\u7684\u6559\u5e08\u5f3a\u5236\uff08teacher forcing\uff09\u65b9\u6cd5\u4e00\u6837\u3002\u6559\u5e08\u5f3a\u5236\u6cd5\u662f\u6307\u6bcf\u4e00\u6b65\u751f\u6210\u65f6\uff0c\u6a21\u578b\u4f7f\u7528\u4e0a\u4e00\u6b65\u7684\u771f\u5b9e\u8f93\u51fa\u4f5c\u4e3a\u8f93\u5165\u3002</p> </li> <li> <p>\u9010\u6e10\u8fc7\u6e21\uff1a\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\uff0c\u6a21\u578b\u9010\u6e10\u589e\u52a0\u4f7f\u7528\u81ea\u8eab\u751f\u6210\u7684\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u7684\u6982\u7387\u3002\u8fd9\u662f\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u6982\u7387 \\( \\epsilon \\) \u6765\u5b9e\u73b0\u7684\u3002\u6bcf\u4e00\u6b65\u751f\u6210\u65f6\uff0c\u6a21\u578b\u6709\u6982\u7387 \\( 1 - \\epsilon \\) \u4f7f\u7528\u771f\u5b9e\u7684\u8f93\u5165\uff0c\u6709\u6982\u7387 \\( \\epsilon \\) \u4f7f\u7528\u6a21\u578b\u81ea\u5df1\u751f\u6210\u7684\u8f93\u5165\u3002\u8fd9\u4e2a\u6982\u7387 \\( \\epsilon \\) \u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\u9010\u6e10\u589e\u52a0\uff0c\u4ece\u800c\u4f7f\u5f97\u6a21\u578b\u9010\u6e10\u9002\u5e94\u5728\u6d4b\u8bd5\u65f6\u7684\u8f93\u5165\u60c5\u51b5\u3002</p> </li> <li> <p>\u6700\u7ec8\u9636\u6bb5\uff1a\u5728\u8bad\u7ec3\u7684\u540e\u671f\uff0c\u6a21\u578b\u5b8c\u5168\u4f7f\u7528\u81ea\u8eab\u751f\u6210\u7684\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u6a21\u62df\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u60c5\u51b5\u3002</p> </li> </ol> <p>Scheduled Sampling \u7684\u4f18\u70b9\u662f\u53ef\u4ee5\u4f7f\u6a21\u578b\u9010\u6b65\u9002\u5e94\u751f\u6210\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u81ea\u8eab\u8f93\u51fa\u4f5c\u4e3a\u8f93\u5165\u7684\u60c5\u51b5\uff0c\u4ece\u800c\u51cf\u5c11\u66b4\u9732\u504f\u5dee\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002</p>"},{"location":"assignment1/","title":"Assignment 1","text":"<p>Record some issues encountered during working on assignment 1.</p>"},{"location":"assignment1/#knn","title":"KNN","text":"<ul> <li>How to implement a fully vectorized code to calculate the distance matrix between the training set and the test set.</li> </ul> <p>\u5b9a\u4e49 <code>dists[i][j]</code> \u8868\u793a\u7b2c<code>i</code> \u4e2a\u6d4b\u8bd5\u6837\u672c\u548c\u7b2c <code>j</code> \u4e2a\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u7684\u8ddd\u79bb(\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb)\u3002\u4efb\u610f\u4e24\u5f20\u56fe\u7247\u7684\u8ddd\u79bb\u5b9a\u4e49\u4e3apixel-wise\u76f8\u51cf\u5e73\u65b9\u518d\u6c42\u548c, \u6700\u540e\u5f00\u6839\u53f7.</p> \\[ dist_{a, b} = \\sqrt{\\sum_{i \\in pixel} (a_i - b_i)^2} \\] vectorized implementation<pre><code>test_square = np.sum(X**2, axis = 1).reshape(num_test, 1)\ntrain_square = np.sum(self.X_train**2, axis = 1).reshape(1, num_train)\ninner_product = np.dot(X, self.X_train.T)\ndists = np.sqrt(test_square + train_square - 2*inner_product)\n</code></pre> <p>\u9996\u5148\u5c06distance\u7684\u8ba1\u7b97\u5f0f\u5c55\u5f00: $$ dist_{a, b} = \\sqrt{\\sum_{i \\in pixel} a_i^2 + \\sum_{i \\in pixel} b_i^2 - 2 \\sum_{i \\in pixel} a_i b_i} $$</p> <p>\u539f\u59cb\u77e9\u9635\u6bcf\u4e00\u884c\u4ee3\u8868\u4e00\u4e2a\u6837\u672c, \u6309\u884c\u6c42\u5e73\u65b9\u548c\u5373\u5f97\u5230\u4e86\u6bcf\u4e00\u5f20\u56fe\u7247\u50cf\u7d20\u7684\u5e73\u65b9\u548c, \u7136\u540e\u5c06<code>test_square</code>\u548c<code>train_square</code>\u7684shape\u4ece<code>(num_test, 1)</code>\u548c<code>(1, num_train)</code>\u53d8\u4e3a<code>(num_test, num_train)</code>\u4ee5\u4fbf\u4e8e\u76f8\u52a0. \u8fd9\u6837\u5904\u7406\u4e4b\u540e\u8fd9\u4e24\u4e2a\u6570\u7ec4\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u5bf9\u5e94\u4e86\u4e00\u5f20\u56fe\u7247\u7684\u50cf\u7d20\u5e73\u65b9\u548c.</p> <p><code>test_square + train_square</code>\u4f7f\u7528\u4e86python\u7684broadcasting\u673a\u5236, \u5c06\u4e24\u4e2ashape\u5206\u522b\u4e3a<code>(num_test, 1)</code>\u548c<code>(1, num_train)</code>\u7684\u77e9\u9635\u76f8\u52a0, \u5f97\u5230\u4e00\u4e2ashape\u4e3a<code>(num_test, num_train)</code>\u7684\u77e9\u9635, <code>Matrix[i][j]</code>\u5c31\u8868\u793a\u4e86\u7b2c<code>i</code>\u4e2a\u6d4b\u8bd5\u6837\u672c\u548c\u7b2c<code>j</code>\u4e2a\u8bad\u7ec3\u6837\u672c\u7684\u6240\u6709\u50cf\u7d20\u5e73\u65b9\u548c.</p> <p><code>inner_product = np.dot(X, self.X_train.T)</code>\u8ba1\u7b97\u4e86\u6240\u6709\u4ea4\u53c9\u9879, <code>inner_product[i][j]</code>\u8868\u793a\u4e86\u7b2c<code>i</code>\u4e2a\u6d4b\u8bd5\u6837\u672c\u548c\u7b2c<code>j</code>\u4e2a\u8bad\u7ec3\u6837\u672cpixel-wise\u7684\u50cf\u7d20\u4e58\u79ef\u4e4b\u548c.</p> <p>\u6700\u540e\u5c06<code>test_square + train_square - 2*inner_product</code>\u7684\u7ed3\u679c\u5f00\u6839\u53f7\u5373\u53ef\u5f97\u5230\u8ddd\u79bb\u77e9\u9635.</p>"},{"location":"assignment1/#svm","title":"SVM","text":"<ul> <li>Bias Trick</li> </ul> <p>Recall that we defined the score function as:</p> \\[ f(x_i, W, b) = W \\cdot x_i + b \\] <p>As we proceed through the material it is a little cumbersome to keep track of two sets of parameters (the biases b and weights W) separately. A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector xi with one additional dimension that always holds the constant 1 - a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply:</p> <p></p> <ul> <li>Image Data Preprocessing</li> </ul> <p>In Machine Learning, it is a very common practice to always perform normalization of your input features (in the case of images, every pixel is thought of as a feature). In particular, it is important to center your data by subtracting the mean from every feature. In the case of images, this corresponds to computing a mean image across the training images and subtracting it from every image to get images where the pixels range from approximately [-127 \u2026 127]. Further common preprocessing is to scale each input feature so that its values range from [-1, 1].</p> <p>\u9700\u8981\u5c06\u56fe\u7247\u50cf\u7d20\u503c\u5f52\u4e00\u5316\u5230[-1, 1]\u4e4b\u95f4, \u5373\u51cf\u53bb\u50cf\u7d20\u503c\u7684\u5747\u503c\u518d\u9664\u4ee5\u50cf\u7d20\u503c\u7684\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u7684\u5dee. \u8fd9\u6837\u5229\u4e8e\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\u4e0e\u6536\u655b\u901f\u5ea6.</p> Image Data Preprocessing<pre><code># Preprocessing: subtract the mean image\n# first: compute the image mean based on the training data\nmean_image = np.mean(X_train, axis=0)\nprint(mean_image[:10]) # print a few of the elements\nplt.figure(figsize=(4,4))\nplt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\nplt.show()\n\n# second: subtract the mean image from train and test data\nX_train -= mean_image\nX_val -= mean_image\nX_test -= mean_image\nX_dev -= mean_image\n\n# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n# only has to worry about optimizing a single weight matrix W.\nX_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\nX_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\nX_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\nX_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n</code></pre> <ul> <li>How to calculate the gradient of SVM loss?</li> </ul> <p>\u6211\u4eec\u8fd9\u91cc\u4e0d\u8ba8\u8bbaregularization\u90e8\u5206\u7684loss\u4e86, \u4e3b\u8981\u96be\u70b9\u5728\u4e0e\u524d\u9762\u7684data loss.</p> <p>Recall the process of calculating the SVM loss:</p> <ol> <li>\u8ba1\u7b97\\(XW^T\\), \u5f97\u5230\u5bf9\u6bcf\u4e2a\u6837\u672c\u5904\u4e8e\u5404\u4e2a\u7c7b\u522b\u7684\u5206\u6570\u4f30\u8ba1.</li> <li>\u5bf9\u4e8e\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u8ba1\u7b97\\(L_i  = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} + 1)\\).</li> <li>\u8ba1\u7b97\\(\\frac{1}{N}\\sum_i L_i\\), \u5f97\u5230\u603bloss.</li> </ol> <p>\u8bb0N\u8868\u793a\u6837\u672c\u6570\u91cf, D\u8868\u793a\u56fe\u7247\u5927\u5c0f(width*height*channel), C\u8868\u793a\u7c7b\u522b\u6570\u91cf, \\(X\\)\u77e9\u9635\u4e2d\u6bcf\u4e2a\u56fe\u7247\u6837\u672c\u5360\u636e\u4e00\u884c.</p> <p>\u6211\u4eec\u9996\u5148\u8003\u8651\u6700\u7ec8\u7684loss\u5173\u4e8e\\(XW^T\\)\u7684\u68af\u5ea6, \u5176\u5f62\u72b6\u5e94\u5f53\u4e3aN*C.</p> <p>2, 3\u4e24\u6b65, \u77e9\u9635\u7684\u5f62\u6001\u4e0d\u4f1a\u53d8\u5316, \u5927\u5c0f\u90fd\u4e3aN*C, \u6240\u4ee5\u6211\u4eec\u5bf9\u6bcf\u4e2a\u5143\u7d20\u8003\u8651\u5176\u68af\u5ea6.</p> \\[ \\begin{bmatrix} dW^{'}_{11} &amp; dW^{'}_{12} &amp; \\cdots &amp; dW^{'}_{1C} \\\\ dW^{'}_{21} &amp; dW^{'}_{22} &amp; \\cdots &amp; dW^{'}_{2C} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ dW^{'}_{N1} &amp; dW^{'}_{N2} &amp; \\cdots &amp; dW^{'}_{NC} \\end{bmatrix} \\] <p>\u9996\u5148\u521d\u59cb\u5316\u68af\u5ea6\\(dW^{'}\\)\u77e9\u9635\u4e3a\u5168\u96f6, \u5927\u5c0f\u4e3aN*C.</p> <p>\u7b2c\u4e00\u6b65\u8ba1\u7b97\u5b8c\u6bd5\u4e4b\u540e, \u6bcf\u4e00\u884c\u5bf9\u5e94\u4e00\u5f20\u56fe\u7247\u5728C\u4e2a\u7c7b\u4e0a\u7684\u5206\u6570, \u6240\u4ee5\u4e00\u4e2a\\(L_i\\)\u5bf9\u5e94\u7740\u77e9\u9635\u7684\u4e00\u884c. \u5728\u8ba1\u7b97\\(L_i\\)\u65f6, \u5bf9\u6bcf\u4e00\u9879\u8003\u8651, \u5982\u679c\\(s_j - s_{y_i} + 1 &gt; 0\\), \u5219\\(a_{ij}\\)\u5904\u68af\u5ea6+1, \\(a_{i, y_i}\\)\u5904\u68af\u5ea6-1, \u5426\u5219\u4e0d\u4fee\u6539\\(dW^{'}\\)\u77e9\u9635. \u6700\u540e\u9664\u4ee5N.</p> <p>\u6309\u7167\u4e0a\u8ff0\u6b65\u9aa4, \u5728\u8ba1\u7b97loss\u7684\u540c\u65f6, \u6211\u4eec\u5c31\u8ba1\u7b97\u51fa\u4e86loss\u5173\u4e8e\\(XW^T\\)\u7684\u68af\u5ea6\\(dW^{'}\\).</p> <p>\u4e0b\u9762\u8981\u8ba1\u7b97loss\u5173\u4e8eW\u7684\u68af\u5ea6\u77e9\u9635(dW\u5e94\u5f53\u4e3aW\u7684\u5f62\u72b6, \u5373D*C), \u53ea\u9700\u8981\u6c42:</p> \\[ dW = X^T \\cdot dW^{'} \\] SVM Gradient<pre><code>def svm_loss_vectorized(W, X, y, reg):\n    \"\"\"\n    Structured SVM loss function, vectorized implementation.\n\n    Inputs and outputs are the same as svm_loss_naive.\n    \"\"\"\n    loss = 0.0\n    dW = np.zeros(W.shape)  # initialize the gradient as zero\n\n\n    N = len(y)     # number of samples\n    Y_hat = X @ W  # raw scores matrix\n\n    y_hat_true = Y_hat[range(N), y][:, np.newaxis]    # scores for true labels\n    margins = np.maximum(0, Y_hat - y_hat_true + 1)   # margin for each score\n    loss = margins.sum() / N - 1 + reg * np.sum(W**2) # regularized loss\n\n    dW = (margins &gt; 0).astype(int)    # initial gradient with respect to Y_hat\n    dW[range(N), y] -= dW.sum(axis=1) # update gradient to include correct labels\n    dW = X.T @ dW / N + 2 * reg * W   # gradient with respect to W\n\n    return loss, dW\n</code></pre>"},{"location":"assignment1/#softmax","title":"Softmax","text":"<ul> <li>Practical issues: Numeric stability</li> </ul> <p>When you\u2019re writing code for computing the Softmax function in practice, the intermediate terms \\(e^{s_{y_i}}\\) and \\(\\sum_j e^{s_j}\\) may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant C and push it into the sum, we get the following (mathematically equivalent) expression:</p> \\[ \\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}} = \\frac{Ce^{s_{y_i}}}{C\\sum_j e^{s_j}} = \\frac{e^{s_{y_i} + \\log C}}{\\sum_j e^{s_j + \\log C}} = \\frac{e^{s_{y_i} - s_{max}}}{\\sum_j e^{s_j -s_{max}}} \\] <p>We are free to choose the value of C . This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for C is to set \\(logC=\u2212max_j s_j\\). This simply states that we should shift the values inside the vector of scores so that the highest value is zero.</p> <ul> <li>How to calculate the gradient of Softmax loss?</li> </ul> <p>\u540c\u6837\u4e0d\u8ba8\u8bbaregularization\u90e8\u5206\u7684loss\u4e86.</p> <p>Recall the process of calculating the Softmax loss:</p> <ol> <li>\u8ba1\u7b97\\(XW^T\\), \u5f97\u5230\u5bf9\u6bcf\u4e2a\u6837\u672c\u5904\u4e8e\u5404\u4e2a\u7c7b\u522b\u7684\u5206\u6570\u4f30\u8ba1.</li> <li>\u5bf9\u4e8e\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u8ba1\u7b97\\(L_i  = -log(\\frac{exp(s_{y_i})}{\\sum_j exp(s_j)}) = -exp(s_{y_i})+ log(\\sum_j exp(s_j))\\).</li> <li>\u8ba1\u7b97\\(\\frac{1}{N}\\sum_i L_i\\), \u5f97\u5230\u603bloss.</li> </ol> <p>\u540c\u6837, \u6211\u4eec\u5148\u8ba1\u7b97loss\u5173\u4e8e\\(XW^T\\)\u7684\u68af\u5ea6, \u5176\u5f62\u72b6\u5e94\u5f53\u4e3aN*C.</p> <p>2, 3\u4e24\u6b65, \u77e9\u9635\u7684\u5f62\u6001\u4e0d\u4f1a\u53d8\u5316, \u5927\u5c0f\u90fd\u4e3aN*C, \u6240\u4ee5\u6211\u4eec\u5bf9\u6bcf\u4e2a\u5143\u7d20\u8003\u8651\u5176\u68af\u5ea6.</p> \\[ \\begin{bmatrix} dW^{'}_{11} &amp; dW^{'}_{12} &amp; \\cdots &amp; dW^{'}_{1C} \\\\ dW^{'}_{21} &amp; dW^{'}_{22} &amp; \\cdots &amp; dW^{'}_{2C} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ dW^{'}_{N1} &amp; dW^{'}_{N2} &amp; \\cdots &amp; dW^{'}_{NC} \\end{bmatrix} \\] <p>\u9996\u5148\u521d\u59cb\u5316\u68af\u5ea6\\(dW^{'}\\)\u77e9\u9635\u4e3a\u5168\u96f6, \u5927\u5c0f\u4e3aN*C.</p> <p>\u7b2c\u4e00\u6b65\u8ba1\u7b97\u5b8c\u6bd5\u4e4b\u540e, \u6bcf\u4e00\u884c\u5bf9\u5e94\u4e00\u5f20\u56fe\u7247\u5728C\u4e2a\u7c7b\u4e0a\u7684\u5206\u6570, \u6240\u4ee5\u4e00\u4e2a\\(L_i\\)\u5bf9\u5e94\u7740\u77e9\u9635\u7684\u4e00\u884c. \u5728\u8ba1\u7b97\\(L_i\\)\u65f6, \u6211\u4eec\u6709:</p> \\[ L_i  = -exp(s_{y_i})+ log(\\sum_j exp(s_j)) \\] <p>\u8003\u8651\\(\\frac{\\partial L_i}{\\partial s_j}\\), \u5982\u679c\\(j = y_i\\), \u5219:</p> \\[ \\frac{\\partial L_i}{\\partial s_j} = -1 + \\frac{exp(s_j)}{\\sum_k exp(s_k)} \\] <p>\u5426\u5219:</p> \\[ \\frac{\\partial L_i}{\\partial s_j} = \\frac{exp(s_j)}{\\sum_k exp(s_k)} \\] <p>\u6240\u4ee5\\(dW^{'}_{i, y_i}\\)\u5904\u68af\u5ea6\u5e94\u5f53\u4e3a\\(-1 + \\frac{exp(s_j)}{\\sum_k exp(s_k)}\\), \u800c\u5176\u4ed6\u4f4d\u7f6e\u68af\u5ea6\u4e3a\\(\\frac{exp(s_j)}{\\sum_k exp(s_k)}\\). </p> <p>\u6309\u7167\u4e0a\u8ff0\u6b65\u9aa4, \u5728\u6c42\u51faloss\u4e4b\u540e\u6211\u4eec\u4e5f\u987a\u4fbf\u6c42\u5f97\u4e86loss\u5173\u4e8e\\(XW^T\\)\u7684\u68af\u5ea6\\(dW^{'}\\), \u4e0d\u8fc7\u8bb0\u5f97\u6700\u540e\u8981\u9664\u4ee5N.</p> <p>\u4e0b\u9762\u8981\u8ba1\u7b97loss\u5173\u4e8eW\u7684\u68af\u5ea6\u77e9\u9635(dW\u5e94\u5f53\u4e3aW\u7684\u5f62\u72b6, \u5373D*C), \u53ea\u9700\u8981\u6c42:</p> \\[ dW = X^T \\cdot dW^{'} \\] Softmax Gradient<pre><code>def softmax_loss_vectorized(W, X, y, reg):\n    \"\"\"\n    Softmax loss function, vectorized version.\n\n    Inputs and outputs are the same as softmax_loss_naive.\n    \"\"\"\n    # Initialize the loss and gradient to zero.\n    loss = 0.0\n    dW = np.zeros_like(W)\n\n    N = X.shape[0] # number of samples\n    Y_hat = X @ W  # raw scores matrix\n\n    P = np.exp(Y_hat - Y_hat.max())      # numerically stable exponents\n    P /= P.sum(axis=1, keepdims=True)    # row-wise probabilities (softmax)\n\n    loss = -np.log(P[range(N), y]).sum() # sum cross entropies as loss\n    loss = loss / N + reg * np.sum(W**2) # average loss and regularize \n\n    P[range(N), y] -= 1                  # update P for gradient\n    dW = X.T @ P / N + 2 * reg * W       # calculate gradient\n\n    return loss, dW\n</code></pre>"},{"location":"assignment1/#two-layer-net","title":"Two Layer Net","text":"<ul> <li>Relation among iteration, epoch and batch_size</li> </ul> <p>batch_size\u6307\u7684\u662f\u4e00\u6b21\u8bad\u7ec3\u4e2d\u7ecf\u8fc7\u6574\u4e2a\u7f51\u7edc\u7684\u6570\u636e\u91cf, \u4e00\u4e2aiteration\u6307\u7684\u5c31\u662f\u4f7f\u7528batch_size\u4e2a\u6570\u636e\u7ecf\u8fc7\u4e00\u6b21\u524d\u5411\u4f20\u64ad\u548c\u4e00\u6b21\u53cd\u5411\u4f20\u64ad, \u4e00\u4e2aepoch\u6307\u7684\u662f\u4f7f\u7528\u6574\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e00\u6b21\u8bad\u7ec3.</p> <p>\u6240\u4ee5\u6211\u4eec\u6709\u5982\u4e0b\u5173\u7cfb:</p> \\[ \\text{Iterations Per Epoch} = \\frac{\\text{Number of Training Samples}}{\\text{Batch Size}} \\] <ul> <li>Affine Layer</li> </ul> <p>\u4eff\u5c04\u5c42\uff0c\u901a\u5e38\u79f0\u4e3a\u4eff\u5c04\u53d8\u6362\u5c42\u6216\u5168\u8fde\u63a5\u5c42\uff08Fully Connected Layer\uff09\uff0c\u662f\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6784\u5efa\u5757\u3002\u5728\u6570\u5b66\u4e0a\uff0c\u4eff\u5c04\u53d8\u6362\u662f\u6307\u5728\u5411\u91cf\u7a7a\u95f4\u4e2d\uff0c\u8f93\u5165\u5411\u91cf\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u540e\uff0c\u518d\u52a0\u4e0a\u4e00\u4e2a\u504f\u7f6e\u5411\u91cf\u3002\u4eff\u5c04\u5c42\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5c06\u8f93\u5165\u5411\u91cf\u6620\u5c04\u5230\u8f93\u51fa\u5411\u91cf\u7a7a\u95f4\u3002</p> <p>\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u4eff\u5c04\u5c42\u7684\u5177\u4f53\u8ba1\u7b97\u8fc7\u7a0b\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> \\[ \\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b} \\]"},{"location":"assignment1/#features","title":"Features","text":"<ul> <li> <p>We can improve our classification performance by training linear classifiers or two-layer-net not on raw pixels but on features(HOG, histogram, etc) that are computed from the raw pixels.</p> </li> <li> <p>For each image we will compute a Histogram of Oriented Gradients (HOG) as well as a color histogram using the hue channel in HSV color space. We form our final feature vector for each image by concatenating the HOG and color histogram feature vectors.</p> </li> <li> <p>Roughly speaking, HOG should capture the texture of the image while ignoring color information, and the color histogram represents the color of the input image while ignoring texture. As a result, we expect that using both together ought to work better than using either alone. </p> </li> </ul> <p>Feature</p> <p>\u539f\u6765\u7684\u6a21\u578b\u7684X\u77e9\u9635\u6bcf\u4e00\u884c\u4e3a\u4e00\u5f20\u56fe\u7247\u7684\u50cf\u7d20\u503c, \u800c\u73b0\u5728\u6211\u4eec\u4f7f\u7528HOG\u548c\u989c\u8272\u76f4\u65b9\u56fe\u4f5c\u4e3a\u7279\u5f81, \u6240\u4ee5X\u77e9\u9635\u6bcf\u4e00\u884c\u5c31\u662f\u4e00\u5f20\u56fe\u7247\u7684HOG\u548c\u989c\u8272\u76f4\u65b9\u56fe\u7684\u7279\u5f81\u5411\u91cf.</p>"},{"location":"assignment2/","title":"Assignment 2","text":"<p>Record some interesting stuff encoutered during assignment 2.</p> <ul> <li> <p>After constructing a network, always check whether it can overfit a small amout of data well. If not, may need to consider there's some bugs in code.</p> </li> <li> <p>The weight scale tend to influence deeper network more, since the deeper a network becomes, the easier gradients vanishing/exploding occurs. \u5f53\u7136\u8fd9\u662f\u5728\u6ca1\u6709\u52a0batch normalization\u7684\u60c5\u51b5\u4e0b.</p> </li> <li> <p>\u5728\u4f7f\u7528dropout\u7684\u60c5\u51b5\u4e0b, \u5982\u679c\u6211\u4eec\u6ca1\u6709\u5728\u8bad\u7ec3\u65f6\u7ed9\u8f93\u51fa\u9664\u4ee5<code>p</code>, \u4f1a\u53d1\u751f\u4ec0\u4e48? <code>p</code>\u5b9a\u4e49\u4e3a\u4fdd\u7559\u4e00\u4e2aneuron\u7684\u6982\u7387.</p> <p>\u56e0\u4e3a\u5728test\u65f6\u6211\u4eec\u4e0d\u8003\u8651dropout, \u800c\u662f\u4f7f\u7528\u6240\u6709\u795e\u7ecf\u5143\u8fdb\u884c\u9884\u6d4b, \u6240\u4ee5\u8fd9\u6837\u7684\u505a\u6cd5\u4f1a\u5bfc\u81f4\u5728test\u9636\u6bb5\u6211\u4eec\u6bcf\u5c42\u5f97\u5230\u7684\u8f93\u51fa\u4e0d\u662f\u6b63\u786e\u7684\u9884\u6d4b\u7ed3\u679c.</p> <p>\u8bad\u7ec3\u65f6:</p> \\[ E[\\hat{y}] = p\\hat{y} + (1-p)0 = p\\hat{y} \\] <p>\u6d4b\u8bd5\u65f6:</p> \\[ E[\\hat{y}] = \\hat{y} \\] <p>\u6240\u4ee5\u8bad\u7ec3\u65f6\u548c\u6d4b\u8bd5\u65f6\u5f97\u5230\u7684\u8f93\u51fa\u4e0d\u4e00\u81f4, \u8fd9\u4f1a\u5bfc\u81f4\u7edf\u4e00\u6570\u636e\u5728\u8bad\u7ec3\u65f6\u5f97\u5230\u7684loss\u548ctest\u65f6\u5f97\u5230\u7684loss\u76f8\u5dee\u4e00\u4e2a\u500d\u6570, \u8fd9\u663e\u7136\u662f\u4e0d\u5408\u7406\u7684. \u6240\u4ee5\u6211\u4eec\u65e9\u8bad\u7ec3\u65f6\u7ed9\u7ed3\u679c\u9664\u4ee5p, \u7528\u4ee5\u62b5\u6d88\u8fd9\u4e00\u5dee\u5f02.</p> \\[ E[\\hat y]=\\frac{p}{p}\\hat y+\\frac{(1-p)}{p}0=\\hat y \\] </li> <li> <p>Spatial Batch Normalization</p> <p>Normally, batch-normalization accepts inputs of shape <code>(N, D)</code> and produces outputs of shape <code>(N, D)</code>, where we normalize across the minibatch dimension <code>N</code>. For data coming from convolutional layers, batch normalization needs to accept inputs of shape <code>(N, C, H, W)</code> and produce outputs of shape <code>(N, C, H, W)</code> where the <code>N</code> dimension gives the minibatch size and the <code>(H, W)</code> dimensions give the spatial size of the feature map.</p> <p>If the feature map was produced using convolutions, then we expect every feature channel's statistics e.g. mean, variance to be relatively consistent both between different images, and different locations within the same image -- after all, every feature channel is produced by the same convolutional filter! Therefore, spatial batch normalization computes a mean and variance for each of the <code>C</code> feature channels by computing statistics over the minibatch dimension <code>N</code> as well the spatial dimensions <code>H</code> and <code>W</code>.</p> <p>\u7b80\u5355\u6765\u8bb2, \u5982\u679c\u5f53\u524d\u6709<code>C</code>\u5c42layer, \u90a3\u4e48\u6211\u4eec\u5bf9\u6574\u4e2abatch\u7684\u6bcf\u4e00\u5c42layer\u7684\u6240\u6709\u50cf\u7d20\u8fdb\u884c\u4e00\u6b21normalize, \u603b\u8fc7\u8fdb\u884c<code>C</code>\u6b21.</p> <pre><code>N, C, H, W = x.shape\n# move the C channel to the last axis, and group the same layer over the entire batch together.\nx = np.moveaxis(x, 1, -1).reshape(-1, C) \n# use the vinilla batch normalization to deal with the x after reshaping.\nout, cache = batchnorm_forward(x, gamma, beta, bn_param) \nout = np.moveaxis(out.reshape(N, H, W, C), -1, 1)\n</code></pre> </li> <li> <p>Batch Norm, Layer Norm and Group Norm</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230, batch normalization\u662f\u9488\u5bf9\u540c\u4e00\u50cf\u7d20\u4f4d\u7f6e\u5728\u6574\u4e2abatch\u4e0anormalize, \u800clayer normalization\u662f\u9488\u5bf9\u540c\u4e00\u6837\u672c\u56fe\u50cf\u81ea\u6211\u8fdb\u884cnormalize. \u8fd9\u4e8c\u8005\u90fd\u80fd\u6709\u6548\u4f7f\u5f97\u6bcf\u4e2alayer\u7684\u8f93\u5165\u6570\u636ezero-centered, \u5e76\u4e14\u5206\u5e03\u8f83\u4e3a\u4e00\u81f4. Group normalization\u5219\u662f\u4fee\u6539\u4e86layer normalization, \u4e0d\u662f\u5bf9\u6574\u4e2a\u4e00\u4e2a\u6837\u672c\u8fdb\u884c\u64cd\u4f5c, \u800c\u662f\u5c06\u4e00\u4e2a\u6837\u672c\u4e5f\u5206\u4e3a\u82e5\u5e72\u7ec4, \u5728\u6bcf\u4e2a\u7ec4\u6bcf\u6b65normalize.</p> <p>\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d, group normalization. \u5373\u5c31\u662f\u5728layer normalization\u7684\u57fa\u7840\u4e0a\u518d\u628a\u6bcf\u4e00\u5c42\u5206\u6210G\u7ec4.</p> <pre><code>N, C, H, W = x.shape\nln_param = {\"shape\":(W, H, C, N), \"axis\":(0, 1, 3), **gn_param}\n\n# N group for layer normalization, N*G group for spatial group normalization.\nx = x.reshape(N*G, -1)\ngamma = np.tile(gamma, (N, 1, H, W)).reshape(N*G, -1)\nbeta = np.tile(beta, (N, 1, H, W)).reshape(N*G, -1)\n\n# then we just use the vinilla layer normalization.\nout, cache = layernorm_forward(x, gamma, beta, ln_param)\nout = out.reshape(N, C, H, W)\ncache = (G, cache)\n</code></pre> </li> <li> <p>Pytorch</p> <p>There are loads of stuff, we should consult:</p> <ol> <li>Pytorch Tutorial.</li> <li>API doc.</li> <li>Pytorch Forum.</li> </ol> </li> </ul>"},{"location":"assignment3/","title":"Assignment 3","text":""},{"location":"assignment3/#generative-adversarial-networks-vinilla-gans","title":"Generative Adversarial Networks (Vinilla GANs)","text":"<p>So far in CS 231N, all the applications of neural networks that we have explored have been discriminative models that take an input and are trained to produce a labeled output. This has ranged from straightforward classification of image categories to sentence generation (which was still phrased as a classification problem, our labels were in vocabulary space and we\u2019d learned a recurrence to capture multi-word labels). In this notebook, we will expand our repetoire, and build generative models using neural networks. Specifically, we will learn how to build models which generate novel images that resemble a set of training images.</p>"},{"location":"assignment3/#what-is-a-gan","title":"What is a GAN?","text":"<p>In 2014, Goodfellow et al. presented a method for training generative models called Generative Adversarial Networks (GANs for short). In a GAN, we build two different neural networks. Our first network is a traditional classification network, called the discriminator. We will train the discriminator to take images and classify them as being real (belonging to the training set) or fake (not present in the training set). Our other network, called the generator, will take random noise as input and transform it using a neural network to produce images. The goal of the generator is to fool the discriminator into thinking the images it produced are real.</p> <p>We can think of this back and forth process of the generator (\\(G\\)) trying to fool the discriminator (\\(D\\)) and the discriminator trying to correctly classify real vs. fake as a minimax game:</p> \\[ \\underset{G}{\\text{minimize}}\\; \\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right] \\] <p>where \\(z \\sim p(z)\\) are the random noise samples, \\(G(z)\\) are the generated images using the neural network generator \\(G\\), and \\(D\\) is the output of the discriminator, specifying the probability of an input being real. In Goodfellow et al., they analyze this minimax game and show how it relates to minimizing the Jensen-Shannon divergence between the training data distribution and the generated samples from \\(G\\).</p> <p>To optimize this minimax game, we will aternate between taking gradient descent steps on the objective for \\(G\\) and gradient ascent steps on the objective for \\(D\\): 1. update the generator (\\(G\\)) to minimize the probability of the discriminator making the correct choice.  2. update the discriminator (\\(D\\)) to maximize the probability of the discriminator making the correct choice.</p> <p>While these updates are useful for analysis, they do not perform well in practice. Instead, we will use a different objective when we update the generator: maximize the probability of the discriminator making the incorrect choice. This small change helps to allevaiate problems with the generator gradient vanishing when the discriminator is confident. This is the standard update used in most GAN papers and was used in the original paper from Goodfellow et al.. </p> <p>In this assignment, we will alternate the following updates:</p> <ul> <li>Update the generator (\\(G\\)) to maximize the probability of the discriminator making the incorrect choice on generated data:</li> </ul> \\[ \\underset{G}{\\text{maximize}}\\;  \\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right] \\] <ul> <li>Update the discriminator (\\(D\\)), to maximize the probability of the discriminator making the correct choice on real and generated data:</li> </ul> \\[ \\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right] \\]"},{"location":"assignment3/#discriminator","title":"Discriminator","text":"<p>Our first step is to build a discriminator. Fill in the architecture as part of the <code>nn.Sequential</code> constructor in the function below. All fully connected layers should include bias terms. The architecture is:</p> <ul> <li>Fully connected layer with input size 784 and output size 256</li> <li>LeakyReLU with alpha 0.01</li> <li>Fully connected layer with input_size 256 and output size 256</li> <li>LeakyReLU with alpha 0.01</li> <li>Fully connected layer with input size 256 and output size 1</li> </ul> <p>Recall that the Leaky ReLU nonlinearity computes \\(f(x) = \\max(\\alpha x, x)\\) for some fixed constant \\(\\alpha\\); for the LeakyReLU nonlinearities in the architecture above we set \\(\\alpha=0.01\\).</p> <p>The output of the discriminator should have shape <code>[batch_size, 1]</code>, and contain real numbers corresponding to the scores that each of the <code>batch_size</code> inputs is a real image.</p>"},{"location":"assignment3/#generator","title":"Generator","text":"<p>Now to build the generator network:</p> <ul> <li>Fully connected layer from noise_dim to 1024</li> <li><code>ReLU</code></li> <li>Fully connected layer with size 1024 </li> <li><code>ReLU</code></li> <li>Fully connected layer with size 784</li> <li><code>TanH</code> (to clip the image to be in the range of [-1,1])</li> </ul>"},{"location":"assignment3/#least-squares-gan","title":"Least Squares GAN","text":"<p>We'll now look at Least Squares GAN, a newer, more stable alernative to the original GAN loss function. For this part, all we have to do is change the loss function and retrain the model. We'll implement equation (9) in the paper, with the generator loss:</p> \\[ \\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right] \\] <p>and the discriminator loss:</p> \\[  \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right] \\] <p>HINTS: Instead of computing the expectation, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing. When plugging in for \\(D(x)\\) and \\(D(G(z))\\) use the direct output from the discriminator (<code>scores_real</code> and <code>scores_fake</code>).</p>"},{"location":"assignment3/#deeply-convolutional-gans","title":"Deeply Convolutional GANs","text":"<p>In the first part of the notebook, we implemented an almost direct copy of the original GAN network from Ian Goodfellow. However, this network architecture allows no real spatial reasoning. It is unable to reason about things like \"sharp edges\" in general because it lacks any convolutional layers. Thus, in this section, we will implement some of the ideas from DCGAN, where we use convolutional networks </p>"},{"location":"assignment3/#discriminator_1","title":"Discriminator","text":"<p>We will use a discriminator inspired by the TensorFlow MNIST classification tutorial, which is able to get above 99% accuracy on the MNIST dataset fairly quickly. </p> <ul> <li>Reshape into image tensor (Use Unflatten!)</li> <li>Conv2D: 32 Filters, 5x5, Stride 1</li> <li>Leaky ReLU(alpha=0.01)</li> <li>Max Pool 2x2, Stride 2</li> <li>Conv2D: 64 Filters, 5x5, Stride 1</li> <li>Leaky ReLU(alpha=0.01)</li> <li>Max Pool 2x2, Stride 2</li> <li>Flatten</li> <li>Fully Connected with output size 4 x 4 x 64</li> <li>Leaky ReLU(alpha=0.01)</li> <li>Fully Connected with output size 1</li> </ul>"},{"location":"assignment3/#generator_1","title":"Generator","text":"<p>For the generator, we will copy the architecture exactly from the InfoGAN paper. See Appendix C.1 MNIST. See the documentation for tf.nn.conv2d_transpose. We are always \"training\" in GAN mode. </p> <ul> <li>Fully connected with output size 1024</li> <li><code>ReLU</code></li> <li>BatchNorm</li> <li>Fully connected with output size 7 x 7 x 128 </li> <li>ReLU</li> <li>BatchNorm</li> <li>Reshape into Image Tensor of shape 7, 7, 128</li> <li>Conv2D^T (Transpose): 64 filters of 4x4, stride 2, 'same' padding (use <code>padding=1</code>)</li> <li><code>ReLU</code></li> <li>BatchNorm</li> <li>Conv2D^T (Transpose): 1 filter of 4x4, stride 2, 'same' padding (use <code>padding=1</code>)</li> <li><code>TanH</code></li> <li>Should have a 28x28x1 image, reshape back into 784 vector</li> </ul>"},{"location":"assignment3/#inline-question-6","title":"Inline Question 6","text":"<p>If the generator loss decreases during training while the discriminator loss stays at a constant high value from the start, is this a good sign? Why or why not? A qualitative answer is sufficient.</p>"},{"location":"assignment3/#your-answer","title":"Your answer:","text":"<p>No, a high loss for the discriminator means it is easy to \"fool\" it. The generator loss decreases because the classifier \"thinks\" it generates good samples, however the discriminator is actually unable to classify well and therefore either needs some parameter tuning or network restructuring.</p>"},{"location":"assignment3/#self-supervised-learning","title":"Self-Supervised Learning","text":""},{"location":"assignment3/#what-is-self-supervised-learning","title":"What is self-supervised learning?","text":"<p>Modern day machine learning requires lots of labeled data. But often times it's challenging and/or expensive to obtain large amounts of human-labeled data. Is there a way we could ask machines to automatically learn a model which can generate good visual representations without a labeled dataset? Yes, enter self-supervised learning! </p> <p>Self-supervised learning (SSL) allows models to automatically learn a \"good\" representation space using the data in a given dataset without the need for their labels. Specifically, if our dataset were a bunch of images, then self-supervised learning allows a model to learn and generate a \"good\" representation vector for images. </p> <p>The reason SSL methods have seen a surge in popularity is because the learnt model continues to perform well on other datasets as well i.e. new datasets on which the model was not trained on!</p>"},{"location":"assignment3/#what-makes-a-good-representation","title":"What makes a \"good\" representation?","text":"<p>A \"good\" representation vector needs to capture the important features of the image as it relates to the rest of the dataset. This means that images in the dataset representing semantically similar entities should have similar representation vectors, and different  images in the dataset should have different representation vectors. For example, two images of an apple should have similar representation vectors, while an image of an apple and an image of a banana should have different representation vectors.</p>"},{"location":"assignment3/#contrastive-learning-simclr","title":"Contrastive Learning: SimCLR","text":"<p>Recently, SimCLR introduces a new architecture which uses contrastive learning to learn good visual representations. Contrastive learning aims to learn similar representations for similar images and different representations for different images. As we will see in this notebook, this simple idea allows us to train a surprisingly good model without using any labels.</p> <p>Specifically, for each image in the dataset, SimCLR generates two differently augmented views of that image, called a positive pair. Then, the model is encouraged to generate similar representation vectors for this pair of images. See below for an illustration of the architecture (Figure 2 from the paper).</p> <p></p> <p>Given an image x, SimCLR uses two different data augmentation schemes t and t' to generate the positive pair of images \\(\\hat{x}_i\\) and \\(\\hat{x}_j\\). \\(f\\) is a basic encoder net that  extracts representation vectors from the augmented data samples, which yields \\(h_i\\) and \\(h_j\\), respectively. Finally, a small neural network projection head \\(g\\) maps the representation vectors to the space where the contrastive loss is applied. The goal of the contrastive loss is to maximize agreement between the final vectors \\(z_i = g(h_i)\\) and \\(z_j = g(h_j)\\). We will discuss the contrastive loss in more detail later, and you will get to implement it.</p> <p>After training is completed, we throw away the projection head \\(g\\) and only use \\(f\\) and the representation \\(h\\) to perform downstream tasks, such as classification. You will get a chance to finetune a layer on top of a trained SimCLR model for a classification task and compare its performance with a baseline model (without self-supervised learning).</p>"},{"location":"assignment3/#simclr-contrastive-loss","title":"SimCLR: Contrastive Loss","text":"<p>A mini-batch of \\(N\\) training images yields a total of \\(2N\\) data-augmented examples. For each positive pair \\((i, j)\\) of augmented examples, the contrastive loss function aims to maximize the agreement of vectors \\(z_i\\) and \\(z_j\\). Specifically, the loss is the normalized temperature-scaled cross entropy loss and aims to maximize the agreement of \\(z_i\\) and \\(z_j\\) relative to all other augmented examples in the batch:</p> \\[ l \\; (i, j) = -\\log \\frac{\\exp (\\;\\text{sim}(z_i, z_j)\\; / \\;\\tau) }{\\sum_{k=1}^{2N} \\mathbb{1}_{k \\neq i} \\exp (\\;\\text{sim} (z_i, z_k) \\;/ \\;\\tau) } \\] <p>where \\(\\mathbb{1} \\in \\{0, 1\\}\\) is an indicator function that outputs \\(1\\) if \\(k\\neq i\\) and \\(0\\) otherwise. \\(\\tau\\) is a temperature parameter that determines how fast the exponentials increase.</p> <p>sim\\((z_i, z_j) = \\frac{z_i \\cdot z_j}{|| z_i || || z_j ||}\\) is the (normalized) dot product between vectors \\(z_i\\) and \\(z_j\\). The higher the similarity between \\(z_i\\) and \\(z_j\\), the larger the dot product is, and the larger the numerator becomes. The denominator normalizes the value by summing across \\(z_i\\) and all other augmented examples \\(k\\) in the batch. The range of the normalized value is \\((0, 1)\\), where a high score close to \\(1\\) corresponds to a high similarity between the positive pair \\((i, j)\\) and low similarity between \\(i\\) and other augmented examples \\(k\\) in the batch. The negative log then maps the range \\((0, 1)\\) to the loss values \\((\\inf, 0)\\). </p> <p>The total loss is computed across all positive pairs \\((i, j)\\) in the batch. Let \\(z = [z_1, z_2, ..., z_{2N}]\\) include all the augmented examples in the batch, where \\(z_{1}...z_{N}\\) are outputs of the left branch, and \\(z_{N+1}...z_{2N}\\) are outputs of the right branch. Thus, the positive pairs are \\((z_{k}, z_{k + N})\\) for \\(\\forall k \\in [1, N]\\). </p> <p>Then, the total loss \\(L\\) is:</p> \\[ L = \\frac{1}{2N} \\sum_{k=1}^N [ \\; l(k, \\;k+N) + l(k+N, \\;k)\\;] \\] <p>NOTE: this equation is slightly different from the one in the paper. We've rearranged the ordering of the positive pairs in the batch, so the indices are different. The rearrangement makes it easier to implement the code in vectorized form.</p>"},{"location":"lecture1/","title":"Lecture 1: Introduction","text":""},{"location":"lecture10/","title":"Lecture 10: Recurrent Neural Networks","text":"somthing about lecture 9 <p>\u5728\u4e0a\u9762\u8fd9\u5f20\u56fe\u4e2d\u7684DenseNet, FarctalNet\u6216\u8005\u4e4b\u524d\u5b66\u4e60\u8fc7\u7684ResNet, \u4ed6\u4eec\u90fd\u6709\u4e00\u4e9b\u8de8\u8d8a\u6027\u7684\u8fde\u63a5, \u5373\u5c06\u4e00\u5c42\u7684\u8f93\u51fa\u8d8a\u8fc7\u4e0b\u4e0b\u9762\u51e0\u5c42\u8fde\u63a5\u5230\u540e\u9762\u7684\u5c42\u53bb. \u8fd9\u6837\u505a\u7684\u4e00\u70b9intuition\u5c31\u662f\u76f8\u5f53\u4e8e\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u4e3a\u68af\u5ea6\u6d41\u52a8\u5efa\u7acb\u7684\u9ad8\u901f\u516c\u8def, \u8fd9\u6837\u68af\u5ea6\u5c31\u53ef\u4ee5\u5feb\u901f\u5730\u6d41\u52a8, \u800c\u4e0d\u9700\u8981\u7ecf\u8fc7\u5f88\u591a\u5c42, \u53ef\u4ee5\u907f\u514d\u68af\u5ea6\u5728\u6bcf\u4e00\u5c42\u6d41\u52a8\u9020\u6210\u7684\u8fc7\u5206\u8870\u51cf.</p> <p>Info</p> <ul> <li>An amazing blog about RNN.</li> <li>An RNN network constructed with 112 lines of python and numpy.</li> </ul>"},{"location":"lecture10/#recurrent-neural-networks-process-sequences","title":"Recurrent Neural Networks: Process Sequences","text":"<p>\u6700\u5de6\u4fa7\u7684\u662f\u6211\u4eec\u4e4b\u524d\u8ba8\u8bba\u8fc7\u7684\u795e\u7ecf\u7f51\u7edc, \u4ed6\u4eec\u90fd\u662f\u4e00\u4e2aone to one \u7684\u51fd\u6570\u6620\u5c04, \u800cRNN\u53ef\u4ee5\u5904\u7406\u8f93\u5165\u6570\u91cf\u4e0d\u56fa\u5b9a, \u4e14\u8f93\u51fa\u6570\u91cf\u4e0d\u56fa\u5b9a\u7684\u95ee\u9898.</p> <ul> <li>one to many: Image Captioning, image -&gt; sequence of words.</li> <li>many to one: Sentimental Classification, sequence of words -&gt; sentiment.</li> <li>many to many: Machine Translation, sequence of words -&gt; sequence of words.</li> <li>many to many: Video Classification, sequence of frames -&gt; category.</li> </ul> <p>We can process a sequence of vectors \\(x\\) by applying a recurence formula at every time step:</p> \\[ h_t = f_W(h_{t-1}, x_t) \\] <p>where \\(h_t\\) and \\(h_{t-1}\\) are hidden states at time \\(t\\) and \\(t-1\\) respectively, \\(x_t\\) is the input at time \\(t\\), \\(f_W\\) is some combination of a weight matrix and a non-linearility. And usually, we want to predict a vector at some time steps.</p> <p></p> <p>Warning</p> <p>Notice: the same function and the same set of parameters are used at every time step.</p>"},{"location":"lecture10/#vanilla-recurrent-neural-network","title":"Vanilla Recurrent Neural Network","text":"<p>The state consist a single hidden vector \\(h\\).</p> <p></p>"},{"location":"lecture10/#rnn-computational-graph","title":"RNN: Computational Graph","text":"<p>Re-use the same weight matrix at every time step.</p> <p></p> <p>\u6211\u4eec\u5728\u505a\u53cd\u5411\u4f20\u64ad\u7684\u65f6\u5019, \u8981\u5bf9\\(W\\)\u6c42\u68af\u5ea6, \u9700\u8981\u5728\u6bcf\u4e2atime step\u6c42\\(W\\)\u7684\u68af\u5ea6\u7136\u540e\u76f8\u52a0. \u5373\u5148\u6c42\\(L\\)\u5bf9\u6bcf\u4e2aloss \\(L_i\\)\u7684\u68af\u5ea6, \u5728\u6c42\u51fa\u6bcf\u4e2a\\(L_i\\)\u5bf9\\(W\\)\u7684\u68af\u5ea6, \u6700\u540e\u5bf9\u4e8e\u6bcf\u4e2atime step\u7684\u68af\u5ea6\u6c42\u548c.</p> <p>Encoder and Decoder</p> <p> Encoder\u5c06\u4e00\u4e9b\u5217\u7684\\(x_i\\)\u8f93\u5165\u7f16\u7801\u6210\u4e3a\u4e00\u4e2a\u5411\u91cf, \u4ea4\u7ed9Decoder\u8fdb\u884c\u89e3\u7801, \u7136\u540eDecoder\u5c06\u8fd9\u4e2a\u5411\u91cf\u89e3\u7801\u6210\u4e3a\u4e00\u7cfb\u5217\u7684\\(y_i\\).</p> <p>Example</p> <p> At test time sample characters one at a time, feed back to model. \u8fd9\u53e5\u8bdd\u5f88\u91cd\u8981, \u6211\u4eec\u4e0d\u662f\u9009\u62e9argmax(probability)\u4f5c\u4e3a\u4e0b\u4e00\u5c42\u8f93\u5165\u7684\\(x\\), \u800c\u662f\u4f9d\u7167\u6982\u7387\u5206\u5e03\u6765\u53d6\u6837\u4f5c\u4e3a\u4e0b\u4e00\u4e2atime step\u7684\u8f93\u5165. \u8fd9\u6837\u505a\u7684\u597d\u5904\u662f\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u591f\u589e\u5927\u751f\u6210\u6587\u672c\u7684diversity.</p>"},{"location":"lecture10/#backpropagation-through-time","title":"Backpropagation Through Time","text":""},{"location":"lecture10/#naive-implementation","title":"Naive Implementation","text":"<p>The naive idea is to just froward through the entire sequence to compute the loss, and then backward through entire sequence to compute gradient.</p> <p>\u4f46\u4ed4\u7ec6\u60f3\u60f3\u8fd9\u6837\u80af\u5b9a\u4e0d\u884c, \u5047\u5982\u6211\u4eec\u7684\u8bad\u7ec3\u6570\u636e\u662f\u6574\u4e2awikipedia\u7684\u6587\u672c, \u8fd9\u65e0\u7591\u4f1a\u4f7f\u5f97\u6211\u4eec\u7684\u8ba1\u7b97\u8fc7\u7a0b\u975e\u5e38\u590d\u6742\u4e14\u7f13\u6162, \u5185\u5b58\u5f00\u9500\u4e5f\u5f88\u5927.</p>"},{"location":"lecture10/#truncated-backpropagation-through-time","title":"Truncated Backpropagation Through Time","text":"<ul> <li>Run forward and backwrad through chunks of the sequence instead of whole sequence.</li> <li>Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps.</li> </ul> <p>\u6211\u4eec\u6bcf\u6b21forward pass \u548c backprop\u7684\u65f6\u5019, \u90fd\u53ea\u7528\u5f53\u524d\u4e00\u6bb5RNN\u6765\u6765\u8fdb\u884c\u6c42loss\u548c\u68af\u5ea6, \u89e3\u51b3\u4e86\u8ba1\u7b97\u91cf\u8fc7\u5927\u7684\u95ee\u9898.</p>"},{"location":"lecture10/#image-captioning","title":"Image Captioning","text":"<p>Combined both CNN and RNN.</p> <p></p> <p>Info</p> <p> \u6211\u4eec\u5c06CNN\u63d0\u53d6\u51fa\u7684\u56fe\u50cf\u7279\u5f81\u8f93\u5165\u5230RNN\u4e2d, \u5229\u7528RNN\u751f\u6210\\(y_0\\), \u800c\u540e\u5728\\(y_0\\)\u4e2dsample\u51fa\u4e00\u4e2aword, \u4f5c\u4e3a\u4e0b\u4e00\u5c42\u7684\u8f93\u5165, \u5982\u6b64\u8fed\u4ee3\u8bad\u7ec3RNN\u7f51\u7edc, \u76f4\u5230\u6700\u540e\u4ece\u67d0\u4e2a\\(y_i\\)\u4e2dsample\u7684word\u662f\\(&lt;End&gt;\\) token, \u6211\u4eec\u5c31\u7ed3\u675fcaption. \u53e6\u5916, \u73b0\u5728\u7684\u4e00\u4e2arecurrent module\u591a\u4e86\u4e00\u4e2a\u56fe\u50cf\u7279\u5f81\u7684\u8f93\u5165, \u6240\u4ee5\u9700\u8981\u52a0\u5165\u4e00\u4e2a\u65b0\u7684weight matrix \\(W_{ih}\\).</p>"},{"location":"lecture10/#long-short-term-memorylstm","title":"Long Short Term Memory(LSTM)","text":""},{"location":"lecture10/#vanilla-rnn-gradient-flow","title":"Vanilla RNN Gradient Flow","text":"<p>\u5728\u4e00\u4e2a\u5355\u4e00\u7684module\u4e2d, \u901a\u8fc7\u5bf9\\(h_t\\)\u7684\u68af\u5ea6\u6c42\u89e3\u5bf9\\(h_{t-1}\\)\u7684\u68af\u5ea6, \u5b9e\u9645\u4e0a\u662f\u4e58\u4ee5\u4e86\\(W_{hh}^T\\).</p> <p></p> <p>\u4f46\u662f\u5728\u8fd9\u6837\u4e00\u4e2a\u5f88\u957f\u7684RNN\u67b6\u6784\u4e0b, \u9700\u8981\u591a\u6b21\u4e58\u4ee5\\(W_{hh}^T\\), \u53ef\u80fd\u4f1a\u5bfc\u81f4exploding gradients \u6216\u8005 vanishing gradients.</p> <p>Exploding gradients \u76f8\u5bf9\u597d\u89e3\u51b3\u4e00\u4e9b, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528gradien clipping, \u5373\u5982\u679c\u68af\u5ea6\u592a\u5927\u6211\u4eec\u5c31\u628a\u4ed6\u653e\u7f29\u5230\u67d0\u4e00\u4e2athreshold. \u4f46\u662f\u68af\u5ea6\u8d8b\u4e8e\u96f6\u8fd9\u4ef6\u4e8b\u4e0d\u662f\u5f88\u597d\u89e3\u51b3, \u9700\u8981\u6211\u4eec\u4fee\u6539\u4e00\u4e0bRNN\u7684\u67b6\u6784, \u8fd9\u4e5f\u5c31\u5f15\u5165\u4e86\u4e0b\u9762\u7684LSTM.</p>"},{"location":"lecture10/#lstm","title":"LSTM","text":"<ul> <li>LSTM\u7684\u5f15\u5165\u4e86\u9664\\(h_i\\)\u4e4b\u5916\u7684\u53e6\u4e00\u4e2a\u72b6\u6001\\(c_i\\).</li> <li>f: Forget Gate, whther to erase cell.</li> <li>i: Input Gate, whether to write to cell.</li> <li>g: G Gate, how much to write to cell.</li> <li>o: Output Gate, how much to reveal cell.</li> </ul> <p>\u6211\u4eec\u53ef\u4ee5\u770b\u5230, \u5982\u679c\u5bf9\\(C_i\\)\u6c42\u68af\u5ea6, \u90a3\u4e48\u6574\u4e2abackprop\u7684\u8fc7\u7a0b\u662f\u4e0d\u5b58\u5728\u77e9\u9635\u4e58\u6cd5\u7684, \u53ea\u6709\u5411\u91cf\u70b9\u4e58. \u800c\u4e14, \u7531\u4e8e\u6bcf\u4e00\u4e2amodule\u751f\u6210\u7684forget gate\u5927\u6982\u7387\u4e0d\u5b8c\u5168\u4e00\u6837, \u6240\u4ee5\u4e0d\u4f1a\u5b58\u5728\u8fde\u7eed\u70b9\u4e58\u4e00\u4e2a\u5411\u91cf\u7684\u60c5\u51b5. \u8fd9\u6837\u4ee5\u6765, \u76f8\u5f53\u4e8e\u6211\u4eec\u4e3a\\(C_i\\)\u7684gradient flow \u5efa\u7acb\u4e86\u4e00\u6761\"\u9ad8\u901f\u516c\u8def\", \u53ef\u4ee5\u4fbf\u6377\u4e14\u51c6\u786e\u5730\u6c42\u51fa\u5176\u68af\u5ea6. \u8fd9\u4e00\u601d\u60f3\u548cresnet, densenet\u90fd\u5f88\u76f8\u4f3c.</p> <p>\u4f46\u662fW\u7684\u68af\u5ea6\u5176\u5b9e\u624d\u662f\u6211\u4eec\u771f\u6b63care\u7684. \u7531\u4e8e\\(W\\)\u7684\u68af\u5ea6\u4e00\u65b9\u9762\u6765\u81ea\\(C\\), \u53e6\u4e00\u65b9\u9762\u6765\u81ea\\(h\\), \u6211\u4eec\u4f18\u5316\u4e86\\(C\\)\u7684gradient flow, \u4e5f\u5c31\u76f8\u5e94\u4f18\u5316\u4e86\\(W\\)\u7684gradient flow.</p>"},{"location":"lecture10/#other-rnn-variants","title":"Other RNN Variants","text":""},{"location":"lecture10/#gru","title":"GRU","text":"<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation, 2014.</p>"},{"location":"lecture11/","title":"Lecture 11: Detection and Segmentation","text":""},{"location":"lecture11/#semantic-segmentation","title":"Semantic Segmentation","text":"<ul> <li>Label each pixel in the image with a category label.</li> <li>Don't differentiate instances only care about pixels. \u5c31\u6bd4\u5982\u4e0a\u9762\u53f3\u4fa7\u7684\u56fe\u7247\u4e2d, \u53ef\u4ee5\u5212\u5206\u51fa\u5976\u725b\u5728\u54ea\u91cc, \u4f46\u662f\u660e\u786e\u5730\u533a\u5206\u4e24\u5934\u4e4b\u95f4\u7684\u754c\u9650.</li> </ul>"},{"location":"lecture11/#idea-fully-convolutional","title":"Idea: Fully Convolutional","text":"<ul> <li>Design network as a bunch of convolutional layers, with downsampling and upsampling inside the network.</li> <li>We can perform downsampling by Pooling or strided convolution, but what about upsampling?</li> </ul>"},{"location":"lecture11/#upsampling","title":"Upsampling","text":""},{"location":"lecture11/#unpooling","title":"Unpooling","text":"<p>Nearest Neighbour \u53ef\u4ee5\u7406\u89e3\u4e3aaverage pooling\u7684\u9006\u64cd\u4f5c, \u800cBed of Nails\u53ef\u4ee5\u7406\u89e3\u4e3amax pooling\u7684\u9006\u64cd\u4f5c.</p> <p></p> <p>\u6211\u4eec\u53ef\u4ee5\u5c06\u6574\u4e2a\u7f51\u7edc\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u5bf9\u79f0\u7684\u7ed3\u6784, downsampling\u65f6\u7684\u6bcf\u4e2amax pooling\u5bf9\u5e94upsampling \u65f6\u7684\u4e00\u4e2amax unpooling, \u5728unpooling\u7684\u65f6\u5019\u6211\u4eec\u6839\u636emax pooling\u65f6\u9009\u53d6\u7684\u6700\u5927\u503c\u7684\u4f4d\u7f6e\u8fdb\u884c\u586b\u5145.</p> <p>\u4f46\u662f\u4e0a\u8ff0\u8fd9\u4e09\u4e2aupsampling\u7684\u7b97\u6cd5\u90fd\u548cpooling\u4e00\u6837, \u6ca1\u6709wight parameter, \u6240\u4ee5\u4e0d\u662flearnable\u7684.</p> <p></p> <p>\u4ee5\u4e0a\u9762\u8fd9\u5f20\u56fe\u4e3a\u4f8b, \u6211\u4eec\u6709\u4e00\u4e2a3*3\u7684\u53c2\u6570filter, \u57282*2\u7684\u5206\u8fa8\u7387\u4e0a\u5bf9\u4e8e\u6bcf\u4e2a\u5143\u7d20\u4e58\u4ee5\u8be5filter, \u4f5c\u4e3aupsampling\u7ed3\u679c\u7684\u4e00\u90e8\u5206. \u8fd9\u4e00\u8fc7\u7a0b\u53ef\u4ee5\u7406\u89e3\u4e3a\u5377\u79ef\u7684\u9006\u8fc7\u7a0b.</p> 1D Example <p></p>"},{"location":"lecture11/#classification-localization","title":"Classification+ Localization","text":"<p>Localization \u5176\u5b9e\u5c31\u662f\u5728\u56fe\u7247\u4e2d\u5bf9\u5355\u4e2a\u7269\u4f53\u753b\u51fa\u4e00\u4e2a\u77e9\u5f62\u8fb9\u6846.</p> <p></p> <p>fc layer\u4e4b\u524d\u7684convolutional layer\u4e00\u822c\u662f\u5728imageNet\u4e0apretrain\u51fa\u6765\u7684.</p> <p>\u8fd9\u4e00\u6211\u4eec\u9700\u8981\u5c06\u4e24\u4e2a\u4e0d\u540c\u7684loss\u52a0\u548c\u6210\u4e00\u4e2a\u603bloss, \u9700\u8981\u5bf9\u4e24\u4e2aloss\u5206\u914d\u4e0d\u540c\u7684\u6743\u91cd, \u8fd9\u4e24\u4e2a\u6743\u91cd\u4e5f\u662fhyperparameter, \u9700\u8981\u4e0d\u65ad\u8c03\u6574.</p> <p>\u901a\u8fc7\u9884\u6d4b\u70b9\u5750\u6807\u548c\u8fb9\u6846\u5bbd\u5ea6 \u9ad8\u5ea6, \u6211\u4eec\u5b9e\u9645\u4e0a\u662f\u628a\u8fd9\u4e2a\u95ee\u9898\u5904\u7406\u6210\u4e3a\u4e86\u4e00\u4e2aregression problem.</p>"},{"location":"lecture11/#aside-human-pose-estimation","title":"Aside: Human Pose Estimation","text":"<p>\u540c\u6837\u7528regression\u7684\u601d\u8def\u5904\u7406, \u6211\u4eec\u5c06\u4eba\u7684\u59ff\u6001\u62bd\u8c61\u621014\u4e2a\u70b9\u5750\u6807, \u7136\u540e\u5728\u5377\u79ef\u7f51\u7edc\u540e\u9762\u52a0\u4e0afc layer, \u9884\u6d4b14\u4e2a\u5750\u6807, Loss\u662f14\u4e2a\u70b9\u5de6\u8fb9\u7684L2 Loss\u4e4b\u548c.</p> <p>Info</p> <p></p>"},{"location":"lecture11/#object-detection","title":"Object Detection","text":""},{"location":"lecture11/#region-proposals","title":"Region Proposals","text":"<ul> <li>Find \"blooby\" image regions that are likely to contain objects.</li> <li>Relatively fast to run, r.g. Selective Search gives 1000 region proposals in a few seconds on CPU.</li> </ul> <p>\u4f7f\u7528\u7279\u5b9a\u7684\u7b97\u6cd5\u53ef\u4ee5\u5728\u8f83\u4e3a\u5feb\u901f\u7684\u65f6\u95f4\u5185\u627e\u5230\u5f88\u591aRegion proposals, \u8fd9\u4e9bregions\u90fd\u662f\u8f83\u4e3a\u53ef\u80fd\u5305\u542b\u67d0\u4e2aobject\u7684region, \u6240\u4ee5\u6211\u4eec\u5728\u8fd9\u4e9bregion proposal\u4e0a\u518d\u53bb\u64cd\u4f5c.</p>"},{"location":"lecture11/#r-cnn","title":"R-CNN","text":"<p>\u8fd9\u6837\u7684R-CNN\u9700\u8981\u5bf9\u6bcf\u4e2aregion proposal\u8fdb\u884cfoward pass\u548cbackward pass. \u5bf9\u4e8e\u6bcf\u4e2aregion proposal, \u6211\u4eec\u9884\u6d4b\u4e00\u4e2aoffset vector, \u8868\u793a\u5f53\u524d\u8fd9\u4e2aregion proposal\u548c\u4e00\u4e2a\u771f\u5b9e\u7684bounding box\u7684\u5dee\u5f02\u6709\u591a\u5c11, \u7136\u540e\u6211\u4eec\u5bf9\u8fd9\u4e2a\u9884\u6d4b\u7ed3\u679c\u4f7f\u7528SVM Loss.</p> <p>Problems</p> <ul> <li>Training is slow, takes a lot disk space, about 84 hours.</li> <li>Inference(detection) is slow, about 47s/image with VGG 16.</li> </ul>"},{"location":"lecture11/#fast-r-cnn","title":"Fast R-CNN","text":"<p>\u6211\u4eec\u5148\u5c06\u6574\u5f20\u56fe\u7247\u5728ConvNet\u4e2d\u8fdb\u884cforward pass, \u751f\u6210\u4e00\u5f20feature map, \u7136\u540e\u5728\u8fd9\u5f20feature map\u4e0a\u9762\u518d\u6839\u636eregion proposals\u9009\u53d6\u4e00\u4e9b\u533a\u57df, \u518d\u901a\u8fc7Pooling \u548c FC layer\u9884\u6d4b\u7c7b\u522b(softamx)\u548cbounding box, \u6700\u540e\u5728\u8bad\u7ec3\u65f6\u8fd8\u662f\u5c06\u4e24\u4e2aLoss\u52a0\u6743\u6c42\u548c.</p> Faster R-CNN: ROI Pooling <p></p> <p>Fast R-CNN\u901f\u5ea6\u975e\u5e38\u5feb, \u5feb\u5230\u4e86\u8fd0\u884cRegion Proposals\u7684\u65f6\u95f4\u6210\u4e3a\u4e86\u6574\u4e2a\u7b97\u6cd5\u7684bottleneck.</p>"},{"location":"lecture11/#faster-r-cnn","title":"Faster R-CNN","text":"<p>Make CNN do proposals.</p> <p>Insert Region Proposal Network(RPN) to predict proposals from features.</p> <p></p> <p>We need jointly train four losses:</p> <ol> <li> <p>RPN classify object/not object.</p> </li> <li> <p>RPN regress box coordinates.</p> </li> <li> <p>Final classification score(object calsses).</p> </li> <li> <p>Final box coordinates.</p> </li> </ol> Test-time speed <p></p>"},{"location":"lecture11/#yolossd","title":"YOLO/SSD","text":""},{"location":"lecture11/#aside-object-detection-captioning-dense-captioning","title":"Aside: Object Detection + Captioning = Dense Captioning","text":""},{"location":"lecture11/#instance-segmentation","title":"Instance Segmentation","text":""},{"location":"lecture11/#mask-r-cnn","title":"Mask R-CNN","text":"<p>\u5728Detection\u7684\u540c\u65f6(\u9884\u6d4b\u7c7b\u522b\u548cbounding box), \u6211\u4eec\u540c\u65f6\u7ee7\u7eed\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u9884\u6d4b\u51fa\u4e00\u4e2amask, \u7528\u4e8e\u8ba4\u5b9a\u54ea\u4e00\u90e8\u5206pixels\u662f\u8be5\u7c7b\u522b.</p> <p>Info</p> <p>Mask R-CNN can also does pose. \u6211\u4eec\u53ea\u9700\u8981\u5728\u9884\u6d4bbounding box\u7684\u540c\u65f6\u5c06Joint coordinates\u4e5f\u9884\u6d4b\u51fa\u6765. </p>"},{"location":"lecture12/","title":"Lecture 12: Visualizing and Understanding","text":"<p>An neural network, especially a convolutional neural network, is just like a blackbox to us, where we input an image and get some results. But how does it actually work, what's all those intermediare feature looking for? These questions are what we are going to answer in this lecture.</p>"},{"location":"lecture12/#visualizing-filters","title":"Visualizing Filters","text":""},{"location":"lecture12/#first-layer","title":"First Layer","text":"<p>\u53ef\u4ee5\u770b\u5230\u7b2c\u4e00\u5c42\u5377\u79ef\u6838\u5f80\u5f80\u662f\u5728\u5bfb\u627e\u4e00\u4e9b\u8f83\u4e3a\u7b80\u5355\u7684\u7279\u5f81, \u6bd4\u5982\u4e00\u4e9bedge, blob\u4e4b\u7c7b.</p> <p>Question</p> <p>\u4e3a\u4ec0\u4e48\u5377\u79ef\u6838\u957f\u4ec0\u4e48\u6837\u5b50\u4ee3\u8868\u4e86\u4ed6\u5bfb\u627e\u4ec0\u4e48\u6837\u7684\u7279\u5f81? \u5377\u79ef\u6838\u4e0e\u6211\u4eec\u7684\u56fe\u50cf\u8fdb\u884c\u7684\u662f\u70b9\u79ef\u64cd\u4f5c, \u5728\u4e8c\u8005\u76f8\u540c\u7684\u60c5\u51b5\u4e0b\u54cd\u5e94\u503c\u6700\u5927.</p>"},{"location":"lecture12/#other-layers","title":"Other Layers","text":"<p>Info</p> <p>\u6211\u4eec\u4e5f\u53ef\u4ee5\u53ef\u89c6\u5316\u4e00\u4e0b\u9664\u4e86first layer\u4e4b\u5916\u7684\u5176\u4ed6layer, \u4f46\u662f\u7531\u4e8e\u8fd9\u4e9blayer\u4f5c\u7528\u7684\u5bf9\u8c61\u5e76\u4e0d\u662f\u56fe\u50cf\u672c\u8eab, \u800c\u662f\u7ecf\u8fc7\u4e00\u4e9bfilter\u5904\u7406\u7684activation map, \u6240\u4ee5\u4ed6\u4eec\u7684\u53ef\u89c6\u5316\u7ed3\u679c\u5e76\u4e0d\u662f\u5f88\u76f4\u89c2.</p>"},{"location":"lecture12/#last-layer","title":"Last Layer","text":"<p>Suppose our last layer is mapping a 4096 feature vector to 10 categories. We run the network on many images, then collect the feature vectors for all these images.</p>"},{"location":"lecture12/#nearest-neighbour","title":"Nearest Neighbour","text":"<p>For each of the image, we take its fearure vector, and find the nearest neighbour of its feature vactor produced by other images, then compare these images. The results are as follows:</p> <p></p> <p>\u4e0d\u540c\u4e8e\u76f4\u63a5\u5bf9\u56fe\u50cf\u6c42nearest neighbour, \u6211\u4eec\u5bf9\u7279\u5f81\u5411\u91cf\u6c42nearest neighbour, \u53ef\u4ee5\u5927\u5927\u589e\u52a0\u56fe\u50cf\u5339\u914d\u7684\u51c6\u786e\u7387, \u6bd4\u5982\u7b2c\u4e8c\u884c\u4e2d\u4e00\u53ea\u5934\u671d\u5de6\u7684\u5927\u8c61\u53ef\u4ee5\u6210\u529f\u5339\u914d\u5230\u4e00\u53ea\u5934\u671d\u53f3\u7684\u5927\u8c61, \u8fd9\u8bf4\u660e\u53ea\u8981\u662f\u5927\u8c61\u8fd9\u4e00\u7c7b, \u63d0\u53d6\u51fa\u6765\u7684feature vector\u90fd\u5f88\u76f8\u8fd1.</p>"},{"location":"lecture12/#dinmensionality-reduction","title":"Dinmensionality Reduction","text":"<p>Visualize the \"space\" of feature vectos by reducing dimensionality of vectors from 4096 to 2 dimensions.</p> <p>Algorithms: Principle Component Analysis(PCA), t-SNE...</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230\u6709\u660e\u663e\u7684\u805a\u7c7b\u6548\u679c, \u4e0d\u540c\u7684\u7c7b\u522b\u88ab\u5206\u6210\u4e86\u4e00\u4e2a\u4e2acluster.</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230, \u5728\u88abreduced\u4e4b\u540e\u7684\u4e8c\u7ef4\u7a7a\u95f4\u4e2d, \u4e0d\u540c\u7c7b\u522b\u7684\u56fe\u50cf\u5927\u81f4\u88ab\u5206\u5728\u4e86\u4e0d\u540c\u533a\u57df, \u5982\u53f3\u4e0a\u89d2\u6709\u4e00\u4e9b\u90fd\u6709\u7740\u84dd\u5929\u80cc\u666f\u7684\u56fe\u7247, \u5de6\u4e0b\u89d2\u5219\u662f\u4e00\u4e9b\u690d\u7269. See high-resolution version at http://cs.stanford.edu/people/karpathy/cnnembed/.</p>"},{"location":"lecture12/#visualizing-activations","title":"Visualizing Activations","text":""},{"location":"lecture12/#maximally-activating-patches","title":"Maximally Activating Patches","text":"<ul> <li> <p>Pick a layer and a channel, e.g. conv5 is 128*13*13, pick channel 17/128.</p> </li> <li> <p>Run many images through the network, record values of chosen channel.</p> </li> <li> <p>Visualize image patches taht correspond to maximal actiavtions.</p> </li> </ul> <p>\u56e0\u4e3a\u4e00\u4e2achannel\u662f\u7531\u540c\u4e00\u4e2a\u5377\u79ef\u6838\u751f\u6210\u7684, \u6240\u4ee5\u8fd9\u4e00\u5c42\u4e0a\u4e0d\u7ba1\u4ec0\u4e48\u4f4d\u7f6e\u7684\u9ad8\u54cd\u5e94\u503c\u90fd\u5bf9\u5e94\u7740\u5728\u67d0\u5f20\u56fe\u7247\u7684\u67d0\u4e2a\u4f4d\u7f6e\u51fa\u73b0\u4e86\u7279\u5b9a\u7279\u5f81. \u6211\u4eec\u5c06\u6240\u6709\u5728\u8fd9\u4e00\u5c42\u7684\u8fd9\u4e00channel\u6709\u7740\u9ad8\u54cd\u5e94\u503c\u7684\u56fe\u7247\u63d0\u53d6\u51fa\u6765, \u7136\u540e\u622a\u53d6\u51fa\u9ad8\u54cd\u5e94\u503c\u5bf9\u5e94\u7684reception field, \u5c31\u5f97\u5230\u4e86\u8fd9\u4e9b\u56fe\u7247\u4e0a\u51fa\u73b0\u7279\u5b9a\u7279\u5f81\u7684\u4f4d\u7f6e.</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230, \u6bcf\u4e2achannel\u6240\u5bfb\u627e\u7684\u7279\u5f81\u4e0d\u5c3d\u76f8\u540c, \u6bd4\u5982\u7b2c\u4e00\u884c\u5c31\u662f\u5728\u5bfb\u627e\u7c7b\u4f3c\u4e00\u4e2a\"\u6d1e\"\u7684\u7279\u5f81, \u7b2c\u4e8c\u884c\u5219\u662f\u5728\u5bfb\u627e\u4e00\u4e9b\u5f27\u5f62\u8fb9\u7f18.</p>"},{"location":"lecture12/#occlusion-experiments","title":"Occlusion Experiments","text":"<p>Mask part of the image before feeding to CNN, draw hetmap of probability at each mask location.</p> <p></p> <p>\u8fd9\u4e00\u505a\u6cd5\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u4e86\u89e3\u5230, \u56fe\u50cf\u7684\u90a3\u4e2a\u90e8\u4f4d\u5bf9\u56fe\u50cf\u8bc6\u522b\u7684\u7ed3\u679c\u5f71\u54cd\u6700\u5927, \u7ea2\u8272\u533a\u57df\u662f\u5f71\u54cd\u6700\u5927\u7684, \u9ec4\u8272\u533a\u57df\u5219\u5f71\u54cd\u8f83\u5c0f.</p>"},{"location":"lecture12/#saliency-map","title":"Saliency Map","text":"<ul> <li>How to tell which pixel matter for classification?</li> <li>Compute gradient of (unnormalized) class score with respect to image pixels, take absolute value and max over RGB channels.</li> </ul> <p>\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u770b\u51fa\u56fe\u50cf\u7684\u54ea\u4e00\u90e8\u5206\u5bf9\u8bc6\u522b\u7684\u8d21\u732e\u8f83\u5927.</p>"},{"location":"lecture12/#imtermediate-features-via-guided-backprop","title":"Imtermediate Features via (guided) backprop","text":"<ul> <li>Pick a single intermediate neruro, e.g. one value in 128*13*13 conv5 feature map.</li> <li> <p>Compute gradient of neuron value with respect to image pixels.</p> </li> <li> <p>Images come out nicer if you only backprop positive gradients through each ReLU (guided backprop).</p> </li> </ul> <p></p>"},{"location":"lecture12/#visualizing-cnn-features-gradient-ascent","title":"Visualizing CNN Features: Gradient Ascent","text":"<p>Guided Backprop: Find the part of an image that a neuron responds to. Gradient Ascent: Generate a synthetic iamge that maximally activates a neuron.</p> \\[ I^* = argmax_I f(I) + R(I) \\] <p>where \\(f(I)\\) is the neuron value, and \\(R(I)\\) is the iamge regularizer.</p> <ul> <li>Initialize image to zeros.</li> </ul> <p>Repeat:</p> <ul> <li>Froward image to compute current scores.</li> <li>Backprop to get gradient of neuron value with respect to image pixels.</li> <li>Make a small update to the image.</li> </ul> <p>\u73b0\u5728\u5047\u5982\u6211\u4eec\u8bd5\u56fe\u901a\u8fc7\u6700\u5927\u5316\u67d0\u4e2a\u7c7b\u522b\u7684\u5206\u6570, \u6765\u751f\u6210\u8be5\u7c7b\u522b\u7684\u4e00\u5f20\u56fe\u7247:</p> \\[ argmax_I S_c(I) - \\lambda||I||^2_2 \\] <p>Simple Regularizer: Penalize L2 norm of generated image.</p> <p>Better Regularizer: Penalize L2 norm of iamge; also during optimaization periodically: 1. Gaussian blur image 2. Clip pixels with small values to 0. 3. Clip pixels with small gradients to 0.</p> <p>\u53ef\u4ee5\u770b\u5230\u751f\u6210\u7684\u56fe\u7247\u90fd\u8f83\u4e3a\u5408\u7406\u7684, \u6709\u4e9b\u8fbe\u5230\u4e86\u4e0d\u9519\u7684\u6548\u679c, \u5982flamingo, billiard table.</p> <p>Info</p> <p>We can use the same approach to visualize intermediate features: </p> mutil-faceted <p></p>"},{"location":"lecture12/#deep-dream-amplify-existing-features","title":"Deep Dream: Amplify existing features","text":""},{"location":"lecture12/#feature-inversion","title":"Feature Inversion","text":"<p>Given a CNN feature vector for an image, find a new image that:</p> <ul> <li>Matches the given feature vector.</li> <li>Looks natural.</li> </ul> \\[ x^* = \\mathop{\\arg\\min}_{x \\in \\mathbb{R}^{H \\times W \\times C}} L(\\Phi(x), \\Phi_0) + \\lambda R(x) \\] <p>where $$ L(\\Phi(x), \\Phi_0) = ||\\Phi(x) - \\Phi_0||^2 $$</p> <p>$$ R(x) = \\sum_{i, j}((x_{i, j+1}-x_{i, j})^2+(x_{i+1, j}-x_{i, j})^2)^{\\frac{\\beta}{2}} $$ This regularizer is called Total Variation Regularizer, which encourages spatial smmothness.</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230, \u4f7f\u7528\u8d8a\u4f4e\u7ef4\u7684\u7279\u5f81, \u751f\u6210\u7684\u56fe\u7247\u4e0e\u539f\u56fe\u8d8a\u543b\u5408, \u8fd9\u8bf4\u660e\u4e86\u9ad8\u7ef4\u7279\u5f81\u4fe1\u606f\u4f1a\u4e22\u5f03\u6389\u4e00\u4e9b\u4e0d\u90a3\u4e48\u91cd\u8981\u7684\u4fe1\u606f, \u53ea\u4fdd\u7559\u56fe\u50cf\u4e00\u4e9b\u6bd4\u8f83\u663e\u8457\u7684\u7279\u5f81.</p>"},{"location":"lecture12/#texture-synthesis","title":"Texture synthesis","text":"<p>Given a simple patch of some texture can we generate a bigger image of the same texture.</p> <p></p>"},{"location":"lecture12/#neural-style-transfer","title":"Neural Style Transfer","text":"<p>We combine feature reconstruction and texture synthesis together to do the neural style transfer.</p>"},{"location":"lecture13/","title":"Lecture 13: Generative Models","text":""},{"location":"lecture13/#unsupervised-learning","title":"Unsupervised Learning","text":"<ul> <li>Just data, no labels, which means training is cheap.</li> <li>Learn some underlying hidden structure of the data.</li> </ul>"},{"location":"lecture13/#generative-model","title":"Generative Model","text":"<p>Given training data, generate new samples from same distribution.</p> <p></p> <p>Taxonomy of Generative Models</p> <p></p>"},{"location":"lecture13/#pixelrnn-and-pixelcnn","title":"PixelRNN and PixelCNN","text":"<p>Explicit density model, which use chain rule to decompose likelihood of an image x into product of 1-d distributions:</p> \\[ p(x) = \\prod_{i=1}^{n}p(x_i|x_1, ..., x_{i-1}) \\] <ul> <li> <p>On the left is the likelihood of image x, while on the right is the probabilities of ith pixel value given all previous pixels.</p> </li> <li> <p>What we want to do is to maximize the likelihood of the training data.</p> </li> <li> <p>But how do we know about the complex distribution over pixel values? Express them using a neural network.</p> </li> <li> <p>Also, we need to define ordering of \"previous pixels\".</p> </li> </ul>"},{"location":"lecture13/#pixelrnn","title":"PixelRNN","text":"<ul> <li>Generate image pixels starting from corner.</li> <li>Dependency on previous pixels modeled using an RNN(LSTM).</li> <li>Drawback: Sequential generation is slow.</li> </ul>"},{"location":"lecture13/#pixelcnn","title":"PixelCNN","text":"<ul> <li>Still generate image pixels starting from corner.</li> <li>Dependency on previous pixels now modeled using a CNN over context region.</li> <li>Training: Maximize likelihood of training images.</li> <li>Training is faster than PixelRNN (can parallelize convolutions since context region values known from training images).</li> <li>Generation must still proceed sequentially =&gt; still slow.</li> </ul> Pros and Cons"},{"location":"lecture13/#variational-autoencodervae","title":"Variational Autoencoder(VAE)","text":"<p>PixelCNNs define tractable density function, optimize likelihood of training data:</p> \\[ p(x) = \\prod_{i=1}^{n}p(x_i|x_1, ..., x_{i-1}) \\] <p>VAEs define intractable density function with latent z:</p> \\[ p_\\theta (x) = \\int p_\\theta(z)p(x|z)dz \\] <p>\\(z\\)\u53ef\u4ee5\u8ba4\u4e3a\u662f\u4e00\u4e9b\u63d0\u53d6\u51fa\u6765\u7684feature.</p> <p>Cannot optimize directly, derive and optimize lower bound on likelihood instead, which will be discussed later.</p>"},{"location":"lecture13/#autoencoders","title":"Autoencoders","text":"<p>Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data.</p> <p></p> <p>\u6211\u4eec\u4f7f\u7528\u8bf8\u5982CNN\u7684\u624b\u6bb5\u63d0\u53d6\u51fafeature Z, \u7528\u4e8e\u6355\u6349\u56fe\u50cf\u4e2d\u7684meaningful factor.</p> <p> \u901a\u8fc7\u8bad\u7ec3, \u6211\u4eec\u5e0c\u671bfeature Z\u80fd\u91cd\u6784\u51fa\u539f\u59cb\u56fe\u50cf, \u4f7f\u7528L2 loss:</p> \\[ ||x - \\hat{x}||_2^2 \\] <p>\u8bad\u7ec3\u4e4b\u540e, \u6211\u4eecthrow away decoder\u90e8\u5206, \u53ea\u4fdd\u7559encoder\u90e8\u5206\u7528\u4e8e\u63d0\u53d6\u56fe\u50cf\u7684\u7279\u5f81Z.</p>"},{"location":"lecture13/#variational-autoencoders","title":"Variational Autoencoders","text":"<p>\u4f7f\u7528encoder\u751f\u6210z\u7684\u5206\u5e03, \u7136\u540e\u6839\u636e\u5747\u503c\u548c\u65b9\u5dee\u91c7\u6837\u751f\u5c42z, \u518d\u901a\u8fc7decoder\u751f\u6210\u91cd\u6784\u56fe\u50cf\u7684\u5747\u503c\u548c\u65b9\u5dee, \u91c7\u6837\u751f\u6210\u91cd\u6784\u56fe\u50cf.</p> <p></p> <p>\u8fd9\u91cc\u662f\\(p_\\theta(x^{(i)})\\)\u8868\u8fbe\u5f0f\u7684\u63a8\u5bfc\u8fc7\u7a0b. \u6700\u540e\u4e00\u884c\u4e2d,\u524d\u4e24\u9879\u53ef\u4ee5\u8ba4\u4e3a\u662ftractable\u7684lower bound, which we can take gradient of and optimize. \u7b2c\u4e09\u9879\u662fintractable\u7684, \u4f46\u662f\u5176\\(\\geq 0\\). \u6240\u4ee5\u6211\u4eec\u5728\u8bad\u7ec3\u65f6\u4f18\u5316\u524d\u4e24\u9879\u5373\u53ef, \u76f8\u5f53\u4e8e\u6700\u5927\u5316\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u6982\u7387\u7684lower bound.</p> <p>Info</p> <p></p> Example Pros and Cons <p></p>"},{"location":"lecture13/#generative-adversarial-networksgan","title":"Generative Adversarial Networks(GAN)","text":"<p>What if we give ip on explicitly modeling density, and just want ability to sample?</p> <p>GANS: don't work with any explicitly density function, instead, take game theoretic approach: learn to generate from training distribution through 2-player game.</p> <p></p> <p>\u6211\u4eec\u9700\u8981\u8bad\u7ec3\u4e24\u4e2a\u7f51\u7edc, \u4e00\u4e2agenerator, \u4e00\u4e2adiscriminator, \u4e8c\u8005\u540c\u65f6\u8bad\u7ec3.</p> <p></p> <p>discriminator\u5e0c\u671b\u6700\u5927\u5316\u8fd9\u4e2a\u51fd\u6570, \u5373\u5f53\u8f93\u5165\u4e3a\u771f\u5b9e\u6570\u636e\u65f6, \u8f93\u51fa\u4e3a1, \u5f53\u8f93\u5165\u4e3a\u751f\u6210\u6570\u636e\u65f6, \u8f93\u51fa\u4e3a0. generator\u5e0c\u671b\u6700\u5c0f\u5316\u8fd9\u4e2a\u51fd\u6570, \u5373\u80fd\u591f\u6210\u529f\u9a97\u8fc7discriminator.</p> <p>GAN training algorithm</p> <p></p> <p>After training, we only use the generator network to generate new images.</p> <p>Results of generation</p> <p> \u6211\u4eec\u53ef\u4ee5\u770b\u51fagenerator\u4e0d\u4f1a\u76f4\u63a5\u751f\u6210\u4e00\u4e2a\u5b8c\u5168\u548c\u8bad\u7ec3\u96c6\u67d0\u4e2a\u6570\u636e\u76f8\u540c\u7684\u56fe\u50cf, \u800c\u662f\u5728\u5b66\u4e60training set\u7684\u57fa\u7840\u4e0a\u751f\u6210\u65b0\u56fe\u50cf.</p> Generative Adversarial Nets: Convolutional Architectures <p> </p>"},{"location":"lecture14/","title":"Lecture 14: Reinforcement Learning","text":""},{"location":"lecture14/#what-is-reinforcement-learning","title":"What is reinforcement learning?","text":"<p>\u5728\\(state_i\\)\u4e0b\u91c7\u53d6\u4e00\u4e2aaction \\(a_t\\), \u83b7\u5f97reward\\(r_t\\), \u4ea7\u751f\u4e0b\u4e00\u4e2a\u72b6\u6001\\(state_{t+1}\\).</p>"},{"location":"lecture14/#cart-pole-problem","title":"Cart-Pole Problem","text":""},{"location":"lecture14/#robot-locomotion","title":"Robot Locomotion","text":""},{"location":"lecture14/#atari-games","title":"Atari Games","text":""},{"location":"lecture14/#go","title":"Go","text":""},{"location":"lecture14/#markov-decision-process","title":"Markov Decision Process","text":"<ul> <li>Mathematical formulation of the RL problem.</li> <li>Markov Property: Current state completely characterizes the state of the world.</li> </ul> A simple MDP: Grid World"},{"location":"lecture14/#the-optimal-policy-pi","title":"The optimal policy \\(\\pi^{*}\\)","text":""},{"location":"lecture14/#value-function-and-q-value-function","title":"Value Function and Q-value Function","text":""},{"location":"lecture14/#bellman-equation","title":"Bellman Equation","text":""},{"location":"lecture14/#solving-for-the-optimal-policy-q-learning","title":"Solving for the optimal policy: Q-learning","text":"<p>Q-learning use a function approximator to estimate the action-value function:</p> \\[ Q(s, a; \\theta) \\approx Q^*(s, a) \\] <p>where \\(\\theta\\) is the function parameters(weights).</p> <p>If the function approximator is a deep neural network =&gt; deep 1-learning.</p> <p></p>"},{"location":"lecture14/#case-study-playing-atari-games","title":"Case Study: Playing Atari Games","text":"<ul> <li>Objective: Complete the game with highest score.</li> <li>State: Raw pixel inputs of the game state.</li> <li>Action: Game controls, left, right...</li> <li>Reward: Score increase/decrease at each time step.</li> </ul>"},{"location":"lecture14/#q-network-architecture","title":"Q-network Architecture","text":"<p>\u6700\u7ec8\u751f\u6210\u56db\u4e2a\u503c, \u5206\u522b\u4e3a\u4e0a\u4e0b\u5de6\u53f3\u7684Q value. \u6211\u4eec\u4f7f\u7528\u6700\u8fd1\u7684\u56db\u5e27\u6765\u9884\u6d4bQ value.</p> <p>Number of actions between 4-18 depending on Atari game.</p>"},{"location":"lecture14/#experience-replay","title":"Experience Replay","text":""},{"location":"lecture14/#policy-gradients","title":"Policy Gradients","text":"<p>What is a problem with Q-learning?</p> <p>The Q-function can be very complicated!</p> <p>Example: a robot grasping an object has a very high-dimensional state =&gt; hard to learn exact value of every (state, action) pair. But the policy can be much simpler: just close your hand. Can we learn a policy directly, e.g. finding the best policy from a collection of policies?</p> <p></p> <p>\u8fd9\u91cc\u5b9a\u4e49\u4e86\u5bf9\u4e8e\u4e00\u4e2aPolicy\u7684\u4ef7\u503c, \u5373\u7d2f\u8ba1reward\u7684\u671f\u671b.</p> <p></p> <p>\u4e00\u4e2aPolicy\u7d2f\u8ba1reward\u7684\u671f\u671b\u5176\u5b9e\u5c31\u662f\u5bf9\u4e8e\u6240\u6709\u8f68\u8ff9\u6c42\u548c\u4f9d\u636e\u6982\u7387\u53d6\u5e73\u5747.</p> <p>\u8df3\u8fc7\u590d\u6742\u7684\u6570\u5b66\u63a8\u5bfc, \u5f97\u5230\u4e0b\u9762\u7684\u7ed3\u8bba:</p> <p></p> <p>\u529b\u4e89\u5c06\u80fd\u83b7\u5f97\u8f83\u5927reward\u7684action\u7684\u6982\u7387\u62c9\u5927, \u53cd\u4e4b\u5219\u51cf\u5c0f. \u6211\u4eec\u671f\u671b\u5728\u591a\u6b21\u8bad\u7ec3\u4e4b\u540e\u80fd\u5c06\u90a3\u4e9b\u8f83\u4e3a\u4f18\u79c0\u7684action\u51f8\u663e\u51fa\u6765.</p> <p>\u4f46\u662f\u53ef\u80fd\u4f1a\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u9047\u5230\u9ad8\u65b9\u5dee\u95ee\u9898\u3002\u8fd9\u610f\u5473\u7740\u4f30\u8ba1\u7684\u503c\u5728\u4e0d\u540c\u7684\u8bad\u7ec3\u8fed\u4ee3\u4e4b\u95f4\u6ce2\u52a8\u5f88\u5927\uff0c\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u4e0d\u7a33\u5b9a\u3002\u540c\u65f6, \u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u7279\u522b\u662f\u5728\u5ef6\u8fdf\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u5b9a\u67d0\u4e2a\u7279\u5b9a\u52a8\u4f5c\u5bf9\u672a\u6765\u5956\u52b1\u7684\u8d21\u732e\u662f\u975e\u5e38\u56f0\u96be\u7684\u3002\u4f8b\u5982\uff0c\u67d0\u4e2a\u52a8\u4f5c\u53ef\u80fd\u4f1a\u5728\u5f88\u4e45\u4e4b\u540e\u624d\u4ea7\u751f\u660e\u663e\u7684\u6548\u679c\u3002\u8fd9\u4f7f\u5f97\u51c6\u786e\u5730\u5f52\u56e0\u53d8\u5f97\u590d\u6742\uff0c\u8fdb\u4e00\u6b65\u589e\u52a0\u4e86\u4f30\u8ba1\u5668\u7684\u65b9\u5dee\u3002\u9ad8\u65b9\u5dee\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u4ea7\u751f\u4e25\u91cd\u7684\u4e0d\u7a33\u5b9a\u6027, \u6240\u4ee5\u9700\u8981\u91c7\u53d6\u4e00\u5b9a\u624b\u6bb5\u51cf\u5c0f\u65b9\u5dee.</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"lecture2/","title":"Lecture 2: Image Classification with Linear Classifiers","text":"<p>Image Classification is a core task in computer vision.</p>"},{"location":"lecture2/#the-image-classification-task","title":"The Image Classification Task","text":"<p>When a computer sees an image, it actually sees a whole bunch of data, a data matrix.</p> <p></p>"},{"location":"lecture2/#challenges","title":"Challenges:","text":"<ul> <li>Viewpoint Variation</li> <li>Illumination</li> <li>Background Clutter</li> <li>Occlusion</li> <li>Deformation</li> <li>Intraclass Variation</li> </ul> <p>These can all make a difference to the data matrix.</p>"},{"location":"lecture2/#an-image-classfier","title":"An image classfier","text":"<p><pre><code>def classify_image(image):\n    # some magic here?\n    return class_label\n</code></pre> Machine Learning: Data-Driven Approach</p> <ol> <li>Collect a dataset of images and labels</li> <li>Use Machine Learning algorithms to train a classifier</li> <li>Evaluate the classifier on new images</li> </ol> <pre><code>def train(images, labels):\n    # machine learning\n    return model\n\ndef predict(model, test_iamges):\n    # Use model to predict labels\n    return test_labels\n</code></pre>"},{"location":"lecture2/#nearest-neighbour-classifier","title":"Nearest Neighbour Classifier","text":"<ul> <li>Memorize all the data and labels.</li> <li>Predict the label of the most similar training image.</li> </ul>"},{"location":"lecture2/#distance-metric-to-compare-images","title":"Distance Metric to Compare Images","text":"<p>L1(Manhattan) distance:</p> \\[ d_1(I_1, I_2) = \\sum_p |I^p_1 - I^p_2| \\] <p>L2(Euclidean) distance:</p> \\[ d_2(I_1, I_2) = \\sqrt{\\sum_p (I^p_1 - I^p_2)^2} \\] <p></p> <p>\u6211\u4eec\u53ef\u4ee5\u770b\u5230, L1 distance\u7684\u4e0d\u53d8\u6027\u662f\u5728\u4e00\u4e2a\u83f1\u5f62\u7684\u8fb9\u754c\u4e0a, \u800cL2 distance\u7684\u4e0d\u53d8\u6027\u4f53\u73b0\u5728\u4e00\u4e2a\u5706\u7684\u8fb9\u754c\u4e0a. \u5f53\u6211\u4eec\u65cb\u8f6c\u5750\u6807\u8f74, L1\u7684\u4e0d\u53d8\u6027\u533a\u57df\u4f1a\u53d1\u751f\u6539\u53d8, \u800cL2\u4e0d\u4f1a.</p>"},{"location":"lecture2/#1-nearest-neighbour-classifier","title":"1-nearest Neighbour Classifier","text":"<ul> <li>Find the nearest training image to the test image.</li> <li>Use the label of the nearest training image to classify the test image.</li> </ul> <pre><code>class NearestNeighbour:\n    def _init_(self):\n        pass\n\n    def train(self, X, y):\n        # Simply memorize the data.\n        self.xtr = X\n        self.ytr = y\n\n    def predict(self, X):\n        num_test = X.shape[0]\n        Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n\n        # loop over all the test images.\n        for i in xrange(num_test):\n            # We use L1 distance as an example.\n            distance = np.sum(np.abs(self.Xtr - X[i, :]), axis = 1)\n            min_index = np.argmin(distance)\n            Ypred[i] = self.ytr[min_index]\n\n        return Ypred\n</code></pre> <p>With N examples, how fast are tarining and prediction?</p> <p>Training: O(1)  Prediction: O(N)</p> <p>This is bad: we want classifiers that are fast at prediction; slow for training is ok.</p> <p>\u771f\u5b9e\u7684\u9884\u6d4b\u6a21\u578b\u9700\u8981\u5728\u4f7f\u7528\u65f6\u6709\u7740\u8f83\u9ad8\u7684\u54cd\u5e94\u901f\u5ea6, \u800c\u8bad\u7ec3\u8fc7\u7a0b\u7684\u65f6\u95f4\u53ef\u4ee5\u76f8\u5bf9\u8f83\u957f.</p> <p></p> <p>Warning</p> <p>\u5982\u679c\u6211\u4eec\u4ec5\u4ec5\u53ea\u8003\u8651\u6700\u8fd1\u90bb, \u90a3\u4e48\u5728\u5b9e\u9645\u5206\u7c7b\u95ee\u9898\u4e2d\u5f88\u53ef\u80fd\u51fa\u73b0\u4e0b\u56fe\u7684\u60c5\u51b5, \u5373\u53ef\u80fd\u7531\u4e8e\u67d0\u4e9b\u5e26\u6709\u9519\u8bef\u6216\u8005\u7f55\u89c1\u7684\u6570\u636e\u800c\u5bfc\u81f4\u5206\u7c7b\u8fb9\u7f18\u5b58\u5728\u952f\u9f7f\u72b6, \u6216\u67d0\u4e00\u7c7b\u4e2d\u51fa\u73b0\u53e6\u5916\u4e00\u7c7b\u7684\u5b64\u5c9b.</p>"},{"location":"lecture2/#k-nearest-neighbour-classifier","title":"K-nearest Neighbour Classifier","text":"<p>Instead of copying label from nearest neighbor, take majority vote from K closest points.</p> <p></p> <p>\u6211\u4eec\u53ef\u4ee5\u770b\u5230, \u9002\u5f53\u589e\u5927K, \u4f1a\u4f7f\u5f97\u7c7b\u95f4\u8fb9\u7f18\u53d8\u5f97\u5e73\u6ed1, \u4e14\u5b64\u5c9b\u9762\u79ef\u7f29\u51cf. \u8bf4\u660e\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027.</p> <p>\u4e0d\u540c\u7684distance metric\u4e5f\u4f1a\u5bfc\u81f4\u5206\u7c7b\u7ed3\u679c\u6709\u6240\u5dee\u5f02.</p> <p>K-Nearest Neighbors: try it yourself</p> <p>A website to tune K and distance metrices.</p>"},{"location":"lecture2/#hyperparameters","title":"Hyperparameters","text":"<p>What is the best value of k to use? What is the best distance to use?</p> <p>These are hyperparameters: choices about the algorithms themselves.</p> <ul> <li>Very problem/dataset-dependent.</li> <li>Must try them all out and see what works best.</li> </ul> <p>To set hyperparameters, we need to divide the dataset into training data, validation data, and test data. </p> <p></p> <p>Futhermore, we can do the Cross-Validation, which is to split data into folds, and try each fold as validation and average the results. </p> <p></p> <p>Cross-Validation is useful for small datasets, but not used too frequently in deep learning. \u6211\u4eec\u628a\u6570\u636e\u5206\u4e3aN\u4e2afolds, \u7136\u540e\u9009\u62e9\u5176\u4e2d\u4e00\u4e2a\u4f5c\u4e3avalidation set, \u5176\u4f59\u7684\u4f5c\u4e3atraining set, \u7136\u540e\u91cd\u590dN\u6b21, \u6bcf\u6b21\u9009\u62e9\u4e0d\u540c\u7684fold\u4f5c\u4e3avalidation set. \u8fd9\u6837N\u6b21\u8bad\u7ec3\u4e2d, \u6bcf\u6b21\u8ba1\u7b97\u51fa\u5728validation set\u4e0a\u7684\u51c6\u786e\u7387, \u6700\u540eN\u6b21\u53d6\u5e73\u5747\u4f5c\u4e3a\u6700\u7ec8\u7684\u8bad\u7ec3\u6548\u679c\u6307\u6807.</p> <p>Example of 5-fold cross-validation for the value of k(Seems that k ~= 7 works best for this data):</p> <p></p> <p>Choose hyperparameters using the validation set, and only run on the test set once at the very end!</p> <p>Distance metrics on pixels are not informative Original</p> <p> Occluded, shifted or tinted \u7684\u56fe\u50cf\u4e0d\u4e00\u5b9a\u80fd\u591f\u88ab\u5f88\u597d\u5730\u533a\u5206. \u800c\u4e14\u8fd9\u4e9b\u56fe\u7247\u5c31\u662f\u539f\u56fe\u7a0d\u4f5c\u5904\u7406\u7684\u7ed3\u679c, \u4f46\u662f\u53ef\u80fd\u4e0e\u539f\u56fe\u6709\u7740\u663e\u8457\u7684distance.</p> <p>Curse of dimensionality</p> <p> \u4f7f\u7528K\u8fd1\u90bb\u7b97\u6cd5\u7684\u8bdd, \u6211\u4eec\u9700\u8981\u8db3\u591f\u591a\u7684\u6570\u636e\u70b9\u6765cover\u6574\u4e2aspace, \u4ee5\u4fdd\u8bc1\u4e0d\u4f1a\u51fa\u73b0\u6700\u8fd1\u90bb\u90fd\u5f88\u8fdc\u7684test data. \u6240\u4ee5\u968f\u7740dimensionality\u7684\u589e\u52a0, \u6240\u9700\u7684\u8bad\u7ec3\u6570\u636e\u70b9\u7684\u4e2a\u6570\u4f1a\u4ee5\u6307\u6570\u7ea7\u522b\u589e\u957f.</p>"},{"location":"lecture2/#linear-classifier","title":"Linear Classifier","text":""},{"location":"lecture2/#definition","title":"Definition","text":"<p>\u6211\u4eec\u4f7f\u7528\u4e00\u4e2aWeight\u77e9\u9635\u4e0e\u8f93\u5165\u7684\u56fe\u7247\u5411\u91cf\u76f8\u4e58, \u52a0\u4e0a\u4e00\u4e2aBias\u5411\u91cf, \u751f\u6210\u4e00\u4e2a\\(N\\times 1\\)\u7684\u5411\u91cf(N\u4e3a\u7c7b\u522b\u6570), \u6bcf\u4e2a\u5143\u7d20\u4ee3\u8868\u56fe\u50cf\u5904\u4e8e\u8be5\u7c7b\u7684score. Weight\u77e9\u9635\u548cBias\u5411\u91cf\u5373\u4e3alinear classifier\u7684Hyperparameter.</p> <p>Weight\u77e9\u9635\u7684\u53c2\u6570\u901a\u8fc7Training\u5b66\u4e60\u5f97\u5230, \u800cBias\u5411\u91cf\u4f53\u73b0\u7740preference, \u5373\u5047\u5982\u5f85\u5206\u7c7b\u7684\u56fe\u50cf\u4e2dCat\u7684\u6570\u91cf\u8f83\u591a, \u90a3\u4e48\u5728bias\u4e2d\u5c31\u53ef\u80fd\u4f1a\u5c06Cat\u5bf9\u5e94\u7684\u53c2\u6570\u4eba\u4e3a\u8bbe\u7f6e\u5f97\u6bd4\u5176\u4ed6\u7c7b\u90fd\u9ad8\u4e00\u4e9b.</p>"},{"location":"lecture2/#interpretation","title":"Interpretation","text":"<ul> <li>Algebraic Viewpoint</li> </ul> <p>Example with an image with 4 pixels and 3 classes.</p> <p></p> <ul> <li>Visual Viewpoint</li> </ul> <p></p> <p>\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u89c2\u5bdf\u4e00\u4e0b\u6743\u91cd\u77e9\u9635\u6bcf\u4e00\u884c\u5b66\u4e60\u51fa\u6765\u7684\u5185\u5bb9\u5230\u5e95\u662f\u4ec0\u4e48, \u6bcf\u4e00\u884c\u4ee3\u8868\u4e00\u4e2a\u7c7b\u522b. \u53ef\u4ee5\u770b\u5230, plane\u5927\u81f4\u5c31\u662f\u84dd\u8272\u7684\u5e95\u8272(\u5929\u7a7a)\u52a0\u4e0a\u4e86\u4e00\u4e2a\u5f62\u72b6\u7c7b\u4f3c\u98de\u673a\u7684\u4e1c\u897f, \u800chorse\u7c7b\u4e0b\u90e8\u6709\u7740\u7eff\u8272\u7684\u80cc\u666f, \u56e0\u4e3a\u9a6c\u4e00\u822c\u548c\u8349\u5730\u4e00\u8d77\u51fa\u73b0, \u4f46\u662f\u8fd9\u5339\u9a6c\u4f3c\u4e4e\u6709\u4e24\u4e2a\u5934, \u8fd9\u662f\u56e0\u4e3a\u5bf9\u4e8e\u6bcf\u4e00\u7c7b\u6211\u4eec\u53ea\u80fd\u6709\u4e00\u884c\u4f5c\u4e3a\u5206\u7c7b\u5668, \u90a3\u4e48\u4e0d\u540c\u59ff\u6001\u7684\u9a6c\u88ab\u5e73\u5747\u4e4b\u540e\u5c31\u4f1a\u4ea7\u751f\u8fd9\u79cd\u53cc\u5934\u7684\u60c5\u5f62.</p> <ul> <li>Geometric Viewpoint</li> </ul> <p></p> <p>\u5728\u9ad8\u7ef4\u7a7a\u95f4\u8fdb\u884c\u7ebf\u6027\u89c4\u5212.</p>"},{"location":"lecture2/#hard-cases","title":"Hard Cases","text":"<p>\u4e0a\u8ff0\u8fd9\u4e9b\u5206\u7c7b\u4e2d, \u6bcf\u4e00\u4e2a\u7c7b\u90fd\u5f88\u96be\u7528\u76f4\u7ebf\u4e00\u6761\u76f4\u7ebf\u5212\u5206, \u6545Linear Classifier\u5728\u8fd9\u4e9b\u5206\u7c7b\u4e2d\u8868\u73b0\u4e0d\u597d.</p>"},{"location":"lecture3/","title":"Lecture 3: Loss Functions and Optimization","text":""},{"location":"lecture3/#loss-functions","title":"Loss Functions","text":"<p>A loss function tells how good our current classifier is.</p> <p>Given a dataset of examples \\(\\{(x_i, y_i)\\}_{i=1}^{N}\\), where \\(x_i\\) is image and \\(y_i\\) is (integer) label.</p> <p>Loss over the dataset is a sum of loss over examples: $$ L = \\frac{1}{N} \\sum_{i}L_i(f(x_i, W), y_i) $$</p>"},{"location":"lecture3/#multiclass-svm-loss","title":"Multiclass SVM Loss","text":"\\[ L_i = \\sum_{j \\neq y_i} \\begin{cases}      0 &amp; \\text{if } s_{y_i} \\geq s_j + \\Delta \\\\     s_j - s_{y_i} + \\Delta &amp; \\text{otherwise} \\\\ \\end{cases} \\] <p>Or: $$ L_i = \\sum_{j \\neq y_i}max(0, s_j - s_{y_i} + \\Delta) $$</p> <pre><code>def L_i_vectorized(x, y, W):\n    scores = W.dot(x)\n    margins = np.maximum(0, socres - scores[y] + Delta)\n    margins[y] = 0\n    loss_i = np.sum(margins)\n    return loss_i\n</code></pre> <p>The final Loss will be: $$ L = \\frac{1}{N} \\sum_{i=1}^{N}L_i $$</p> <p>Example</p> <p> \u6211\u4eec\u7528\u7b2c\u4e00\u884c\u4e3e\u4f8b\u8ba1\u7b97Loss(\u53d6\\(\\Delta = 1\\)), \\(L = (5.1-3.2+1) + 0 = 2.9\\).</p> <p>Info</p> <p> The Multiclass Support Vector Machine \"wants\" the score of the correct class to be higher than all other scores by at least a margin of delta. If any class has a score inside the red region (or higher), then there will be accumulated loss. Otherwise the loss will be zero. Our objective will be to find the weights that will simultaneously satisfy this constraint for all examples in the training data and give a total loss that is as low as possible.</p> <p>\u5bf9\u4e8e\\(max(0, -)\\)\u8fd9\u79cd\u5f62\u5f0f\u7684\u51fd\u6570\u4f1a\u88ab\u79f0\u4e3ahinge loss, \u6709\u4e9b\u4eba\u8fd8\u4f1a\u4f7f\u7528 square hinge loss, \u5373\\(max^2 (0, -)\\). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation.</p>"},{"location":"lecture3/#softmax-loss","title":"Softmax Loss","text":"<p>scores = unnormalized log probabilities of the classes.</p> <p>$$ P(Y = k|X = x_i) = \\frac{e^{s_k}}{\\sum_j e^{s_j}}  $$ where \\(s = f(x_i, W)\\).</p> <p>Want to maximize the log likelihood, or (for a loss function) to minimize the negative log likelihood of the correct class:</p> \\[ L_k = -logP(Y = k|X = x_i) \\] <p>in summary: $$ L_k = -log(\\frac{e^{s_k}}{\\sum_j e^{s_j}}) $$</p> <p>Comparison between softmax and SVM</p> <p> Suppose I take a datapoint and I jiggle a bit (changing its score slightly). What happens to the loss in both cases? \u5bf9\u4e8eSVM\u6765\u8bb2, \u53ea\u8981\u6b63\u786e\u7c7b\u7684\u5206\u6570\u6bd4\u522b\u7684\u7c7b\u81f3\u5c11\u5927\u4e00\u4e2a\\(\\Delta\\)\u7684margin, \u90a3\u4e48\u8fd9\u4e00\u9879\\(L_i\\)\u5c31\u4f1a\u53d8\u4e3a0, \u4e0d\u5728\u5bf9loss\u4ea7\u751f\u8d21\u732e. \u4f46\u662f\u5bf9\u4e8esoftmax\u6765\u8bb2, \\(L_i\\)\u59cb\u7ec8\u4e0d\u53ef\u80fd\u8870\u51cf\u4e3a\u96f6. In other words, the Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint.</p> <p>Question</p> <p>Suppose that we found a W such that L = 0. Is this W unique? No! 2W also has \\(L=0\\).</p>"},{"location":"lecture3/#regularization","title":"Regularization","text":""},{"location":"lecture3/#intro","title":"Intro","text":"<p>Suppose that we have a dataset and a set of parameters W that correctly classify every example (i.e. all scores are so that all the margins are met, and \\(L_i=0\\)  for all i). The issue is that this set of W is not necessarily unique: there might be many similar W that correctly classify the examples. One easy way to see this is that if some parameters W correctly classify all examples (so loss is zero for each example), then any multiple of these parameters \\(\\lambda W\\)  where \\(\\lambda &gt;1\\)  will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of W by 2 would make the new difference 30.</p> <p>\u5176\u5b9e\u4e0a\u9762\u8fd9\u6bb5\u5f15\u5165\u5c31\u662f\u60f3\u8bf4\u660e\u6211\u4eec\u9700\u8981\u5728\u5b58\u5728\u591a\u4e2aW\u77e9\u9635\u7684\u65f6\u5019\u91c7\u7528\u67d0\u79cd\u65b9\u6cd5\u907f\u514d\u8fc7\u62df\u5408.</p> <p></p>"},{"location":"lecture3/#definition","title":"Definition","text":"<p>Regularization\u5373\u4e3a\u7ed9\u635f\u5931\u51fd\u6570\u589e\u52a0\u4e00\u9879regularization penalty\u6765\u9632\u6b62\u8fc7\u62df\u5408:</p> <p>$$  L = \\frac{1}{N}\\sum_{i=1}^N L_i(f(x_i, W), y_i) + \\lambda R(W)  $$</p> <ul> <li>Data loss: Model predictions should match training data</li> <li>Regularization: Model should be \u201csimple\u201d, so it works on test data</li> <li>\\(\\lambda\\) is a hyperparameter</li> </ul>"},{"location":"lecture3/#types","title":"Types","text":"<ul> <li>L2 regularization: \\(R(W) = \\sum_k \\sum_l W_{k,l}^2\\)</li> <li>L1 regularization: \\(R(W) = \\sum_k \\sum_l |W_{k,l}|\\)</li> <li>Elastic net: \\(\\lambda R(W) = \\lambda \\alpha R_1(W) + (1-\\lambda) R_2(W)\\)</li> </ul> <p>\u76f4\u89c9\u4e0a\u6765\u8bb2, L2 regularization\u503e\u5411\u4e8e\u8ba9\u6570\u503c\u5728W\u77e9\u9635\u5185\u5206\u5e03\u66f4\u5747\u5300, \u800c\u4e0d\u662f\u67d0\u51e0\u4e2a\u5143\u7d20\u7279\u522b\u5927.</p>"},{"location":"lecture3/#optimization","title":"Optimization","text":""},{"location":"lecture3/#strategy-follow-the-slope","title":"Strategy: Follow the slope","text":"<ul> <li>In multiple dimensions, the gradient is the vector of (partial derivatives) along each dimension.</li> <li>In multiple dimensions, the gradient is the vector of (partial derivatives) along each dimension.</li> <li>The direction of steepest descent is the negative gradient.</li> </ul>"},{"location":"lecture3/#calculation-of-the-gradient","title":"Calculation of the Gradient","text":"<p>Note that in the mathematical formulation the gradient is defined in the limit as h goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5 as seen in the example). Ideally, you want to use the smallest step size that does not lead to numerical issues. Additionally, in practice it often works better to compute the numeric gradient using the centered difference formula: \\([f(x+h)\u2212f(x\u2212h)]/2h\\). </p> <p></p> <p>This, however, has a problem of efficiency. The loss is just a function of W, so we use calculus to compute an analytic gradient, and plug the current W in to get a numeric gradient.</p> <p>In summary:</p> <ul> <li>Numerical gradient: approximate, slow, easy to write</li> <li>Analytic gradient: exact, fast, error-prone(\u5bb9\u6613\u51fa\u9519)</li> </ul> <p>In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check.</p>"},{"location":"lecture3/#gradient-descent","title":"Gradient Descent","text":"<pre><code>while True:\n    weights_grad = evaluate_gradient(loss_fun, data, weights)\n    weights += -step_size * weights_grad\n</code></pre>"},{"location":"lecture3/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"\\[     L(W) = \\frac{1}{N}\\sum_{i=1}^N L_i(f(x_i, W), y_i) + \\lambda R(W) \\\\ \\] \\[     \\nabla_W L(W) = \\frac{1}{N}\\sum_{i=1}^N \\nabla_W L_i(f(x_i, W), y_i) + \\lambda \\nabla_W R(W) \\\\ \\] <p>\u8fd9\u4e24\u4e2a\u5f0f\u5b50\u662f\u68af\u5ea6\u4e0b\u964d\u7684\u8868\u8fbe\u5f0f, \u4f46\u662f\u5f53N\u5f88\u5927\u7684\u65f6\u5019, \u8ba1\u7b97\u91cf\u4f1a\u975e\u5e38\u5927. \u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5927\u5c0f\u4e3a32/64/128\u7684mini-batch\u6765\u4ee3\u66ffN, \u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4f18\u5316\u8ba1\u7b97\u91cf.</p> <pre><code>while True:\n    data_batch = sample_training_data(data, 256)\n    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n    weights += -step_size * weights_grad\n</code></pre>"},{"location":"lecture3/#step-sizelearning-rate","title":"Step Size(Learning Rate)","text":"<p>The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step. Choosing the step size (also called the learning rate) will become one of the most important (and most headache-inducing) hyperparameter settings in training a neural network.</p> <p>Info</p> <p>A website to visualize the Linear Classification Loss.</p>"},{"location":"lecture3/#image-features","title":"Image Features","text":""},{"location":"lecture3/#color-histogram","title":"Color Histogram","text":""},{"location":"lecture3/#histogram-of-oriented-gradients-hog","title":"Histogram of Oriented Gradients (HoG)","text":""},{"location":"lecture3/#bag-of-words","title":"Bag of Words","text":""},{"location":"lecture4/","title":"Lecture 4: Backpropagation and Neural Networks","text":""},{"location":"lecture4/#chain-rule","title":"Chain Rule","text":"<p>\u5047\u8bbe\u6211\u4eec\u6709\u4e24\u4e2a\u51fd\u6570 \\( f \\) \u548c \\( g \\)\uff0c\u5176\u4e2d \\( f \\) \u662f \\( g \\) \u7684\u51fd\u6570\uff0c\u5373 \\( f(g(x)) \\)\u3002\u94fe\u5f0f\u6c42\u5bfc\u6cd5\u5219\u544a\u8bc9\u6211\u4eec\uff0c\u590d\u5408\u51fd\u6570 \\( f(g(x)) \\) \u7684\u5bfc\u6570\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> \\[ \\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x)  \\] <p>\u5176\u4e2d\uff0c\\( f'(g(x)) \\) \u662f\u51fd\u6570 \\( f \\) \u5bf9 \\( g(x) \\) \u7684\u5bfc\u6570\uff0c\\( g'(x) \\) \u662f\u51fd\u6570 \\( g \\) \u5bf9 \\( x \\) \u7684\u5bfc\u6570\u3002</p> <p>\u4e5f\u5373: $$ \\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx} $$</p>"},{"location":"lecture4/#computational-graphs","title":"Computational Graphs","text":"<p>\u628a\u4e00\u4e2a\u590d\u6742\u7684\u4ee3\u6570\u8868\u8fbe\u5f0f\u7b80\u5316\u6210\u4e00\u4e2a\u56fe\u7ed3\u6784, \u6bcf\u4e2anode\u4e3a\u4e00\u4e2aoperator, \u63a5\u53d7\u4e00\u4e2a\u6216\u591a\u4e2aoperand\u8fdb\u884c\u8fd0\u7b97. \u5f53\u6bcf\u4e2avariable\u6709\u5177\u4f53\u503c\u7684\u65f6\u5019, \u6574\u4e2a\u56fe\u4e2d\u62d3\u6251\u5e8f\u6700\u9760\u540e\u7684node\u7684\u8f93\u51fa\u4e3a\u8be5\u8868\u8fbe\u5f0f\u7684\u503c.</p> <p>An Simple Example</p> <p></p>"},{"location":"lecture4/#forward-pass","title":"Forward Pass","text":"<p>\u5728\u771f\u6b63\u5229\u7528computation graph\u65f6, \u6211\u4eec\u9996\u5148\u9700\u8981\u4f7f\u7528forward pass\u6765\u8ba1\u7b97\u51fa\u5728\u6bcf\u4e2avariable\u7ed9\u5b9a\u65f6\u6700\u7ec8\u7684Loss\u503c. \u6211\u4eec\u53ea\u9700\u8981\u6309\u7167\u62d3\u6251\u5e8f\u5bf9\u6574\u5f20\u56fe\u8fdb\u884c\u4f9d\u6b21\u8ba1\u7b97\u5373\u53ef. \u5bf9\u4e8e\u67d0\u4e00\u4e2anode, \u63a5\u53d7\u4e0b\u6e38\u7684\u4e00\u4e2a\u6216\u591a\u4e2a\u6570\u503c, \u5bf9\u5176\u8fdb\u884c\u8be5node\u5bf9\u5e94\u7684\u64cd\u4f5c, \u7136\u540e\u518d\u5c06\u8ba1\u7b97\u7ed3\u679c\u4f20\u9012\u7ed9\u4e0a\u6e38.</p>"},{"location":"lecture4/#backpropagation","title":"Backpropagation","text":"<p>\u5728\u5229\u7528forward pass\u8ba1\u7b97\u5b8c\u5404\u4e2anode\u7684\u5bf9\u5e94\u503c\u4e4b\u540e, \u6211\u4eec\u5c31\u53ef\u4ee5\u6309\u7167\u6574\u5f20\u56fe\u7684\u9006\u62d3\u6251\u5e8f\u5229\u7528chain rule\u8fdb\u884c\u6c42\u5bfc.</p> <p>\u5177\u4f53\u6765\u8bb2, \u56e0\u4e3a\u6bcf\u4e2anode\u90fd\u662f\u8f83\u4e3a\u7b80\u5355\u7684\u51fd\u6570(\u5982+, -, *, /, sigmoid, max\u7b49), \u6211\u4eec\u53ef\u4ee5\u9884\u5148\u786e\u5b9a\u8fd9\u4e9bnode\u7684\u5173\u4e8e\u8f93\u5165\u53d8\u91cf\u7684\u5bfc\u6570\u8868\u8fbe\u5f0f, backpropagation\u65f6\u53ea\u9700\u8981\u5c06\u67d0\u4e2a\u53d8\u91cf(a)\u7684\u5177\u4f53\u503c\u4ee3\u5165\u5bf9\u5e94\u504f\u5bfc\u8868\u8fbe\u5f0f, \u4e58\u4e0a\u4e0a\u6e38\u4f20\u6765\u7684\u5bfc\u6570\u503c, \u5c31\u80fd\u6c42\u5f97\\(\\frac{\\partial L}{\\partial a}\\).</p> <p>A more complicated example</p> <p> \u53ef\u4ee5\u770b\u5230, \u6211\u4eec\u53ea\u9700\u8981\u4f9d\u6b21\u8fdb\u884c\u521a\u521a\u6240\u53d9\u8ff0\u7684forward pass\u548cpropagation\u5373\u53ef\u5b8c\u6210Loss\u5173\u4e8e\u5404\u4e2a\u53d8\u91cf\u504f\u5bfc\u6570\u5177\u4f53\u503c\u7684\u8ba1\u7b97.  \u53e6\u5916, \u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4e0d\u540cgranularity(\u7c92\u5ea6)\u7684node, \u6bd4\u5982\u5c06\u7b2c\u4e00\u5f20\u56fe\u7684\u51e0\u4e2anode\u5408\u5e76\u6210\u4e00\u4e2asigmoid node, \u8fd9\u6837\u53ef\u4ee5\u901a\u8fc7\u6784\u5efa\u7a0d\u5fae\u590d\u6742\u4e00\u4e9b\u7684node\u6765\u7b80\u5316computation graph.</p>"},{"location":"lecture4/#some-gates-in-particular","title":"Some Gates in Particular","text":"<p>Note that here we use gate and node interchangeably.</p> <p></p> <ul> <li>add gate: gradient distributor</li> <li>max gate: gradient router</li> <li>mul gate: gradient switcher</li> </ul>"},{"location":"lecture4/#gradients-for-vectorized-code","title":"Gradients for Vectorized Code","text":"<p>Jacobian Matrix:</p> <p>\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u4ece \\(\\mathbb{R}^n\\) \u6620\u5c04\u5230 \\(\\mathbb{R}^m\\) \u7684\u51fd\u6570 \\(\\mathbf{f} : \\mathbb{R}^n \\to \\mathbb{R}^m\\)\uff0c\u5176\u5f62\u5f0f\u4e3a\uff1a</p> \\[ \\mathbf{f}(\\mathbf{x}) = \\begin{bmatrix} f_1(x_1, x_2, \\ldots, x_n) \\\\ f_2(x_1, x_2, \\ldots, x_n) \\\\ \\vdots \\\\ f_m(x_1, x_2, \\ldots, x_n) \\end{bmatrix} \\] <p>\u96c5\u53ef\u6bd4\u77e9\u9635 \\(\\mathbf{J}\\) \u662f \\(\\mathbf{f}\\) \u5bf9 \\(\\mathbf{x}\\) \u7684\u504f\u5bfc\u6570\u7ec4\u6210\u7684\u77e9\u9635\uff0c\u5b9a\u4e49\u4e3a\uff1a</p> \\[ \\mathbf{J}(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\frac{\\partial f_m}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\] <p>\u8fd9\u91cc\uff0c\\(\\frac{\\partial f_i}{\\partial x_j}\\) \u8868\u793a \\(f_i\\) \u5bf9 \\(x_j\\) \u7684\u504f\u5bfc\u6570\u3002</p> <p></p> <p>A useful conclusion</p> <p>\u5982\u679c\u6211\u4eec\u6709: $$ y = f(AB) = f(c) $$ \u5176\u4e2d \\(A, B, C\\) \u90fd\u662f\u77e9\u9635\uff0c\u90a3\u4e48\u6709\u5982\u4e0b\u7ed3\u8bba: $$ \\frac{\\partial f}{\\partial A} = \\frac{\\partial f}{\\partial C} \\cdot B^T $$ \u540c\u7406: $$ \\frac{\\partial f}{\\partial B} = A^T \\cdot \\frac{\\partial f}{\\partial C} $$</p> <p>\u7ed3\u5408\u4e0a\u9762\u7684\u7ed3\u8bba\u6211\u4eec\u770b\u4e00\u4e2a\u4f8b\u5b50: </p> <p>\u53ef\u4ee5\u770b\u5230: $$     \\frac{\\partial f}{\\partial q} = 2q $$ $$     \\frac{\\partial f}{\\partial W} = \\frac{\\partial f}{\\partial q} \\cdot x^T $$ $$     \\frac{\\partial f}{\\partial x} = W^T \\cdot \\frac{\\partial f}{\\partial q}\\ $$</p> <p>Warning</p> <p>Always check: The gradient with respect to a variable should have the same shape as the variable.</p>"},{"location":"lecture4/#modularized-implementation","title":"Modularized Implementation","text":""},{"location":"lecture4/#graph-object","title":"Graph Object","text":"<pre><code>class ComputationalGraph\n    # ...\n    def forward(inputs):\n        # 1. pass inputs to input gates\n        # 2. forward the computational graph\n        for gate in self.graph.nodes_topologically_sorted():\n            gate.forward()\n        return loss\n    def backward():\n        for gate in reversed(self.graph.nodes_topologically_sorted()):\n            gate.backward()\n        return inputs_gradients\n</code></pre>"},{"location":"lecture4/#gate-object","title":"Gate Object","text":"<pre><code>class MultiplyGate(object)\n    def forward(x, y)\n        z = x*y\n        self.x = x\n        self.y = y\n        return z\n    def backward(dz)\n        dx = self.y * dz\n        dy = self.x * dz\n        return [dx, dy]\n</code></pre>"},{"location":"lecture4/#neural-networks","title":"Neural Networks","text":"<ul> <li>(Before) Linear score function: \\(f = Wx\\)</li> <li>(Now) 2-layer Neural Network: \\(f = W_2max(0, W_1x)\\)</li> <li>(Further) 3-layer Neural Network: \\(f = W_3max(0, W_2max(0, W_1x))\\)</li> </ul> <p>\u5728Linear Classifier\u91cc, \u6211\u4eec\u8bf4W\u77e9\u9635\u7684\u6bcf\u4e00\u884c\u4f1a\u5b66\u4e60\u5230\u6bcf\u79cd\u7c7b\u522b\u7684\u6a21\u677f, \u6700\u7ec8\u751f\u621010\u4e2a\u7c7b\u522b\u76f8\u5e94\u7684\u5206\u6570. \u4f46\u662f\u56e0\u4e3a\u6bcf\u79cd\u7c7b\u522b\u53ef\u80fd\u4f1a\u51fa\u73b0\u591a\u79cd\u60c5\u51b5, \u6240\u4ee5\u5355\u4e00\u6a21\u677f\u4f1a\u5c06\u8fd9\u4e9b\u60c5\u51b5\u5e73\u5747\u5730\u5b66\u4e60\u8fdb\u53bb, \u5982\u6709\u7740\u4e24\u4e2a\u5934\u7684\u9a6c. </p> <p>\u5728\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u4e2d, \u6211\u4eec\u5c06h\u5c42(hidden layer)\u8bbe\u7f6e\u6210100\u4e2a\u8282\u70b9, \u8fd9\u6837W1\u77e9\u9635\u4f1a\u76f8\u5e94\u589e\u5927, \u90a3\u4e48\u6bcf\u4e00\u884c\u4f1a\u5b66\u4e60\u5230\u66f4\u52a0\u5177\u4f53\u7684\u7279\u5f81. \u6bd4\u5982, \u65b0\u7684W1\u7684\u4e00\u884c\u53ef\u80fd\u5b66\u4e60\u5230\u4e86\u4e00\u4e2a\u5934\u504f\u5411\u5de6\u8fb9\u7684\u9a6c, \u800c\u6881\u53e6\u5916\u6709\u4e00\u884c\u5b66\u4e60\u5230\u7684\u662f\u5934\u504f\u5411\u53f3\u7684\u9a6c. \u8fd9\u4e9b\u66f4\u4e3a\u5177\u4f53\u7684\u7279\u5f81, \u518d\u901a\u8fc7W2\u7684\u52a0\u6743\u6c42\u548c, \u6700\u7ec8\u624d\u751f\u6210\u4e8610\u4e2a\u7c7b\u522b\u7684\u5206\u6570. \u90a3\u4e48\u5047\u5982\u73b0\u5728\u6211\u4eec\u6709\u4e09\u5339\u9a6c, \u5934\u7684\u671d\u5411\u5206\u522b\u4e3a\u5de6, \u53f3\u548c\u4e2d\u95f4, \u90a3\u4e48\u7b2c\u4e00\u5339\u9a6c\u5728\"\u5934\u671d\u5de6\"\u7c7b\u522b\u4e0a\u5206\u6570\u5f88\u9ad8, \u7b2c\u4e8c\u5339\u5728\"\u5934\u671d\u53f3\"\u7c7b\u522b\u5206\u6570\u5f88\u9ad8, \u7b2c\u4e09\u5339\u5728\u4e8c\u8005\u90fd\u6709\u4e00\u5b9a\u7684\u5206\u6570, \u90a3\u4e48\u7ecf\u8fc7\u8bad\u7ec3\u540e\u7684W2\u7684\u52a0\u6743\u6c42\u548c, \u4e09\u5339\u9a6c\u90fd\u80fd\u5728horse\u8fd9\u4e00\u7c7b\u4e0a\u83b7\u5f97\u8f83\u9ad8\u7684score.</p> <p>\u8fd9\u91cc\u7684\\(max\\)\u51fd\u6570\u88ab\u79f0\u4e3anon-linear \u7684 activation function, \u5bf9\u4e8e\u795e\u7ecf\u7f51\u7edc\u6765\u8bb2\u6709\u591a\u79cd\u9009\u62e9: </p> Full implementation of training a 2-layer Neural Network<pre><code>import numpy as np\nfrom numpy.random import randn\n\nN, D_in, H, D_out = 64, 1000, 100, 10\nx, y = randn(N, D_in), randn(N, D_out)\nw1, w2 = randn(D_in, H), randn(H, D_out)\nlearning_rate = 1e-4\n\nfor t in range(2000):\n    h = 1 / (1 + np.exp(-x.dot(w1))) # sigmoid\n    y_pred = h.dot(w2)\n    loss = np.square(y_pred - y).sum()\n    print(t, loss)\n\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2  = h.T.dot(frad_y_pred)\n    grad_h = grad_y_pred.dot(w2.T)\n    grad_w1 = x.T.dot(grad_h * h * (1-h))\n    # \u4e0a\u9762\u8fd9\u51e0\u884c\u5e94\u5f53\u8ba4\u4e3a\u7684\u662f S = hW2 = sigmoid(xW1)W2, \u5982\u679c\u987a\u5e8f\u53cd\u8fc7\u6765\u7684\u8bdd\u4e0a\u9762dot\u62ec\u53f7\u5185\u5916\u7684\u987a\u5e8f\u9700\u8981\u8981\u53cd\u4e00\u4e0b.\n\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n</code></pre>"},{"location":"lecture5/","title":"Lecture 5: Convolutional Neural Networks","text":""},{"location":"lecture5/#convolution-layer","title":"Convolution Layer","text":""},{"location":"lecture5/#official-definition","title":"Official Definition","text":"<p>Convolution of two signals:</p> \\[ f[x, y]*g[x, y] = \\sum_{n_1 = - \\infty}^{\\infty}\\sum_{n_2 = -\\infty}^{\\infty}f[n_1, n_2]g[x-n_1, y-n_2] \\] <p>This is an elementwise multiplication and a sum of a filter and the signal (image).</p>"},{"location":"lecture5/#our-usage","title":"Our Usage","text":"<p>Suppose we have a 32*32*3 image, and a 5*5*3 filter, we can perform convolution as following:</p> <p></p> <ul> <li>Convolve the filter with the image, i.e. \"slide over the image spatially, computing dot products\". \u6211\u4eec\u8fd9\u91cc\u7684\u5377\u79ef\u662f\u4e00\u4e2a\u4e0d\u90a3\u4e48\u4e25\u683c\u7684\u5b9a\u4e49, \u5373\\(f[x, y]\\)\u548c\\(g[x, y]\\)\u505a\u5377\u79ef\u65f6\u76f4\u63a5\u91c7\u7528\u76f8\u540c\u4e0b\u6807\u5373\u53ef.</li> <li>Filters always extend the full depth of the input volume. \u5377\u79ef\u6838\u548c\u8f93\u5165\u56fe\u5c42\u7684\u6df1\u5ea6\u5fc5\u987b\u4e00\u81f4.</li> </ul> <p>\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u591a\u4e2a\u5377\u79ef\u6838\u5904\u7406\u56fe\u50cf, \u6bcf\u4e2a\u5377\u79ef\u6838\u5904\u7406\u5b8c\u5f62\u6210\u4e86\u4e00\u4e2a\u56fe\u5c42, \u90a3\u4e48\u6700\u540e\u8f93\u51fa\u56fe\u5c42\u7684\u6df1\u5ea6\u5c31\u662f\u5377\u79ef\u6838\u7684\u4e2a\u6570, \u5982\u56fe:</p> <p></p> <p>\u5377\u79ef\u6838\u5904\u7406\u5b8c\u7684\u7ed3\u679c\u88ab\u79f0\u4e3a activation map(s).</p> <p>Calculate the size of output volume:</p> <p>Suppose our Conv layer accepts a volume of size \\(W_1 \\times H_1 \\times D_1\\), and the output volume is of the size \\(W_2 \\times H_2 \\times D_2\\), and we use \\(K\\) filters with size \\(F \\times F \\times D_1\\).</p> \\[ W_2 = \\frac{W_1 - F + 2P}{S} + 1 \\] \\[ H_2 = \\frac{H_1 - F + 2P}{S} + 1 \\] \\[ D_2 = K \\] <ul> <li>\\(P\\)\u4ee3\u8868\u4e86padding, \u4e3a\u4e86\u66f4\u597d\u5730\u63d0\u53d6\u56fe\u50cf\u8fb9\u7f18/\u89d2\u843d\u4fe1\u606f, \u9700\u8981\u5728\u56fe\u50cf\u5468\u56f4\u586b\u5145\u4e00\u4e9b\u50cf\u7d20, \u5e38\u89c1\u7684\u586b\u5145\u65b9\u5f0f\u4e3a zero padding.</li> </ul> <p></p> <ul> <li>\\(S\\)\u4ee3\u8868\u4e86stride, \u5373\u5377\u79ef\u6838\u5728\u56fe\u50cf\u4e0a\u79fb\u52a8\u7684\u6b65\u957f.</li> </ul> <p>Example</p> <p> \u8ba1\u7b97\u53c2\u6570\u4e2a\u6570\u65f6\u4e0d\u8981\u5fd8\u8bb0\u5bf9\u4e8e\u6bcf\u4e2a\u5377\u79ef\u6838\u8fd8\u6709\u4e00\u4e2abias.</p> <p>Note</p> <p></p>"},{"location":"lecture5/#fully-connected-layer","title":"Fully Connected Layer","text":"<p>\u6bcf\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u4e0d\u5149\u6709Conv layer, \u5728\u6700\u540e\u8fd8\u4f1a\u5305\u542b\u4e00\u4e2a\u6216\u8005\u591a\u4e2aFC layer, \u6bd4\u5982\u5728image classification\u95ee\u9898\u4e2d, \u5728\u5377\u79ef\u5c42\u4e4b\u540e\u7684fc layer\u5c31\u7528\u4e8e\u751f\u6210\u6bcf\u4e00\u7c7b\u7684\u9884\u6d4b\u5206\u6570. </p> <p></p>"},{"location":"lecture5/#pooling-layer","title":"Pooling Layer","text":"<ul> <li>makes the representation smaller and more manageable.</li> <li>operates over each activation map independently.</li> </ul> <p>\u6700\u5e38\u89c1\u7684pooling\u65b9\u5f0f\u662fmax pooling, \u5373\u53d6\u6bcf\u4e2aactivation map\u4e2d\u6bcf\u4e2a\u7a97\u53e3\u7684\u6700\u5927\u503c, \u800c\u4e14\u4e00\u822c\u8fd9\u4e9b\u7a97\u53e3\u4e0d\u76f8\u4ea4(stride = window size).</p> <p></p> <ul> <li>Pooling\u5904\u7406\u540e\u751f\u6210\u7684volume\u548c\u4e0e\u539f\u6765\u7684volume\u5177\u6709\u76f8\u540c\u7684depth.</li> <li>Introduces zero parameters since it computes a fixed function of the input. Pooling layer \u4e0d\u4f1a\u5f15\u5165\u53c2\u6570.</li> <li>Note taht it is not common to use zero-padding for Pooling layers.</li> </ul>"},{"location":"lecture5/#convolutional-neural-networks-hierarchy","title":"Convolutional Neural Networks Hierarchy","text":"<ul> <li>ConvNets stack CONV, POOL and FC layers.</li> <li>Tredn towards smaller fliters and deeper architectures.</li> <li>Trend towards getting rid of POOL/FC layers(just CONV).</li> <li>Typical architectures look like:</li> </ul> \\[ \\text{[(CONV-RELU)*N -&gt; POOL]*M -&gt; (FC-RELU)*K -&gt; softmax} \\] <p>where N is usually up to ~5, M is large, \\(0\\le K \\le 2\\).</p> <p>Example</p> <p></p>"},{"location":"lecture5/#intuition-of-activation-maps","title":"Intuition of Activation Maps","text":"<p>\u5728\u6574\u4e2aCNN\u4e2d, \u4ece\u8f93\u5165\u56fe\u50cf\u5230\u6700\u540e\u7684FC layer\u4e4b\u95f4, \u4f1a\u4ea7\u751f\u4e00\u7cfb\u5217\u7684activation maps, \u8fd9\u4e9bactivation maps\u53ef\u4ee5\u770b\u505a\u662fCNN\u5bf9\u4e8e\u56fe\u50cf\u7279\u5f81\u7684\u63d0\u53d6. \u4e00\u822c\u6765\u8bb2, \u8f83\u4e3a\u9760\u524d\u7684activation map\u63d0\u53d6\u7684\u662f\u8f83\u4e3a\u7b80\u5355\u7684low level feature, \u800c\u9760\u540e\u7684activation map\u63d0\u53d6\u7684\u662f\u8f83\u4e3a\u590d\u6742\u7684high level feature. \u5982\u56fe:</p> <p></p> <p></p> <p>\u8fd9\u5f20\u56fe\u7247\u4f7f\u752832\u4e2afilter\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u4e86\u7279\u5f81\u63d0\u53d6, \u5728activation map\u4e2d\u6211\u4eec\u662f\u53ef\u4ee5\u770b\u5230\u4e00\u4e9b\u56fe\u50cf\u7684\u7279\u5f81\u7684(\u8fb9\u7f18, \u5f27\u5ea6, etc), \u5e76\u4e14\u6bcf\u4e2afilter\u63d0\u53d6\u51fa\u7684\u7279\u5f81\u90fd\u4e0d\u5c3d\u76f8\u540c.</p>"},{"location":"lecture6/","title":"Lecture 6: Training Neural Networks, Part 1","text":""},{"location":"lecture6/#activation-functions","title":"Activation Functions","text":""},{"location":"lecture6/#sigmoid","title":"Sigmoid","text":"\\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\] <ul> <li>Squashes numbers to range [0, 1].</li> <li>Historically popular since they have nice interpretation as saturating \"firing rate\" of a neuron.</li> </ul> <p>\u5f53\u795e\u7ecf\u5143\u63a5\u6536\u5230\u8db3\u591f\u7684\u523a\u6fc0\u65f6\uff0c\u5b83\u7684\u653e\u7535\u7387\uff08\u8f93\u51fa\uff09\u4f1a\u8d8b\u4e8e\u4e00\u4e2a\u4e0a\u9650\u6216\u9971\u548c\u70b9\uff0c\u800c\u4e0d\u4f1a\u65e0\u9650\u589e\u52a0\u3002</p> <p>Three Problems:</p> <ul> <li>Saturated neurons \"kill\" the gradients.</li> </ul> <p>When x is very positive or very negative, the sigmoid output will be closed to 1 or 0, which means the gradients will be very close to zero.</p> <p>\u8fd9\u6837\u4f1a\u5bfc\u81f4\u67d0\u4e9b\u53c2\u6570\\(w\\)\u7684\u68af\u5ea6\u4e00\u76f4\u63a5\u8fd1\u96f6, \u66f4\u65b0\u901f\u5ea6\u6781\u5176\u7f13\u6162.</p> <ul> <li>Sigmoid outputs are not zero-centered.</li> </ul> <p>\u8fd9\u6837\u4f1a\u5bfc\u81f4\u8fdb\u5165\u4e0b\u4e00layer\u65f6\u7684\u8f93\u51fa\u5168\u90e8\u4e3a\u6b63, Consider what happens when the input to a neuron (x) is always positive?</p> <p>We have: $$ f(XW + b) $$</p> <p>where \\(f\\) is the activation function.</p> <p>For the gradients respect to W:</p> \\[ \\frac{\\partial L}{\\partial W} = X^T \\frac{\\partial L}{\\partial f} \\] <p>where \\(\\frac{\\partial L}{\\partial f}\\) is the upstream gradient, which in some cases could be either positive or negative. However, \\(X^T\\) is always positive, which means the gradients respect to \\(W_{i, j}\\) will be all positive or negative. What will this leads to ?</p> <p>Example</p> <p> \u53ef\u4ee5\u770b\u5230, \u5047\u8bbe\u6211\u4eec\u73b0\u5728\u53ea\u6709\u4e24\u4e2a\\(w\\)\u53c2\u6570, \u6a2a\u8f74\u662f\\(w_1\\), \u7eb5\u8f74\u662f\\(w_2\\), \u4e8c\u8005\u7684\u68af\u5ea6\u6b63\u8d1f\u5f62\u603b\u662f\u76f8\u540c\u7684, \u610f\u5473\u7740\u4ed6\u4eec\u53ea\u80fd\\((w_1, w_2)\\)\u8fd9\u4e00\u5411\u91cf\u53ea\u80fd\u5411\u7740\u7b2c\u4e00\u4e09\u8c61\u9650\u8fdb\u884c\u53d8\u5316. \u4f46\u662f\u5982\u679c\u771f\u6b63\u7684\u6700\u4f18\\(W\\)\u5904\u4e8e\u7b2c\u56db\u8c61\u9650, \u90a3\u4e48gradient descent\u7684\u8fc7\u7a0b\u5fc5\u987b\u7ecf\u5386\u5f88\u591azig-zag, \u4e0d\u5982\u76f4\u7ebf\u6765\u5f97\u6709\u6548\u7387.</p> <p>\u7531\u4e0a\u9762\u8fd9\u4e2a\u4f8b\u5b50\u6211\u4eec\u53ef\u4ee5\u770b\u51fa\u4e3a\u4ec0\u4e48\u6211\u4eec\u5e0c\u671b\u6bcf\u4e2alayer\u7684\u8f93\u5165\u6570\u636e\u662fzero-centered\u7684.</p> <ul> <li><code>exp()</code> is a bit computing expensive.</li> </ul>"},{"location":"lecture6/#tanh","title":"Tanh","text":"\\[ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\] <ul> <li>Squashes numbers to range [-1, 1].</li> <li>Zero-centered(nice).</li> <li>Still kills the gradients when saturated.</li> </ul>"},{"location":"lecture6/#relurectified-linear-unit","title":"ReLU(Rectified Linear Unit)","text":"\\[ \\text{ReLU}(x) = \\max(0, x) \\] <ul> <li>Does not saturated in positive region.</li> <li>Very computationally efficient.</li> <li>Converges much faster than sigmoid/tanh in practice, about 6 times.</li> <li>Actually more biologically plausible than sigmoid.</li> </ul> <p>Failure</p> <ul> <li>Not zero-centered.</li> <li>Can kill the gradients in negative region.</li> </ul> <p>people like to initialize ReLU neurons with slightly positive biases (e.g. 0.01).</p>"},{"location":"lecture6/#leaky-relu","title":"Leaky ReLU","text":"\\[ \\text{Leaky ReLU}(x) = \\max(0.01x, x) \\] <ul> <li>Does not saturate.</li> <li>Computationally efficient.</li> <li>Converges much faster than sigmoid/tanh in practice, about 6 times.</li> <li>Will not kill the gradients in negative region in comparison with the ReLU.</li> </ul> <p>Parametric Rectifier(PReLU)</p> <p>We can also use an activation function of the form: $$ f(x) = max(\\alpha x, x) $$ which means using \\(\\alpha\\) as a parameter, and let the network learn the best \\(\\alpha\\) for each neuron.</p>"},{"location":"lecture6/#exponential-linear-unitelu","title":"Exponential Linear Unit(ELU)","text":"\\[ \\text{ELU}(x) = \\begin{cases} x, &amp; x &gt; 0 \\\\ \\alpha(e^x - 1), &amp; x \\leq 0 \\end{cases} \\] <ul> <li>All benefits of ReLU.</li> <li>Closer to zero mean outputs.</li> <li>Negative saturation refime compared to leaky ReLU.</li> <li>Adds some robust to noises.</li> </ul>"},{"location":"lecture6/#maxout-neural","title":"Maxout Neural","text":"\\[ max(w_1^T x + b_1, w_2^T x + b_2) \\] <ul> <li>Does not have the basic form of dot product -&gt; nonlinearity.</li> <li>Generalizes ReLU and Leaky ReLU.</li> <li>Linear Regime! Does not saturate! Does not die!</li> </ul> <p>\u8fd9\u6837\u505a\u53ef\u4ee5\u907f\u514d\u6389\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898, \u9009\u53d6leaky relu \u548c relu\u4e2d\u8f83\u597d\u7684.</p> <p>\u4f46\u662f\u8fd9\u6837\u505a\u4f1a\u5bfc\u81f4\u6bcf\u4e2aneuron\u7684\u53c2\u6570\u4e2a\u6570\u52a0\u500d.</p>"},{"location":"lecture6/#summary","title":"Summary","text":"<ul> <li>Use ReLU. Be careful with your learning rates.</li> <li>Try out Leaky ReLU / Maxout / ELU.</li> <li>Try out tanh, but don't expect much.</li> <li>Don't use sigmoid.</li> </ul>"},{"location":"lecture6/#data-preprocessing","title":"Data Preprocessing","text":"data_preprocessing<pre><code>X -= np.mean(X, axis=0) # \u53d6Column\u7684\u5e73\u5747\u503c\nX /= np.std(X, axis=0)\n</code></pre> <p>\u5bf9\u4e8e\u6240\u6709training data, \u5728\u6bcf\u4e2a\u50cf\u7d20\u4f4d\u7f6e\u53d6\u5e73\u5747, \u5e76\u51cf\u53bb\u6539\u5e73\u5747\u503c, \u6700\u540e\u9664\u4ee5\u6807\u51c6\u5dee.</p> <p>\u4f46\u662fCV\u91cc\u9762\u4e00\u822c\u90fd\u662f\u53ea\u51cf\u53bb\u5e73\u5747\u5373\u53ef, \u4e0d\u9700\u8981\u9664\u4ee5\u6807\u51c6\u5dee, \u56e0\u4e3a\u50cf\u7d20\u503c\u7684\u8303\u56f4\u90fd\u662f\u6bd4\u8f83\u7edf\u4e00\u7684.</p> <p>Some intuition about why we need to zero-center and normalize the data</p> <p> \u53ef\u4ee5\u770b\u5230\u5f53\u6570\u636e\u88abzero-center\u4e4b\u540e, \u6211\u4eec\u7a0d\u5fae\u6270\u52a8\u4e00\u4e0b\u8fd9\u6761\u5206\u7c7b\u7684\u76f4\u7ebf, \u5206\u7c7b\u7ed3\u679c(loss)\u57fa\u672c\u4e0d\u53d8, \u4f46\u662f\u5f53\u6570\u636e\u6ca1\u6709\u88abzero-center\u65f6, \u6211\u4eec\u7a0d\u5fae\u6270\u52a8\u4e00\u4e0b\u8fd9\u6761\u5206\u7c7b\u7684\u76f4\u7ebf, \u5206\u7c7b\u7ed3\u679c(loss)\u53d8\u5316\u8f83\u5927.</p>"},{"location":"lecture6/#weight-initialization","title":"Weight Initialization","text":""},{"location":"lecture6/#what-happens-when-w0-initialization-is-used","title":"What happens when \\(W=0\\) initialization is used?","text":"<p>\u6211\u4eec\u901a\u8fc7\u8fd9\u5f20\u56fe\u6765\u89e3\u91ca, \u4f46\u662f\u5047\u8bbe\u6709\u4e24\u5c42hidden layer. Input layer \u548c hidden layer\u4e4b\u95f4\u5e94\u5f53\u5b58\u5728\u4e00\u4e2a\\(W_1\\)\u77e9\u9635, \u4e24\u4e2ahidden layer\u4e4b\u95f4\u5e94\u5f53\u5b58\u5728\u4e00\u4e2a\\(W_2\\)\u77e9\u9635, hidden layer \u548c output layer\u4e4b\u95f4\u5e94\u5f53\u5b58\u5728\u4e00\u4e2a\\(W_3\\)\u77e9\u9635. \u5f53\\(W=0\\)\u65f6, \\(W_1\\)\u548c\\(W_2\\)\u90fd\u662f0\u77e9\u9635. \u5047\u8bbe\u4e24\u4e2ahidden layer\u7684activation function \u5206\u522b\u662f\\(f_1\\)\u548c\\(f_2\\).</p> \\[ X^TW_1 = 0 \\] \\[ f_1(X^TW_1) = f_1(0) \\] \\[ f_1(0)W_2 = 0 \\] \\[ f_2(f_1(0)W_2) = f_2(0) \\] <p>\\(f(0)\\)\u4ee3\u8868\u7531\u6fc0\u6d3b\u51fd\u6570\u57280\u5904\u7684\u6570\u503c\u7ec4\u6210\u7684\u5411\u91cf, \u5927\u5c0f\u4e0e\u8be5layer\u8f93\u51fa\u5927\u5c0f\u4e00\u81f4.</p> <p>\u8fd9\u4e24\u4e2a\u7b49\u5f0f\u8bf4\u660e\u4e86\u5728\u6bcf\u4e2ahidden layer\u7684\u8f93\u51fa\u5c31\u662f\u6bcf\u4e2a\u5143\u7d20\\(f(0)\\)\u5217\u5411\u91cf, \u8fd9\u6837\u5728\u65b9\u5411\u4f20\u64ad\u6c42\\(W\\)\u68af\u5ea6\u65f6, \u4f1a\u5bfc\u81f4\u5728\u4e00\u4e2a\\(W_i\\)\u77e9\u9635\u5185, \u6bcf\u4e2a\\(W_{i, j}\\)\u7684\u68af\u5ea6\u90fd\u76f8\u540c, \u5373\u8fd9\u4e9b\u53c2\u6570\u5411\u7740\u76f8\u540c\u65b9\u5411\u53d8\u5316, \u8fd9\u663e\u7136\u4e0d\u662f\u6211\u4eec\u9700\u8981\u7684\u8bad\u7ec3\u6548\u679c.</p>"},{"location":"lecture6/#small-random-numbers","title":"Small Random Numbers","text":"<p>Therefore, we still want the weights to be very close to zero, but as we have argued above, not identically zero. As a solution, it is common to initialize the weights of the neurons to small numbers and refer to doing so as symmetry breaking. \u53bb\u9664\u6743\u91cd\u53c2\u6570\u4e4b\u95f4\u7684\u5bf9\u79f0\u6027.</p> <p>The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. </p> <p>We can use <code>0.01 * np.random.randn(D, H)</code> to implement this, where <code>D</code> is the input dimension, <code>H</code> is the hidden layer dimension and <code>randn</code> samples from a standard normal distribution.</p> <p>Warning</p> <p>It\u2019s not necessarily the case that smaller numbers will work strictly better. For example, a Neural Network layer that has very small weights will during backpropagation compute very small gradients on its data (since this gradient is proportional to the value of the weights). This could greatly diminish the \u201cgradient signal\u201d flowing backward through a network, and could become a concern for deep networks.</p>"},{"location":"lecture6/#xavier-initialization","title":"Xavier Initialization","text":"<p>\u5982\u56fe, \u4e0a\u8ff0\u65b9\u6cd5\u7684\u95ee\u9898\u5728\u4e8e, \u5047\u8bbe\u6211\u4eec\u521d\u59cb\u7684\u6570\u636e\u662f\u6ee1\u8db3\u4e00\u5b9a\u6b63\u6001\u5206\u5e03\u7684, \u968f\u7740\u4e00\u5c42\u4e00\u5c42\u7f51\u7edc\u7684\u4f20\u64ad, \u5747\u503c\u53d8\u5316\u4e0d\u5927, \u4f46\u662f\u6807\u51c6\u5dee\u663e\u8457\u4e0b\u964d, \u5bfc\u81f4\u9760\u540e\u7684layer\u63a5\u6536\u5230\u7684\u6570\u636e\u4e0d\u518d\u662f\u8fd1\u4f3c\u6b63\u6001\u5206\u5e03\u7684\u6570\u636e.</p> <p>It turns out that we can normalize the variance of each neuron\u2019s output to 1 by scaling its weight vector by the square root of its fan-in (i.e. its number of inputs). That is, the recommended heuristic is to initialize each neuron\u2019s weight vector as: <code>W = np.random.randn(n) / sqrt(n), where</code>n` is the number of inputs of this layer. This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence.</p> <p>Info</p> <p>Some people recommends an initialization of the form: <code>W = np.random.randn(n) / sqrt((n_in + n_out) / 2)</code>, where <code>n_in</code> and <code>n_out</code> are the number of units in the previous layer and the next layer.</p> <p>Warning</p> <p>When using ReLU, <code>W = np.random.randn(n) / sqrt(n/2.0)</code> is a better choice.</p> <p>Info</p> <p>A picture to sumarize this.  \u76f4\u65b9\u56fe\u663e\u793a\u7684\u662f\u6bcf\u4e2alayer\u8f93\u5165\u503c\u7684\u5206\u5e03.</p>"},{"location":"lecture6/#bias-initialization","title":"Bias Initialization","text":"<ul> <li>It's common to initialize the biases to zero.</li> <li>Some people may initializa the biases to be a small positive value (e.g. 0.01) when the activation function is ReLU, to avoid \"dead neurons\" (neurons that never activate).</li> </ul>"},{"location":"lecture6/#batch-normalization","title":"Batch Normalization","text":"<p>You want unit gaussian activations? Just make them so.</p> <p>\u5728\u6bcf\u4e2alayer\u540e\u9762\u52a0\u4e0a\u4e00\u6b65\u53d8\u6362, \u5c06\u8be5layer\u7684\u8f93\u51fa\u8f6c\u6362\u6210\u6807\u51c6\u6b63\u6001\u5206\u5e03.</p> <ol> <li> <p>Compute the empirical mean and variance independently for each dimension.</p> <p></p> <p>\u6ce8\u610f\u662f\u5c06\u6240\u6709\u8bad\u7ec3\u6570\u636e\u7684\u540c\u4e00\u50cf\u7d20\u4f4d\u7f6enormalize, \u800c\u4e0d\u662f\u5728\u540c\u4e00\u5f20\u56fe\u7247\u4e0a\u7684\u6240\u6709\u50cf\u7d20\u4f4d\u7f6enormalize.</p> </li> <li> <p>Normalize</p> \\[ \\hat{x}^{(k)} = \\frac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}} \\] <p></p> <p>Question</p> <p>Do we always need the unit input to be gaussian?</p> </li> <li> <p>After normalization, allow the network to squash the range if it wants to:</p> </li> </ol> \\[ y^{(k)} = \\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)} \\] <p>Note</p> <p>\u9700\u8981\u6ce8\u610f\u7684\u662f, \u52a0\u4e0a\u8fd9\u6b65\u64cd\u4f5c, \u76f8\u5f53\u4e8e\u5b9e\u73b0\u4e86\u4e0d\u505anormaliza\u548c\u505anormaliza\u7684\u8c03\u548c, \u5bf9\u4e8e\u6709\u4e9blayer, \u53ef\u80fdnormalize\u5c31\u662f\u6700\u597d\u7684\u5904\u7406, \u90a3\u4e48\u4f1a\u5b66\u4e60\u5230\\(\\gamma = 1,\\ \\beta = 0\\), \u5373\u4e00\u4e2a\u6052\u7b49\u6620\u5c04; \u5bf9\u4e8e\u53e6\u4e00\u4e9blayer, \u53ef\u80fd\u5b8c\u5168\u4e0d\u8fdb\u884c\u5904\u7406\u7684\u5c31\u662f\u6700\u597d\u7684, \u90a3\u4e48\u4f1a\u5b66\u4e60\u5230\\(\\gamma = \\sqrt{Var[x^{(k)}]},\\ \\beta = E[x^{(k)}]\\), \u5373\u64a4\u9500\u4e86normalize\u7684\u64cd\u4f5c. \u4f46\u662f\u5bf9\u4e8e\u5927\u591a\u6570\u60c5\u51b5, \u5e94\u5f53\u662f\u5904\u4e8e\u8fd9\u4e8c\u8005\u7684\u4e2d\u95f4, \u5373\u8981\u8fdb\u884c\u4e00\u5b9a\u5904\u7406, \u4f46\u4e5f\u4e0d\u4e00\u5b9a\u662f\u5b8c\u5168\u7684\u6b63\u6001\u5206\u5e03, \u6bcf\u4e2alayer\u7684\u8fd9\u4e9b\u53c2\u6570\u7531\u8bad\u7ec3\u8fc7\u7a0b\u51b3\u5b9a.</p> <ul> <li>Improve the gradient flow through the network.</li> <li>Allows higher learning rates.</li> <li>Reduces the strong dependence on the initialization.</li> <li>Acts as a form of regularization, reducing the need for dropout, maybe.</li> </ul> <p>\u89e3\u91ca\u4e00\u4e0b\u6700\u540e\u4e00\u70b9, \u56e0\u4e3a\u6bcf\u4e2a\u50cf\u7d20\u4f4d\u7f6e\u90fd\u51cf\u53bb\u4e86\u8be5\u4f4d\u7f6e\u5728\u6240\u6709\u8bad\u7ec3\u6570\u636e\u51fa\u7684\u5e73\u5747\u503c\u5e76\u9664\u4ee5\u4e86\u6807\u51c6\u5dee, \u610f\u5473\u7740\u73b0\u5728\u8fd9\u4e2a\u4f4d\u7f6e\u4e0d\u53ea\u8003\u8651\u4e86\u81ea\u5df1\u7684\u503c, \u800c\u662f\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6743\u8861\u4e86\u5404\u4e2a\u6837\u672c\u7684\u503c, \u6240\u4ee5\u5177\u6709\u4e00\u5b9aregularization\u7684\u6548\u679c.</p> <p>Warning</p> <p> \u4e0a\u9762\u8fd9\u5f20\u56fe\u7247\u662f\u5728training\u7684\u65f6\u5019\u7684\u6d41\u7a0b, \u4f46\u662f\u5728test\u65f6, \u6709\u6240\u533a\u522b.</p> <p>The mean/std are not computed based on the batch. Instead, a single fixed empirical mean of activations during training is used.(e.g. can be estimated during training with running averages)</p>"},{"location":"lecture6/#babysitting-the-learning-process","title":"Babysitting the Learning Process","text":"<ol> <li>Preprocess the data</li> <li> <p>Choose the architecture</p> <ul> <li>\u6bd4\u5982\u795e\u7ecf\u7f51\u7edc\u6574\u4f53\u6709\u591a\u5c11\u5c42, \u6bcf\u5c42\u6709\u591a\u5c11\u795e\u7ecf\u5143, etc.</li> </ul> </li> <li> <p>Double check the loss is reasonable</p> <ul> <li>\u6bd4\u5982softmax, \u521d\u59cb\u5316\u4e4b\u540e\u7684loss\u5e94\u5f53\u662f\\(1/\\text{number of class}\\).</li> <li>\u7136\u540e\u52a0\u4e0aregularization term, loss\u5e94\u5f53\u4f1a\u6709\u6240\u4e0a\u5347.</li> </ul> </li> <li> <p>Try to train now</p> <ul> <li>Make sure that you can overfit very small protion of the training data.</li> <li>Start with small regularization and find learning rate that makes the loss go down.</li> <li>Loss barely changing: Learning rate is probably too low</li> <li>NaN almost always means high learning rate.</li> <li>Rough range for learning rate we should be cross-validating is somewhere [1e-5, 1e-3].</li> </ul> </li> </ol>"},{"location":"lecture6/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>First stage: only a few epochs to get rough idea of what params work.</p> <p>Second stage: longer running time, finer search.</p> <p>Tip for detecting explosions in the solver: If the cost is ever &gt; 3 * original cost, break out early. It's best to optimize in log space. \u610f\u601d\u5c31\u662f\u6bd4\u5982\u60f3\u5c1d\u8bd5[1e-5, 1e-3]\u8fd9\u4e2a\u8303\u56f4, \u4e0d\u8981\u4f7f\u7528\u8fd9\u4e2a\u533a\u95f4\u5185\u90e8\u7684\u7ebf\u6027\u5206\u5e03, \u800c\u662f\u5c06\u6bcf\u6b21\u5c1d\u8bd5\u7684\u503c\u4e58\u4e00\u4e2a\u5e38\u6570\u4f5c\u4e3a\u4e0b\u6b21\u7684\u5c1d\u8bd5\u503c. \u53ef\u4ee5\u4f7f\u7528<code>np.geomspace(begin, end, num)</code>, \u751f\u6210\u4e00\u6bb5\u533a\u95f4\u5185\u7684\u7b49\u6bd4\u6570\u5217.</p> <p>Info</p> <p>\u6709\u65f6\u5019\u8c8c\u4f3c\u627e\u5230\u4e86\u5f88\u597d\u5730\u7684hyper parameter, \u4ece\u800c\u5f97\u5230\u4e86\u8f83\u9ad8\u7684validation set accuracy, \u4f46\u5176\u5b9e\u53ef\u80fd\u5b58\u5728\u6f5c\u5728\u7684\u95ee\u9898.  \u5bf9\u4e8e\u8fd9\u5f20\u56fe\u7247\u4e2d\u7684\u4f8b\u5b50, \u8f83\u597d\u7684learning rate\u90fd\u5904\u4e8e\u6211\u4eec\u9009\u5b9a\u533a\u95f4\u7684\u8fb9\u7f18, \u8fd9\u8bf4\u660e\u6211\u4eec\u6ca1\u6709\u5f88\u597d\u5730\u641c\u7d22\u6574\u4e2a\u7a7a\u95f4, \u9700\u8981\u5728\u76f8\u5e94\u8fb9\u7f18\u5904\u6269\u5927\u641c\u7d22\u8303\u56f4.</p> <p>Random Search vs. Grid Search</p> <p> \u5f53\u8c03\u6574\u7684\u4e24\u4e2ahyperparameter\u65f6, \u5982\u679c\u5176\u4e2d\u4e00\u4e2a\u8f83\u4e3a\u91cd\u8981, \u53e6\u5916\u4e00\u4e2a\u4e0d\u90a3\u4e48\u91cd\u8981, random layout\u53ef\u80fd\u4f1a\u66f4\u6709\u4f18\u52bf, \u56e0\u4e3a\u5728\u540c\u6837\u53d6\u4e5d\u7ec4\u53c2\u6570\u503c\u7684\u60c5\u51b5\u4e0b, \u5bf9\u4e8e\u6bcf\u4e2ahyperparameter, random layout\u53ef\u4ee5\u53d6\u5230\u66f4\u591a\u4e0d\u540c\u7684parameter\u503c, \u800cgrid layout\u53ea\u80fd\u53d6\u5230\u4e09\u4e2a\u4e0d\u540c\u503c.</p> <p>It's always a good idea to visulize the loss curve.</p> <p></p> <p>Question</p> <p>What's happening here?  \u53c2\u6570\u521d\u59cb\u5316\u505a\u7684\u4e0d\u597d, \u4e00\u5f00\u59cb\u5f88\u96be\u627e\u5230\u6b63\u786e\u7684\u524d\u8fdb\u65b9\u5411\u6765\u51cf\u5c11loss, \u540e\u6765\u67d0\u4e00\u65f6\u523b\u8d70\u4e0a\u4e86\u6b63\u786e\u9053\u8def, loss\u5f00\u59cb\u4e0b\u964d, \u56de\u5f52\u6b63\u8f68.</p> <p>Info</p> <p></p> <p>We can also track the ratio of weight updates/ Weight magnitude. \u8ba1\u7b97L2 Norm\u4e4b\u6bd4.</p> <pre><code>param_scale = np.linalg.norm(W.raval())\nupdate = -learning_rate*dW\nupdate_scale = np.linalg.norm(update.ravel())\nW += update\nprint(update_scale/param_scale)\n</code></pre> <p>We want this ratio to be around 0.001 or so.</p>"},{"location":"lecture6/#summary_1","title":"Summary","text":"<ul> <li>Activation Functions(use ReLU)</li> <li>Data Preprocessing(images: subtract mean)</li> <li>Wieght Initialization(use Xavier init)</li> <li>Batch Normalization(use)</li> <li>Babysitting the Learning Process</li> <li>Hyperparameter Optimization(random sample hyperparameters, in log space when appropriate)</li> </ul>"},{"location":"lecture7/","title":"Lecture 7: Training Neural Networks, Part 2","text":""},{"location":"lecture7/#fancier-optimization","title":"Fancier Optimization","text":""},{"location":"lecture7/#problem-with-sgd-optimization","title":"Problem with SGD Optimization","text":"<ul> <li> <p>What if loss changes quickly in one direction and slowly in another? What does gradient descent do?     This means loss function has high condition number: ratio of largest to smallestsingular value of the Hessian matrix is large. \u5176\u5b9e\u5c31\u662f\u68af\u5ea6\u6700\u5927/\u6700\u5c0f\u65b9\u5411\u7684\u68af\u5ea6\u4e4b\u6bd4.</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230, \u5bf9\u4e8eSGD\u6765\u8bb2, \u4e00\u5f00\u59cb\u5728\u5de6\u53f3\u4e24\u8fb9\u5411\u4e0a\u7684\u68af\u5ea6\u8f83\u5927, \u524d\u540e\u4e24\u8fb9\u5411\u4e0b\u7684\u68af\u5ea6\u8f83\u5c0f, \u90a3\u4e48\u524d\u540e\u65b9\u5411\u5f80\u4e0b\u8d70\u7684\u5206\u91cf\u4f1a\u6bd4\u8f83\u5c0f, \u5bfc\u81f4\u6700\u5f00\u59cbSGD\u4f1azig-zag\u632f\u8361, \u663e\u7136\u8fd9\u6837\u4e0d\u5982\u76f4\u63a5\u8d70\u76f4\u7ebf\u6765\u5f97\u6709\u6548\u7387.</p> <p>Very slow progress along shallow dimension, jitter along steep direction.</p> </li> <li> <p>What if the loss function has a local minima or saddle point?      </p> <p>Saddle points much more common in high dimension. \u56e0\u4e3a\u5728\u9ad8\u7ef4\u7a7a\u95f4\u5185, local minima\u9700\u8981\u6240\u6709\u53c2\u6570\u90fd\u5728\u6b64\u5904\u8fbe\u5230\u5c40\u90e8\u6700\u5c0f, \u5373\u4e24\u8fb9\u7684\u5bfc\u6570\u90fd\u5c0f\u4e8e\u96f6, \u8fd9\u6837\u7684\u6982\u7387\u5f88\u4f4e, \u4f46\u662fsaddle points\u53ea\u9700\u8981\u4e00\u8fb9\u5c0f\u4e8e\u96f6, \u53e6\u4e00\u8fb9\u5927\u4e8e\u96f6, \u56e0\u6b64saddle points\u66f4\u591a.</p> <p>Zero gradient, gradient descent gets stuck. \u5728\u8fd9\u4e24\u79cd\u4f4d\u7f6e, \u68af\u5ea6\u90fd\u6bd4\u8f83\u5c0f, gradient descent\u8fdb\u884c\u5730\u5f88\u6162! </p> </li> <li> <p>Our gradients come from minibatches so they can be noisy. \u4ecemini-batch\u4e2d\u83b7\u5f97\u7684\u68af\u5ea6\u4e0d\u4e00\u5b9a\u80fd\u771f\u6b63\u4ee3\u8868\u6574\u4e2a\u8bad\u7ec3\u96c6\u7684\u68af\u5ea6, \u56e0\u6b64\u5728\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\u65f6\u53ef\u80fd\u5b58\u5728\u6270\u52a8, \u4e0d\u80fd\u5b8c\u5168\u6309\u7167\u6b63\u786e\u8def\u5f84\u884c\u8fdb.</p> </li> </ul>"},{"location":"lecture7/#sgd-momentum","title":"SGD + Momentum","text":"\\[ v_{t+1} = \\rho v_t + \\nabla f(x_t) \\] \\[ x_{t+1} = x_t - \\alpha v_{t+1} \\] <pre><code>vx = 0 # initialize the velocity zero.\nwhile True:\n    dx = compute_gradient(x)\n    vx = rho * vx + dx\n    x += learning_rate * vx\n</code></pre> <ul> <li>Build up \"velocity\" as a running mean of gradients. \u5176\u5b9e\u9760\u524d\u7684gradient\u6743\u91cd\u5c0f\u4e8e\u540e\u9762\u7684gradient.</li> <li>Rho gives \"friction\", typically rho = 0.9 or 0.99. </li> </ul> <p>SGD + Momentum addresses all the problem of SGD !</p> <ul> <li>Poor conditioning\u7684\u60c5\u51b5\u4e0b, \u539f\u6765\u6211\u4eec\u4f1a\u5728\u4e00\u5f00\u59cb\u5728\u68af\u5ea6\u8f83\u5927\u7684\u65b9\u5411\u632f\u8361, \u4f46\u662f\u52a0\u4e0amomentum\u4e4b\u540e, \u4e24\u6b21\u9707\u8361\u7684\u68af\u5ea6\u4f1a\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u76f8\u4e92\u62b5\u6d88, \u51cf\u5c11\u632f\u8361, \u52a0\u5febgradient descent.</li> <li>\u5728local minima \u548c saddle point\u5904, \u7531\u4e8e\u6211\u4eec\u73b0\u5728\u5b58\u5728\u4e00\u4e2amomentum\u9879, \u79ef\u7d2f\u4e86\u5148\u524d\u7684\u68af\u5ea6, \u6240\u4ee5\u53ef\u4ee5\u8f83\u5feb\u7ecf\u8fc7\u68af\u5ea6\u5e73\u7f13\u7684\u533a\u57df.</li> <li>Momentum term sort of average the random gradient noise.</li> </ul>"},{"location":"lecture7/#nesterov-momentum","title":"Nesterov Momentum","text":"<p>\u533a\u522b\u5728\u4e8e, \u533a\u522b\u4e8e\u4e4b\u524d\u7684SGD + Momentum, \u8fd9\u91cc\u6211\u4eec\u5148\u6cbf\u7740velocity\u8d70\u4e00\u6b65, \u518d\u8ba1\u7b97\u90a3\u91cc\u7684\u68af\u5ea6, \u6700\u540e\u5f62\u6210actual step.</p> \\[ v_{t+1} = \\rho v_t + \\alpha \\nabla f(x_t + \\rho v_t) \\] \\[ x_{t+1} = x_t + v_{t+1} \\] <p>\u53d8\u91cf\u66ff\u6362\u4e00\u4e0b\\(\\hat{x_t} = x_t + \\rho v_t\\): $$ v_{t+1} = \\rho v_t - \\alpha \\nabla f(\\hat{x_t}) $$</p> \\[ \\hat{x_{t+1}} = \\hat{x_t} + v_{t+1} + \\rho (v_{t+1} - v_t) \\] <p>\u8fd9\u6837\u53d8\u6362\u4e4b\u540e\u7684\u4e3b\u8981\u76ee\u7684\u662f\u65b9\u4fbf\u6211\u4eec\u8ba1\u7b97\u68af\u5ea6, \u76f4\u63a5\u8ba1\u7b97\\(f(\\hat{x_t})\\)\u7684\u68af\u5ea6.</p> <pre><code>dx = compute_gradient(x)\nold_v = v\nv = rho * v - laerning_rate * dx\nx += v + rhp(v - old_v)\n</code></pre> <p>Notes</p> <p>\u5176\u5b9eNesterov Momentum\u76f8\u6bd4\u6700\u57fa\u672c\u7684Momentum, \u5c31\u662f\u52a0\u4e0a\u4e86\\(\\rho (v_{t+1}-v_t)\\)\u8fd9\u4e00\u9879, \u8fd9\u4e00\u9879\u7684\u4f5c\u7528\u53ef\u4ee5\u4f7f\u5f97\u6211\u4eec\u5f53\u524d\u8d70\u51fa\u7684\u4e00\u6b65\u66f4\u591a\u5730\u53c2\u8003\u4e86\u4e00\u70b9\u4e4b\u524d\u7684\u901f\u5ea6, \u4ece\u800c\u9632\u6b62\u592a\u6fc0\u8fdb\u7684\u8d70\u6cd5. \u4f46\u9700\u8981\u6ce8\u610f\u7684\u662f, \u539f\u59cb\u7684Momentum\u5df2\u7ecf\u5728\u8d70\u5f53\u524d\u6b65\u9aa4\u65f6\u53c2\u8003\u4e86\u4e4b\u524d\u6b65\u9aa4, \u800c\u73b0\u5728\u6211\u4eec\u76f8\u5f53\u4e8e\u662f\u7ed9\u4e4b\u524d\u7684\u901f\u5ea6\u52a0\u4e86\u4e00\u4e9b\u6743\u91cd.  \u7531\u8fd9\u5f20\u5bf9\u6bd4\u56fe\u53ef\u4ee5\u770b\u51fa, \u6dfb\u52a0\u4e86Momentum\u7684optimization\u7b97\u6cd5\u6548\u7387\u663e\u8457\u9ad8\u4e8e\u666e\u901aSGD, \u4f46\u4ed6\u4eec\u90fd\u5b58\u5728\u4e00\u4e2a\u8d70\u8fc7\u4e86\u6700\u4f18\u70b9\u4e4b\u540e\u518d\u7ea0\u6b63\u653e\u56de\u8d70\u7684\u8fc7\u7a0b, \u4f46\u662fNesterov Momentum\u6ca1\u6709SGD + Momentum\u90a3\u4e48\u6fc0\u8fdb.</p>"},{"location":"lecture7/#adagrad","title":"AdaGrad","text":"<p>Added element-wise scaling of the gradient based on the historical sum of squares in each dimension.</p> <pre><code>while True:\n    dx = compute_gradient(x)\n    grad_squared += dx * dx\n    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)\n</code></pre> <ul> <li>Intuitely, this algorithm makes the step size in every dimension closer, since all of the gradients are divided by \\(dx*dx\\).</li> <li>But the step size of each dimension will be smaller and smaller during the training time, since <code>grad_squared</code> is monotonically increasing.</li> <li>AdaGrad is not very popular in practice, because it makes the step size too small. But we have a modified version.</li> </ul>"},{"location":"lecture7/#rmsprop","title":"RMSProp","text":"<pre><code>grad_squared = 0\nwhile True:\n    dx = compute_gradient(x)\n    grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dx * dx\n    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)\n</code></pre> <ul> <li>RMSProp is a modification of AdaGrad, which makes the step size decay slower.</li> </ul> <p>Notes</p> <p>RMSProp\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u5f97\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u6b65\u957f\u66f4\u52a0\u5747\u5300, \u662f\u4e0e\u533a\u522b\u4e8emomentum\u7684\u53e6\u4e00\u79cd\u52a0\u901f\u601d\u8def.  \u53ef\u4ee5\u770b\u5230, RMS\u4e0d\u4f1a\u50cfmomentum\u90a3\u6837\u5148\u8d8a\u8fc7\u518d\u7ea0\u6b63.</p>"},{"location":"lecture7/#adam","title":"Adam","text":"<p>Why don't we just combine the momentum and RMSProp?</p>"},{"location":"lecture7/#vinilla-version","title":"Vinilla Version","text":"<pre><code>first_momentum = 0\nsecond_momentum = 0\nwhile True:\n    dx = compute_gradient(x)\n    first_momentum = beta1 * first_momentum + (1 - beta1) * dx\n    second_momentum = beta2 * second_momentum + (1 - beta2) * dx * dx\n    x -= learning_rate * first_momentum / (np.sqrt(second_momentum) + 1e-7)\n</code></pre> <p>However, if we implement just like this , over initial steps will be gigantic, since at that time the second momentum is relatively small.</p>"},{"location":"lecture7/#unbiased-version","title":"Unbiased Version","text":"<pre><code>first_momentum = 0\nsecond_momentum = 0\nwhile True:\n    dx = compute_gradient(x)\n    first_momentum = beta1 * first_momentum + (1 - beta1) * dx\n    second_momentum = beta2 * second_momentum + (1 - beta2) * dx * dx\n    first_unbias = first_momentum / (1 - beta1 ** t)\n    second_unbias = second_momentum / (1 - beta2 ** t)\n    x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7)\n</code></pre> <p>Info</p> <p>Adam with bata1 = 0.9, beta2 = 0.999 and learning_rate = 1e-3 or 5e-4 is a great starting point for many models.</p>"},{"location":"lecture7/#learning-rate-decay","title":"Learning Rate Decay","text":""},{"location":"lecture7/#exponential-decay","title":"Exponential Decay","text":"\\[ \\alpha = \\alpha_0 e^{-kt} \\]"},{"location":"lecture7/#1t-decay","title":"1/t Decay","text":"\\[ \\alpha = \\frac{\\alpha_0}{1 + kt} \\] <p>Info</p> <p> \u5982\u679cLoss\u4e0e\u65f6\u95f4\u7684\u5173\u7cfb\u957f\u8fd9\u4e2a\u6837\u5b50, \u8bf4\u660e\u5728\u67d0\u4e9b\u8282\u70b9\u8fdb\u884c\u4e86learning rate decay. Intuitely, \u5728\u67d0\u4e9b\u65f6\u5019loss\u4e0b\u964d\u5230\u4e00\u5b9a\u7a0b\u5ea6, \u5f53\u4e0b\u7684learning rate\u53ef\u80fd\u4f1a\u5bfc\u81f4loss\u5728\u6700\u4f18\u89e3\u9644\u8fd1\u9707\u8361, \u6240\u4ee5\u6b64\u65f6\u9700\u8981decay\u4e00\u4e0blearning rate, \u903c\u8fd1\u6700\u4f18. \u4e0d\u8fc7\u5bf9\u4e8eAdam\u8fd9\u79cdstep size\u672c\u8eab\u5c31\u4f1a\u9010\u6e10\u964d\u4f4e\u7684\u7b97\u6cd5, \u4e0d\u662f\u90a3\u4e48\u5173\u952e, \u4f46\u662f\u5bf9\u4e8emomentum\u7b97\u6cd5\u8fd8\u662f\u5341\u5206\u91cd\u8981\u7684.</p>"},{"location":"lecture7/#second-order-optimization","title":"Second Order Optimization","text":"<p>What we've talked about is all first order optimization, which is based on gradient. But there are second order optimization algorithms, which is based on Hessian matrix.</p> <p></p> <p>Failure</p> <p>\u4f46\u662f\u8fd9\u79cd\u7b97\u6cd5\u4e00\u822c\u4e0d\u4f1a\u5728deep learning\u4e2d\u4f7f\u7528, \u56e0\u4e3a\u8ba1\u7b97Hessian matrix\u7684\u590d\u6742\u5ea6\u592a\u9ad8, \u800c\u4e14\u5bf9\u4e8e\u5982\u679c\u53c2\u6570\u89c4\u6a21\u8fbe\u5230million\u7ea7\u522b, \u5185\u5b58\u4e5f\u5b58\u4e0d\u4e0b\u6574\u4e2aHessian Matrix.</p>"},{"location":"lecture7/#model-ensembles","title":"Model Ensembles","text":"<ol> <li>Train multiple independence models.</li> <li>At test time average their results.</li> </ol> <p>This will enjoy about 2% extra performance.</p> <p>Model Ensembles: Tips and Tricks</p> <p> </p>"},{"location":"lecture7/#regularization","title":"Regularization","text":"<p>How to improve a single-model performance? -- Regularization!</p> <p>We've already learnt that we can add term to loss, say L2 regularization, L1, etc.</p>"},{"location":"lecture7/#dropout","title":"Dropout","text":"<ul> <li>In each forward pass, randomly set some neurons to zero</li> <li>Probability of dropping is a hyperparameter, 0.5 is common</li> </ul> <pre><code>p = 0.5\n\ndef train_step(X):\n    \"\"\" X contains the data \"\"\"\n    H1 = np.maximum(0, np.dot(W1, X))\n    U2 = np.random.randn(*H1.shape) &gt; p # first dropout mask\n    H1 *= U2 # drop\n    U2 = np.maximum(0, np.dot(W2, H1)) # second dropout mask\n    H2 *= U2 # drop\n    out = np.dot(W3, H2) + b3\n\n    # backprop and parameter update not showm.\n</code></pre> <p>Intuition</p> <p>How can this possibly be a good idea?  \u8fd9\u6837\u505a\u53ef\u4ee5\u4f7f\u5f97\u6211\u4eec\u4e0d\u8981\u8fc7\u5206\u5730\u4f9d\u8d56\u4e00\u4e2a\u7279\u5f81\u6765\u4f30\u8ba1\u7ed3\u679c, \u800c\u662f\u5c06\u6743\u91cd\u5e73\u5747\u5206\u914d\u5230\u5404\u4e2a\u7279\u5f81\u4e0a\u53bb. \u5373\u5f53\u67d0\u4e2a\u7279\u5f81\u88abdrop\u4e4b\u540e, \u6211\u4eec\u4ecd\u80fd\u8f83\u597d\u5730\u9884\u6d4b\u7ed3\u679c.</p> <p>Another Interpretation</p> <ul> <li>Dropout is training a large ensemble of models that share parameters. </li> <li>Each binary mask is one model. </li> <li>So for a FC layer with 4096 units, it has \\(2^{4096} = 10^{1233}\\) possible masks.</li> </ul> <p>during test time</p> <p> </p> <pre><code>def predict(X):\n    #ensembled forward pass\n    H1 = np.maximum(0, np.dot(W1, X) + b1) * p # Note: Scale the activations\n    H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # Note: Scale the activations\n    out = np.dot(W3, H2) + b3\n</code></pre> <p>At test time all neurons are active always, so we must scale the activations so that for each neuron: output at test time = expected output at training time.</p> <p>But a more common case is that we do a \"inverted dropout\", which means we divid the each layer's activation by p dduring training, leaving testing part unchanged.</p>"},{"location":"lecture7/#data-augmentation","title":"Data Augmentation","text":"<ul> <li>Horizontal Flips</li> <li> <p>Random Crops and Scales     Training: sample random crops / scales.     Testing: average a fixed set of crops.</p> </li> <li> <p>Color Jitter     </p> </li> </ul>"},{"location":"lecture7/#transfer-learning","title":"Transfer Learning","text":"<p>You need a lot of a data if you want totrain/use CNNs.</p> <p>Transfer learning busted this sort of idea.</p> <p></p> <p>Transfer learning with CNNs is pervasive</p> <p> \u5f53\u6211\u4eec\u62ff\u5230\u624b\u4e00\u4e2a\u4efb\u52a1\u65f6, \u4e00\u822c\u53ef\u4ee5\u5148\u627e\u627e\u6709\u6ca1\u6709\u5728\u7c7b\u4f3c\u4efb\u52a1\u4e0a\u8bad\u7ec3\u8fc7\u7684\u795e\u7ecf\u7f51\u7edc, \u6709\u7684\u8bdd\u6211\u4eec\u5176\u5b9e\u53ea\u9700\u8981\u62ff\u8fc7\u6765\u5c06\u6700\u540e\u51e0\u5c42\u91cd\u65b0\u8bad\u7ec3\u4e00\u4e0b\u5373\u53ef.</p> <p>Info</p> <p>Deep learning frameworks(Pytorch/TensorFlow/Caffe) provide a \u201cModel Zoo\u201d of pretrained models so you don\u2019t need to train your own.</p>"},{"location":"lecture8/","title":"Lecture 8: Deep Learning Software","text":""},{"location":"lecture8/#cpu-vs-gpu","title":"CPU vs GPU","text":"<p>In deep learning, NVIDIA is dominant, we dont't consider AMD.</p> Comparison between CPU and GPU <p></p>"},{"location":"lecture8/#deep-learning-frameworks","title":"Deep Learning Frameworks","text":"<p>2017\u5e74\u7684\u8bfe\u7a0b\u7565\u6709\u4e9b\u8fc7\u65f6\u4e86, \u8fd9\u91cc\u6211\u4e3b\u8981\u8bb0\u5f55\u4e00\u4e0bPytorch.</p> <p>The point of deep learning frameworks</p> <ul> <li>Easily build big computational graphs.</li> <li>Easily compute gradients in computational graphs.</li> <li>Run it all efficiently on GPU.</li> </ul> <p>In PyTorch, we define <code>Variable</code> to start building a computational graph.</p> <p></p> <p>Calling <code>c.backward()</code> computes all gradients.</p> <p>Run on GPU by casting to <code>.cuda()</code>, just like: <code>x = variable(torch.randn(N, D).cuda(), requires_grad=True)</code>.</p>"},{"location":"lecture8/#pytorch","title":"PyTorch","text":"<p>Three levels of abstraction:</p> <ol> <li> <p>Tensor: Imperative ndarray(\u547d\u4ee4\u5f0f\u7f16\u7a0b\u8303\u5f0f\u4e0b\u4f7f\u7528\u7684\u591a\u7ef4\u6570\u7ec4), but runs on GPU.</p> <p></p> <p>To run on GPU, just cast tensors to a cuda datatype! <code>dtype = torch.cuda.FloatTensor</code>.</p> </li> <li> <p>Variable: Node in a computational graph, stores data and gradient.</p> <p></p> <ul> <li>New Autograd Functions      You can define your own autograd functions by writing forward and backward for Tensors.     </li> </ul> </li> <li> <p>Module: A neural network layer, may store state or learnable weights.</p> <p></p> <ul> <li> <p>We can also use an optimizer for different update rules: <code>optimizer = torch.optim.Adam(model.prameters(), lr = learning_rate)</code>  \u6700\u540e\u9700\u8981\u4f7f\u7528<code>optimizer.step()</code>\u5bf9\u53c2\u6570\u8fdb\u884c\u66f4\u65b0.</p> </li> <li> <p>Define New Modules          No need to define backward, autograd will handle it.</p> </li> <li> <p>Dataset Loader          Iterate over loader to form minibatches.</p> </li> </ul> </li> </ol> <p>\u5728PyTorch\u4e2d\uff0c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u9700\u8981\u7ee7\u627f<code>torch.utils.data.Dataset</code>\u7c7b\uff0c\u5e76\u5b9e\u73b0\u4e09\u4e2a\u65b9\u6cd5\uff1a<code>__init__</code>\u3001<code>__len__</code>\u548c<code>__getitem__</code>\u3002\u4ee5\u4e0b\u662f\u4e00\u4e2a\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7c7b\u7684\u57fa\u672c\u793a\u4f8b\uff0c\u5305\u62ec\u5982\u4f55\u4f7f\u7528PyTorch\u7684DataLoader\u8fdb\u884c\u6570\u636e\u52a0\u8f7d(\u5047\u8bbe\u6211\u4eec\u4eceCSV\u6587\u4ef6\u4e2d\u8bfb\u53d6\u6570\u636e):</p> <pre><code>class CustomDataset(Dataset):\n    def __init__(self, csv_file, transform=None):\n        # \u8bfb\u53d6CSV\u6587\u4ef6\n        self.data_frame = pd.read_csv(csv_file)\n        self.transform = transform\n        self.scaler = StandardScaler()\n\n        # \u5047\u8bbe\u6570\u636e\u96c6\u7684\u6700\u540e\u4e00\u5217\u662f\u6807\u7b7e\uff0c\u5176\u4f59\u5217\u662f\u7279\u5f81\n        self.features = self.data_frame.iloc[:, :-1].values\n        self.labels = self.data_frame.iloc[:, -1].values\n\n        # \u6807\u51c6\u5316\u7279\u5f81\n        self.features = self.scaler.fit_transform(self.features)\n\n    def __len__(self):\n        # \u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\n        return len(self.data_frame)\n\n    def __getitem__(self, idx):\n        # \u83b7\u53d6\u6307\u5b9a\u7d22\u5f15\u7684\u6570\u636e\u548c\u6807\u7b7e\n        features = self.features[idx]\n        label = self.labels[idx]\n\n        # \u5982\u679c\u6709\u6570\u636e\u8f6c\u6362\u64cd\u4f5c\uff0c\u5e94\u7528\u8f6c\u6362\n        if self.transform:\n            features = self.transform(features)\n\n        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n</code></pre> <p>Pretrained Models</p> <p></p>"},{"location":"lecture9/","title":"Lecture 9: CNN Architecture","text":""},{"location":"lecture9/#ilsvrc","title":"ILSVRC","text":"<p>We focus on AlexNet, VGGNet, GooleNet and ResNet.</p>"},{"location":"lecture9/#alexnet","title":"AlexNet","text":"<ul> <li> <p>AlexNet\u770b\u4f3c\u5c06\u6574\u4e2a\u7f51\u7edc\u5206\u6210\u4e86\u4e0a\u4e0b\u4e24\u90e8\u5206, \u8fd9\u662f\u56e0\u4e3a\u57282012\u5e74\u5de6\u53f3, \u4f5c\u8005\u4f7f\u7528\u7684GPU\u7684\u5185\u5b58\u4e0d\u80fd\u5b58\u4e0b\u6574\u4e2a\u7f51\u7edc\u7684\u53c2\u6570, \u6240\u4ee5\u9700\u8981\u5206\u6210\u4e24\u90e8\u5206. \u6211\u4eec\u53ef\u4ee5\u770b\u5230CONV1\u4e4b\u540e\u7684depth\u5e94\u5f53\u662f96(96\u4e2afilter), \u6240\u4ee5\u4e24\u90e8\u5206\u5206\u522b\u5b58\u4e8664\u5c42.    Historical note: Trained on GTX 580 GPU with only 3 GB of memory. Network spread across 2 GPUs, half the neurons (feature maps) on each GPU.</p> </li> <li> <p>CONV1, CONV2, CONV4, CONV5: Connections only with feature maps on same GPU.</p> </li> <li>CONV3, FC6, FC7, FC8: Connections with all feature maps in preceding layer, communication across GPUs.</li> </ul>"},{"location":"lecture9/#vggnet","title":"VGGNet","text":"<p>Small filters, deeper networks.</p> <p></p> <p>Why use smaller fliters?(3*3 conv)</p> <p> Stack of three 3*3 conv(stride 1) layers has the same effective receptive field as one 7*7 conv layer. But deeper, and more non-linearities. \u4e09\u5c423*3\u7684\u5377\u79ef\u6838\u5b9e\u9645\u4e0a\u5f62\u6210\u4e86\u4e00\u4e2a\u91d1\u5b57\u5854\u5f62\u7684\u7ed3\u6784, \u80fd\u591f\u91c7\u96c6\u5230\u539f\u59cb\u56fe\u50cf7*7\u7684\u4fe1\u606f</p> <p>Also, fewer parameters: \\(3*(3^2 C^2)\\) vs. \\(7^2 C^2\\) for C channels per layer. \u6bcf\u4e2a\u5377\u79ef\u6838\u5927\u5c0f\u4e3a33C, \u5171\u6709C\u4e2a, \u7136\u540e\u603b\u51713\u5c42.</p>"},{"location":"lecture9/#googlenet","title":"GoogleNet","text":"<p>Deeper networks, with computational efficinecy.</p> <p></p>"},{"location":"lecture9/#inception-module","title":"Inception Module","text":"<p>Design a good local network topology(network within network) and then stack modules on top of each other.</p> <p></p> <p>\u6ce8\u610f, \u4e0a\u56fe\u4e2d\u7684\u7b2c2, 3, 4\u5217\u76841*1 convolution layer, \u88ab\u79f0\u4e3abottleneck layer, \u4f5c\u7528\u662f\u51cf\u5c11\u8ba1\u7b97\u91cf, \u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u964d\u7ef4. \u4f7f\u7528\u4e00\u5b9a\u91cf\u76841*1\u5377\u79ef\u6838, \u53ef\u4ee5\u5728\u4fdd\u7559\u539f\u56fe\u5c3a\u5bf8\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u5c42\u6570(depth), \u9632\u6b62computationally expensive.</p> Without bottleneck layers <p> \u52a0\u4e0abottleneck\u4e4b\u540e, \u8ba1\u7b97\u91cf\u4f1a\u51cf\u5c0f\u52301/3\u5de6\u53f3. </p>"},{"location":"lecture9/#overall-hierachy","title":"Overall Hierachy","text":"<p>\u53ef\u4ee5\u770b\u5230, \u5927\u81f4\u7ed3\u6784\u4e3a:</p> <ul> <li>\u6700\u5de6\u7aef\u7684stem network, \u7531\u4f20\u7edf\u7684\u5377\u79ef\u5c42\u548c\u6c60\u5316\u5c42\u7ebf\u6027\u7ec4\u6210.</li> <li>\u4e2d\u95f4\u7684stacked inception modules, \u5373\u5f88\u591ainception module\u5806\u53e0\u5728\u4e00\u8d77.</li> <li>\u6700\u53f3\u7aef\u7684classifier output, \u8fd9\u91cc\u53bb\u9664\u4e86\u4f20\u7edfCNN\u5728\u672b\u5c3e\u7684\u591a\u4e2a\u5168\u8fde\u63a5\u5c42, \u53ea\u4f7f\u7528\u4e86\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u7528\u6765\u8f93\u51fascore.</li> <li>\u5f88\u5173\u952e\u7684\u4e00\u4e2a\u70b9\u5728\u4e8e\u4e2d\u95f4\u7684auxiliary classification output, \u8fd9\u662f\u4e3a\u4e86\u9632\u6b62\u68af\u5ea6\u6d88\u5931, \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u4f1a\u5bf9\u8fd9\u4e2a\u8f85\u52a9\u5206\u7c7b\u5668\u8fdb\u884c\u53cd\u5411\u4f20\u64ad, \u4f46\u662f\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d, \u8fd9\u4e2a\u8f85\u52a9\u5206\u7c7b\u5668\u4f1a\u88ab\u5ffd\u7565.</li> </ul>"},{"location":"lecture9/#resnet","title":"ResNet","text":"<p>Very deep networks using residual connections.</p> <p></p> <p>Hypothesis</p> <ul> <li>The problem is an optimization problem, deeper models are harder to optimize.</li> <li>The deeper model should be able to perform at least as well as the shallower model.</li> <li>A solution by construction is copying the learned layers from the shallower model and setting additional layers to identity mapping.</li> </ul> <p>Solution to train a deeper model: Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping.</p> <p></p> <p>\u5e38\u89c4\u7684CNN\u90fd\u662f\u5e0c\u671b\u4e00\u4e2alayer\u7684\u8f93\u51fa \\(H(x)\\) \u80fd\u591f\u903c\u8fd1\u771f\u5b9e\u503c \\(F(x)\\), \u800cResNet\u5219\u662f\u5e0c\u671blayer\u7684\u8f93\u51fa \\(H(x)\\) \u80fd\u591f\u903c\u8fd1\u6b8b\u5dee \\(F(x) - x\\), \u5373 \\(H(x) = F(x) - x\\), \u8fd9\u6837\u7684\u8bdd, \u6211\u4eec\u53ea\u9700\u8981\u8bad\u7ec3 \\(F(x)\\) \u5c31\u53ef\u4ee5\u4e86, \u800c\u4e0d\u9700\u8981\u8bad\u7ec3 \\(H(x)\\), \u56e0\u4e3a \\(H(x)\\) \u53ef\u4ee5\u76f4\u63a5\u7531 \\(F(x)\\) \u548c \\(x\\) \u76f8\u52a0\u5f97\u5230. \u8fd9\u91cc \\(H(x)\\) \u548c \\(F(x)\\) \u5206\u522b\u6307\u7684\u662flayer\u5b9e\u9645\u7684\u8f93\u51fa\u5173\u4e8e\u8f93\u5165\u7684\u51fd\u6570, \u548c\u7406\u60f3\u7684\u8f93\u51fa\u5173\u4e8e\u8f93\u5165\u7684\u51fd\u6570.</p> <p>\u4e00\u70b9intuition, \u8fd9\u6837\u5b66\u4e60\u7684\u8bdd\u5c31\u53ef\u4ee5\u7b26\u5408hypothesis, \u5373deeper model should be able to perform at least as well as the shallower model, \u56e0\u4e3a\u5982\u679cshalow layer\u5df2\u7ecf\u5b66\u5f97\u5f88\u597d\u4e86\u90a3\u4e48\u540e\u9762\u7684layer\u53ef\u4ee5\u5b66\u4e60\u51fa\\(F(x) = 0\\), \u5373 \\(H(x) = x\\), \u8fd9\u6837\u7684\u8bdd, \u6574\u4e2a\u7f51\u7edc\u5c31\u76f8\u5f53\u4e8e\u4e00\u4e2a\u6d45\u5c42\u7f51\u7edc. \u8fd9\u8bf4\u660e\u4e86\u7ecf\u8fc7\u67d0\u5c42layer\u4e4b\u540e, \u7ed3\u679c\u4e00\u5b9a\u4e0d\u4f1a\u6bd4\u4e4b\u524d\u574f.</p> <p></p> <p>For deeper networks, use \"bottleneck\" layer to improve efficiency, very similar to GooleNet.</p> <p></p> <p>\u6ce8\u610f, \u56e0\u4e3a\u6bcf\u4e2a\u6b8b\u5dee\u7ed3\u6784\u7684\u8f93\u51fa\u9700\u8981\u548c\u8f93\u5165\u76f8\u52a0, \u6240\u4ee5\u5e94\u5f53\u4fdd\u8bc1\u4e8c\u8005\u5c3a\u5bf8\u76f8\u5f53.</p>"},{"location":"Games101/games101/","title":"GAMES101: \u73b0\u4ee3\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u5165\u95e8","text":"<p>Course Website</p> <p>\u672c\u8bfe\u7a0b\u4e3a2020\u5e74\u75ab\u60c5\u671f\u95f4\u5f00\u8bbe\u7684\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u57fa\u7840\u8bfe, \u7531\u4e8e\u611f\u89c9\u5b66\u6821\u5f00\u8bbe\u7684\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u8003\u6838\u8981\u6c42\u592a\u591a, \u6240\u4ee5\u9009\u62e9\u4e86\u8fd9\u95e8\u7ebf\u4e0a\u8bfe\u8fdb\u884c\u81ea\u5b66.</p>"},{"location":"Games101/lecture2/","title":"Lecture 2: Review of Linear Algebra","text":"<p>\u8fd9\u8282\u8bfe\u5f88\u7b80\u5355, \u7b80\u5355\u8bb0\u5f55\u51e0\u70b9.</p>"},{"location":"Games101/lecture2/#cross-product","title":"Cross Product","text":"\\[ || \\vec{a} \\times \\vec{b} || = || \\vec{a} || \\cdot || \\vec{b} || \\cdot \\sin \\theta \\] \\[ \\vec{a} \\times \\vec{b} = \\vec{b} \\times \\vec{a} \\] <ul> <li> <p>Cross product is orthogonal to two initial vectors.</p> </li> <li> <p>Direction determined by right-hand rule.</p> </li> <li> <p>Useful in constructing coordinate system(\u9ed8\u8ba4\u53f3\u624b\u7cfb)</p> </li> </ul> \\[ \\vec{x} \\times \\vec{y} = \\vec{z} \\] \\[ \\vec{y} \\times \\vec{z} = \\vec{x} \\] \\[ \\vec{z} \\times \\vec{x} = \\vec{y} \\] <ul> <li>\u77e9\u9635\u5f62\u5f0f:</li> </ul> <p></p> <ul> <li>Determin left/right, inside/outside.</li> </ul> <p></p> <p>\u6bd4\u5982\u5bf9\u4e8e\u8fd9\u4e2a\u4e09\u89d2\u5f62, \u6211\u4eec\u60f3\u5224\u65ad\u70b9P\u662f\u5426\u5728\u4e09\u89d2\u5f62\u5185, \u6211\u4eec\u53ef\u4ee5\u5206\u522b\u8ba1\u7b97AP &amp; AB, BP &amp; BC \u548c CA &amp; CP\u7684\u53c9\u79ef, \u5982\u679c\u7ed3\u679c\u90fd\u5927\u4e8e0, \u5219P\u5728\u4e09\u89d2\u5f62\u5185.</p>"},{"location":"Games101/lecture3/","title":"Lecture 3 &amp; 4: Transformation","text":""},{"location":"Games101/lecture3/#homogeneous-coordinates","title":"Homogeneous Coordinates","text":"<p>Translation cannot be represented by linear transformation.</p> \\[ \\begin{pmatrix} x^{'} \\\\ y^{'} \\end{pmatrix} =  \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} + \\begin{pmatrix} t_x \\\\ t_y \\end{pmatrix} \\] <p>So we add a third coordinate, which is w-coordinate.</p>"},{"location":"Games101/lecture3/#matrix-representation","title":"Matrix Representation","text":"<ul> <li>2D point: \\((x, y, 1)^T\\)</li> </ul> \\[ \\begin{pmatrix} x^{'} \\\\ y^{'} \\\\ w^{'} \\\\ \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 &amp; t_x \\\\ 0 &amp; 1 &amp; t_y \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ 1 \\end{pmatrix} \\] <ul> <li>2D vector: \\((x, y, 0)^T\\)</li> </ul> \\[ \\begin{pmatrix} x \\\\ y \\\\ 0 \\\\ \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 &amp; t_x \\\\ 0 &amp; 1 &amp; t_y \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ 0 \\end{pmatrix} \\] <p>\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u770b\u5230, \u4e4b\u6240\u4ee5\u5c06vector\u7684w-coordinate\u8bbe\u4e3a0, \u662f\u56e0\u4e3avector\u5177\u6709\u5e73\u79fb\u4e0d\u53d8\u6027.</p>"},{"location":"Games101/lecture3/#valid-operation","title":"Valid Operation","text":"<ul> <li>point + vector = point</li> <li>vector + vector = vector</li> <li>point - point = vector</li> <li>point + point = undefined(invalid)</li> </ul> <p>Note</p> <p>In homogeneous coordinates: $$ (x, y, w)^T \\text{is the same 2D point as} (x/w, y/w, 1)^T \\text{, } w \\neq 0 $$</p>"},{"location":"Games101/lecture3/#affine-map","title":"Affine map","text":"<p>Affine map = linear map + translation</p> <p>Using homogeneous coordinates, we can represent affine map as a matrix multiplication:</p> \\[ \\begin{pmatrix} x^{'} \\\\ y^{'} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} a &amp; b &amp; t_x \\\\ c &amp; d &amp; t_y \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ 1 \\end{pmatrix} \\]"},{"location":"Games101/lecture3/#different-2d-transformations","title":"Different 2D Transformations","text":""},{"location":"Games101/lecture3/#3d-transformations","title":"3D Transformations","text":"<p>Same as 2D case:</p> <p></p> <p></p> <p>\u8fd9\u79cd\u77e9\u9635\u8868\u793a\u7684affine map\u662f\u5148\u8fdb\u884clinear map, \u518d\u8fdb\u884ctranslation\u7684.</p> <p>Scale and translation are basically the same as in 2D case, but rotation is much complicated:</p> <p></p> <p></p> <p>Success</p> <p>Actually, we can represent a random 3D representation by composing \\(R_x, R_y, R_z\\):</p> \\[ R(\\vec{n}, \\alpha) = \\cos(\\alpha)I + (1-\\cos(\\alpha))\\vec{n}\\vec{n}^T + \\sin(\\alpha)N \\] <p>where </p> \\[ N = \\begin{pmatrix}     0 &amp; -n_z &amp; n_y \\\\     n_z &amp; 0 &amp; -n_x \\\\     -n_y &amp; n_x &amp; 0 \\\\ \\end{pmatrix} \\] <p>This formula is called Rodrigues' rotation formula.</p>"},{"location":"Games101/lecture3/#viewing-transformations","title":"Viewing Transformations","text":"<p>Think about how to take a photo:</p> <ul> <li>Find a good place to arrange people (model transformation)</li> <li>Find a good place to put the camera (view transformation)</li> <li>Cheese! (projection transformation)</li> </ul>"},{"location":"Games101/lecture3/#viewcamera-transformation","title":"View/Camera Transformation","text":"<p>If both the camera and the object move together, the photo will be the same. We assume that the camera is fixed and all the objects can move. So we need to transform our camera to :</p> <ul> <li>The origin, up at Y, look at -Z.</li> <li>And transform the objects along with the camera.</li> </ul>"},{"location":"Games101/lecture3/#matrix-representation_1","title":"Matrix Representation","text":"<p>\u56e0\u4e3a\u65cb\u8f6c\u77e9\u9635\u662f\u6b63\u4ea4\u77e9\u9635, \u6240\u4ee5\u5176\u9006\u77e9\u9635\u5373\u4e3a\u5176\u8f6c\u7f6e.</p>"},{"location":"Games101/lecture3/#projection-transformation","title":"Projection Transformation","text":"<p>Projection in Computer Graphics</p> <ul> <li>3D to 2D</li> <li>Orthographic projection</li> <li>Perspective projection</li> </ul>"},{"location":"Games101/lecture3/#orthographic-projection","title":"Orthographic Projection","text":"<p>In general, we want to map a cuboid \\([l, r] \\times [b, t] \\times [f, n]\\) to the canonical cube \\([-1, 1]^3\\).</p> <p></p> <ul> <li>Center cuboid by translating</li> <li>Scale into canonical cube</li> </ul>"},{"location":"Games101/lecture3/#perspective-projection","title":"Perspective Projection","text":"<ul> <li>More common in Computer Graphics, art, visual systems.</li> <li>Further objects are smaller.</li> <li>Parallel lines not parallel, but converge to vanishing point.</li> </ul> <p>How do we do perspective projection?</p> <ul> <li>First squish the frustum into a cuboid(n-&gt;n, f-&gt;f)</li> <li>Do orthographic projection</li> </ul> <p></p> <p>\u7b2c\u4e00\u6b65\u5bf9\u5e94\u4e00\u4e2a\\(M_1\\)\u77e9\u9635, \u8868\u793a\u4ecefrustum\u5230cuboid\u7684\u53d8\u6362. \u7b2c\u4e8c\u90e8\u5bf9\u5e94\u524d\u9762Orthographic Projection\u4e2d\u7684\u77e9\u9635.</p> <p></p> <p>\u5bf9\u4e8e\u4efb\u610f\u4e00\u70b9\\((x, y, z)\\), \u53d8\u6362\u540e\u7684x, y\u5750\u6807\u90fd\u53ef\u4ee5\u901a\u8fc7\u76f8\u4f3c\u4e09\u89d2\u5f62\u7684\u5f97\u5230, \u4f46\u662f\u53d8\u6362\u540e\u7684z\u5750\u6807\u672a\u77e5, \u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u5f97\u5230:</p> \\[ \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\\\ \\end{pmatrix} =&gt; \\begin{pmatrix} nx/z \\\\ ny/z \\\\ unknown \\\\ 1 \\\\ \\end{pmatrix} =&gt; \\begin{pmatrix} nx \\\\ ny \\\\ still\\ unknown \\\\ z \\\\ \\end{pmatrix} \\] <p>\u6709\u4e86\u8fd9\u4e2a\u5173\u7cfb, \u5176\u5b9e\u6211\u4eec\u5df2\u7ecf\u53ef\u4ee5\u586b\u51fa\\(M_1\\)\u7684\u90e8\u5206\u5143\u7d20:</p> \\[ M_1 = \\begin{pmatrix} n &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; n &amp; 0 &amp; 0 \\\\ ? &amp; ? &amp; ? &amp; ? \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\] <p>\u4e3a\u4e86\u5f97\u5230\u7b2c\u4e09\u884c\u7684\u5404\u4e2a\u5143\u7d20, \u6211\u4eec\u9700\u8981\u8003\u8651\u8fd8\u6709\u6ca1\u6709\u522b\u7684\u9650\u5236\u6761\u4ef6:</p> <ul> <li>Any point on the near plane will not change.</li> </ul> \\[ \\begin{pmatrix} x \\\\ y \\\\ n \\\\ 1 \\\\ \\end{pmatrix} =&gt; \\begin{pmatrix} nx/n \\\\ ny/n \\\\ n \\\\ 1 \\\\ \\end{pmatrix} =&gt; \\begin{pmatrix} nx \\\\ ny \\\\ n^2 \\\\ n \\\\ \\end{pmatrix} \\] <p>\u5355\u72ec\u770b\u7b2c\u4e09\u884c:</p> \\[ (M_{31}, M_{32}, M_{33}, M_{34}) \\begin{pmatrix} x \\\\ y \\\\ n \\\\ 1 \\\\ \\end{pmatrix} = n^2 \\] <p>\\(n\\) has nothing to do with \\(x\\) and \\(y\\), so \\(M_{31}\\) and \\(M_{32}\\) should be zero.</p> <ul> <li>Any point on the far plane will not change.</li> </ul> \\[ (M_{31}, M_{32}, M_{33}, M_{34}) \\begin{pmatrix} x \\\\ y \\\\ n \\\\ 1 \\\\ \\end{pmatrix} = f^2 \\] <p>Solve these two equations:</p> \\[ M_{33}n + M_{34} = n^2 \\\\ \\] \\[ M_{33}f + M_{34} = f^2 \\] <p>We have:</p> \\[ M_{33} = n+f \\] \\[ M_{34} = -nf \\] <p>Finally, we get our \\(M_1\\) matrix:</p> \\[ M_1 = \\begin{pmatrix} n &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; n &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; n+f &amp; -nf \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\] <p>What's next?</p> <ul> <li>Do orthographic projection(\\(M_2\\)) to finish.</li> <li>\\(M_{persp}\\) = \\(M_2 \\cdot M_1\\)</li> </ul> <p>Question</p> <p>\u5bf9\u4e8e\u4e00\u4e2a\u70b9\\((x, y, z)\\), \u7ecf\u8fc7\u900f\u89c6\u6295\u5f71\u540e, \u53d8\u6362\u540e\u7684\u5750\u6807\u4e3a\\((nx/z, ny/z, z, 1)\\), \u90a3\u4e48\u53d8\u6362\u540e\u7684z\u5750\u6807\u662f\u4ec0\u4e48? \u76f8\u6bd4\u4e8e\u539f\u6765\u7684z\u5750\u6807\u6709\u4ec0\u4e48\u53d8\u5316?</p> \\[ \\begin{pmatrix} n &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; n &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; n+f &amp; -nf \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\\\ \\end{pmatrix} = \\begin{pmatrix} nx \\\\ ny \\\\ (n+f)z - nf \\\\ z \\\\ \\end{pmatrix} \\] <p>\u6211\u4eec\u53ea\u9700\u8981\u6bd4\u8f83\\((n+f)z - nf\\)\u4e0e\\(z^2\\)\u7684\u5173\u7cfb\u5373\u53ef, \u79fb\u9879\u5e76\u5206\u89e3\u56e0\u5f0f\u53ef\u5f97:</p> <p>$$ (n+f)z - nf &gt; z^2 $$ \u4f46\u662f\u56e0\u4e3a\\(0&gt;n&gt;z&gt;f\\), \u6240\u4ee5\u53d8\u6362\u540e\u7684z\u5750\u6807\u7edd\u5bf9\u503c\u53d8\u5c0f, \u6570\u503c\u53d8\u5927.</p> <p>Now, what's near plane's l, r, b, t then?</p> <p>Sometimes people prefer: vertical field-of-view (fovY) and aspect ratio(assume symmetry i. e. I = -r, b = -t)</p> <p></p> <p></p>"},{"location":"Games101/lecture3/#mvp","title":"MVP","text":"<ul> <li>Model transformation (placing objects)</li> <li>View transformation (placing camera)</li> <li>Projection transformation<ul> <li>Orthographic projection (cuboid to \"canonical\" cube \\([-1, 1]^3\\))</li> <li>Perspective projection (frustum to \"canonical\" cube)</li> </ul> </li> </ul>"},{"location":"Games101/lecture5/","title":"Lecture 5 &amp; 6: Rasterization","text":""},{"location":"Games101/lecture5/#basic-concepts","title":"Basic Concepts","text":"<p>After the process of MVP, we will we do to a canonical cube? We need to project the cube to the screen, and then rasterize it.</p> <ul> <li>What is a screen?<ul> <li>An array of pixels</li> <li>Size of the array: resolution</li> </ul> </li> <li>Raster = Screen in German<ul> <li>Rasterize = drawing onto the screen</li> </ul> </li> <li>Pixel(short for picture element)<ul> <li>For now: A pixel is a little square with uniform color</li> <li>Color is a mixture of (red, green , blue)</li> </ul> </li> </ul> <p></p> <p></p>"},{"location":"Games101/lecture5/#rasterization","title":"Rasterization","text":"Polygon Meshes Triangle Meshes"},{"location":"Games101/lecture5/#triangles","title":"Triangles","text":"<p>\u5047\u8bbe\u73b0\u5728\u6709\u4e00\u4e2a\u4e09\u89d2\u5f62\u5df2\u7ecf\u6295\u5f71\u5230\u5c4f\u5e55\u4e0a\u4e86, \u5982\u4f55\u6839\u636e\u5176\u4e09\u4e2a\u9876\u70b9\u7684\u4f4d\u7f6e\u786e\u5b9a\u7ed9\u54ea\u4e9bpixel\u4e0a\u8272.</p>"},{"location":"Games101/lecture5/#sampling","title":"Sampling","text":"<p>\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u4e00\u4e2a\u50cf\u7d20\u7684\u4e2d\u5fc3\u662f\u5426\u5728\u4e09\u89d2\u5f62\u5185\u90e8\u6765\u5224\u65ad\u8be5\u50cf\u7d20\u5757\u662f\u5426\u7740\u8272.</p> <p>\u5224\u65ad\u4e00\u4e2a\u70b9\u662f\u5426inside\u4e09\u89d2\u5f62\u53ef\u4ee5\u4f7f\u7528lecture2\u4e2d\u8bb2\u8fc7\u7684\u4e09\u6b21Corss Product\u7684\u65b9\u6cd5.</p> Real Cases <p>\u771f\u5b9e\u7684\u5e94\u7528\u4e2d, \u50cf\u7d20\u5757\u4e0d\u4e00\u5b9a\u662f\u65b9\u5f62\u7684: </p>"},{"location":"Games101/lecture5/#aliasingjaggies","title":"Aliasing(Jaggies)","text":"<p>Well, if we display the results of the above sampling, we will get something like this:</p> <p></p> <p>It's far from being a perfect triangle.</p> <p>Or like this:</p> <p></p>"},{"location":"Games101/lecture5/#sampling-artifects","title":"Sampling Artifects","text":"<p>Artifects due to sampling -- \"Aliasing\".</p> <ul> <li>Jaggies -- sampling in space   </li> <li>Moire -- undersampling images</li> </ul> <p></p> <ul> <li>Wagon Wheel Effect -- sampling in time</li> </ul> <p></p> <p>\u987a\u65f6\u9488\u65cb\u8f6c\u7684\u8f6e\u76d8\u770b\u8d77\u6765\u5728\u5012\u7740\u8f6c(\u4eba\u773csample\u7684\u9891\u7387\u8f83\u4f4e).</p> <p>Behind all these Aliasing Artifects</p> <p>Signals are changing too fast (high frequency), but sampled too slowly.</p>"},{"location":"Games101/lecture5/#anti-aliasing-idea-blurringpre-filtering-before-sampling","title":"Anti-Aliasing Idea: Blurring(Pre-Filtering) Before Sampling","text":"<p>\u6211\u4eec\u4e0d\u91c7\u7528\u539f\u6765\u7684\u975e0\u53731\u7684\u4e8c\u5143\u7740\u8272, \u800c\u662f\u5728blur\u4e4b\u540e\u586b\u5145\u4e0a\u6e10\u53d8\u7684\u989c\u8272.</p> <p>But why does this work?</p>"},{"location":"Games101/lecture5/#frequency-domain","title":"Frequency Domain","text":"<p>\u901a\u8fc7Fourier\u53d8\u6362, \u6211\u4eec\u53ef\u4ee5\u5c06\u56fe\u50cf\u53d8\u4e3a\u9891\u7387\u4e0d\u540c\u7684\u6ce2\u6bb5, \u4f4e\u9891\u90e8\u5206\u5bf9\u5e94\u7740\u56fe\u50cf\u4e2d\u8fde\u7eed\u53d8\u6362\u7684\u90e8\u5206, \u800c\u9ad8\u9891\u90e8\u5206\u5bf9\u5e94\u7740\u56fe\u50cf\u4e2d\u7a81\u53d8\u7684\u90e8\u5206(\u8fb9\u7f18). \u56e0\u4e3a\u6211\u4eec\u7684sample \u9891\u7387\u8f83\u4f4e, \u6240\u4ee5\u9ad8\u9891\u90e8\u5206\u5f88\u96be\u91c7\u6837\u51fa\u771f\u5b9e\u7684\u7ed3\u679c, \u4ece\u800c\u5bfc\u81f4\u91c7\u6837\u7ed3\u679c\u51fa\u73b0Aliasing. \u5982\u679c\u6211\u4eec\u4f7f\u7528blur\u7b49\u624b\u6bb5\u8fc7\u6ee4\u6389\u56fe\u50cf\u7684\u9ad8\u9891\u90e8\u5206, \u6211\u4eec\u7684\u91c7\u6837\u5c31\u53ef\u4ee5\u8f83\u597d\u5730\u8fd8\u539f\u771f\u5b9e\u56fe\u5f62, \u4ece\u800c\u7684\u5f97\u5230\u8f83\u597d\u7684\u7ed3\u679c.</p> <p>Filtering = Getting rid of certain frequencies</p>"},{"location":"Games101/lecture5/#anti-aliasing-by-computing-average-pixel-value","title":"Anti aliasing by Computing Average Pixel Value","text":"<p>In rasterizing one triangle, the average value inside a pixel area of f(x,y) = inside(triangle,x,y) is equal to the area of the pixel covered by the triangle.</p> <p></p>"},{"location":"Games101/lecture5/#anti-aliasing-by-supersamplingmsaa","title":"Anti aliasing By Supersampling(MSAA)","text":"<p>\u6839\u636e\u521a\u624d\u7684\u60f3\u6cd5, \u6211\u4eec\u5e0c\u671b\u8ba1\u7b97\u51fa\u4e00\u4e2a\u50cf\u7d20\u5185\u90e8\u8be5\u4e09\u89d2\u5f62\u8986\u76d6\u7684\u9762\u79ef\u5360\u50cf\u7d20\u9762\u79ef\u7684\u6bd4\u4f8b, \u4f46\u5b9e\u9645\u4e0a\u8fd9\u662f\u4e00\u4e2a\u8f83\u96be\u6c42\u89e3\u7684\u8fde\u7eed\u95ee\u9898, \u6211\u4eec\u4ecd\u7136\u4f7f\u7528\u79bb\u6563\u5316\u7684\u624b\u6bb5\u6765\u5904\u7406\u8be5\u95ee\u9898.</p> <p>Approximate the effect of the 1-pixel box filter by sampling multiple locations within a pixel and averaging their values:</p> <p></p> <p>\u6bd4\u5982\u8bf4\u4e0a\u9762\u8fd9\u5f20\u56fe, \u6211\u4eec\u5c06\u4e00\u4e2a\u50cf\u7d20\u8fdb\u4e00\u6b65\u7ec6\u5206\u621016\u4e2a\u533a\u57df, \u6bcf\u4e2a\u533a\u57df\u5bf9\u5e94\u4e00\u4e2a\u4e2d\u5fc3\u70b9, \u6bcf\u4e2a\u5206\u5757\u4ecd\u7136\u4f7f\u7528\u4e4b\u524d\u7684\u65b9\u6cd5\u5224\u65ad\u662f\u5426\u5728\u4e09\u89d2\u5f62\u5185\u90e8, \u6700\u540e\u7edf\u8ba1\u5728\u4e09\u89d2\u5f62\u5185\u90e8\u7684\u4e2d\u5fc3\u70b9\u7684\u4e2a\u6570, \u9664\u4ee516, \u518d\u4e58\u4e0a\u4e09\u89d2\u5f62\u7684\u989c\u8272\u4f5c\u4e3a\u8be5\u50cf\u7d20\u6700\u7ec8\u7684\u989c\u8272.</p> <p>Steps:</p> <ol> <li>Take N*N samples within each pixel.</li> <li>Average the N*N samples inside each pixel.</li> </ol> <p></p> <p>Anti aliasing today</p> <p>\u6211\u4eec\u5c06\u6bcf\u4e2a\u50cf\u7d20\u5757\u8fdb\u4e00\u6b65\u7ec6\u5206, \u5e26\u6765\u7684drawback\u5fc5\u7136\u662f\u8ba1\u7b97\u590d\u6742\u6027\u63d0\u5347, \u5982\u4eca\u7684state of the art\u4f1a\u6709\u4e00\u4e9b\u66f4\u52a0\u9ad8\u6548\u7684\u5728\u6bcf\u4e2apixel\u4e2d\u5206\u533a\u7684\u65b9\u6cd5, \u4ec0\u4e48\u4e34\u8fd1\u7684pixels\u8fd8\u53ef\u4ee5\u5171\u7528\u67d0\u4e9b\u5c0f\u5206\u533a. </p>"},{"location":"Games101/lecture5/#depth-control","title":"Depth Control","text":"<p>\u6211\u4eec\u8ba8\u8bba\u4e86\u5982\u4f55\u5c06\u4e00\u4e2a\u4e09\u89d2\u5f62\u8868\u5f81\u5728\u4e8c\u7ef4\u7684\u79bb\u6563\u7684\u50cf\u7d20\u70b9\u4e0a, \u4f46\u5b9e\u9645\u4e0a\u4e00\u4e2a\u771f\u5b9e\u7684\u56fe\u5f62\u4f1a\u88ab\u5206\u5272\u6210\u5f88\u591a\u4e0d\u5728\u540c\u4e00\u5e73\u9762\u4e0a\u7684\u4e09\u89d2\u5f62, \u6211\u4eec\u9700\u8981\u5c06\u8fd9\u4e9b\u4e09\u89d2\u5f62\u5168\u90e8\u6295\u5f71\u5230\u4e8c\u7ef4\u50cf\u7d20\u5e73\u9762\u4e0a. \u4e8e\u662f\u6211\u4eec\u5c31\u9700\u8981\u786e\u5b9a\u4e09\u89d2\u5f62\u7684\u906e\u6321\u5173\u7cfb.</p>"},{"location":"Games101/lecture5/#painters-algorithm","title":"Painter's Algorithm","text":"<p>Inspired by how painters paint, we just paint from back to front, overwrite in the framebuffer.</p> <p></p> <p>\u5bf9\u4e8e\u8fd9\u6837\u4e00\u5f20\u6cb9\u753b\u6765\u8bb2, \u6211\u4eec\u5148\u753b\u8fdc\u5c71, \u518d\u753b\u8349\u5730(\u4f1a\u906e\u6321\u4e00\u90e8\u5206\u8fdc\u5c71), \u6700\u540e\u753b\u6811\u6728(\u4f1a\u906e\u6321\u4e00\u90e8\u5206\u8fdc\u5c71\u548c\u8349\u5730).</p> <p>\u8fd9\u6837\u7684\u7b97\u6cd5\u9700\u8981\u6211\u4eec\u60f3\u5404\u4e2a\u4e09\u89d2\u5f62\u6309\u7167\u6df1\u5ea6\u6392\u5e8f, \\(\\mathcal{O(NlogN)}\\).</p> <p>Failure</p> <p> This algorithm cannot resolve the situation in the above piceture.</p>"},{"location":"Games101/lecture5/#z-buffer","title":"Z-Buffer","text":"<p>This is the algorithm that eventually won.</p> <ul> <li>Store current minimum depth(z-value) for each pixel in the framebuffer.</li> <li>Needs an additional buffer for depth values.<ul> <li>frame buffer stores color values</li> <li>depth buffer stores depth values</li> </ul> </li> </ul> <p>Warning</p> <p>For simplicity, we suppose \\(z\\) is always positive. smaller z -&gt; closer, larger z -&gt; farther</p> <p>Example</p> <p> R\u4ee3\u8868\u65e0\u7a77\u5927, \u8868\u793a\u672a\u5904\u7406\u4e4b\u524d\u50cf\u7d20\u5bf9\u5e94\u7684depth\u4e3a\u65e0\u7a77\u5927.</p> <p>Suppose that each triangle contains a limited number of pixels, the time complexity of z-buffering will be \\(\\mathcal{O(N)}\\).</p> <p>Tip</p> <p>\u5f53\u7136, \u5982\u679c\u6211\u4eec\u8981\u4f7f\u7528MSAA\u7b49\u65b9\u6cd5, \u5373\u5728\u4e00\u4e2a\u50cf\u7d20\u5185\u8bbe\u7f6e\u591a\u4e2a\u91c7\u6837\u70b9, \u5219\u9700\u8981\u5bf9\u6bcf\u4e2a\u91c7\u6837\u70b9\u7ef4\u62a4\u4e00\u4e2adepth\u548ccolor\u7684buffer.</p>"},{"location":"Games101/lecture7/","title":"Lecture 7 &amp; 8 &amp; 9: Shading","text":"<p>What we've covered so far:</p> <p></p>"},{"location":"Games101/lecture7/#definition","title":"Definition","text":"<p>The process of applying a material to an object.</p>"},{"location":"Games101/lecture7/#a-simple-shading-model-blinn-phong-reflective-model","title":"A simple Shading Model: Blinn-Phong Reflective Model","text":""},{"location":"Games101/lecture7/#perceptual-observations","title":"Perceptual Observations","text":"<p>\u5728\u6211\u4eec\u89c2\u5bdf\u4e00\u4e2a\u573a\u666f\u7684\u65f6\u5019, \u6709\u4e9b\u5730\u65b9\u4f1a\u4ea7\u751f\u9ad8\u5149\u7684\u6548\u679c, \u5373\u5149\u7ebf\u57fa\u672c\u5448\u73b0\u955c\u9762\u53cd\u5c04\u7684\u6548\u679c; \u6709\u4e9b\u5730\u65b9\u5341\u5206\u5e73\u6ed1, \u5373\u5149\u7ebf\u57fa\u672c\u5448\u73b0\u6f2b\u53cd\u5c04\u7684\u6548\u679c; \u6709\u4e9b\u5730\u65b9\u8f83\u6697, \u4e0d\u80fd\u88ab\u5149\u6e90\u76f4\u63a5\u7167\u5c04\u5230, \u662f\u4f9d\u9760\u522b\u7684\u5730\u65b9\u7684\u53cd\u5c04\u5149\u7167\u4eae\u7684.</p> <p>Shading \u5bf9\u5e94\u7684\u662f\u7740\u8272\u7684\u8fc7\u7a0b, \u4e0d\u4f1a\u8003\u8651\u5f71\u5b50\u7684\u95ee\u9898.</p> <p></p>"},{"location":"Games101/lecture7/#model-basics","title":"Model Basics","text":"<p>\u6a21\u578b\u7684\u8f93\u5165\u6709\u89c2\u6d4b\u70b9\u76f8\u5bf9\u4e8e\u7740\u8272\u70b9\u7684\u65b9\u5411, \u5149\u7ebf\u76f8\u5bf9\u4e8e\u7740\u8272\u70b9\u7684\u65b9\u5411, \u7740\u8272\u70b9\u5904\u5e73\u9762\u7684\u6cd5\u7ebf, \u7740\u8272\u70b9\u5904\u5e73\u9762\u7684\u4e00\u4e9b\u5176\u4ed6\u4fe1\u606f(\u989c\u8272, \u4eae\u5ea6).</p>"},{"location":"Games101/lecture7/#diffuse-reflection","title":"Diffuse Reflection","text":"<p>Light is uniformly scattered in all directions, and surface color is the same for all viewing directions.</p> <p></p>"},{"location":"Games101/lecture7/#lamberts-law","title":"Lambert's Law","text":"<p>surface \u5438\u6536\u7684\u5149\u7167\u5f3a\u5ea6\u4e0e\u5165\u5c04\u5149\u7ebf\u7684\u89d2\u5ea6\u6709\u5173, \u4e0e\u89c2\u6d4b\u70b9\u7684\u89d2\u5ea6\u65e0\u5173(\u56e0\u4e3a\u6f2b\u53cd\u5c04\u5404\u4e2a\u65b9\u5411\u89c2\u6d4bcolor\u90fd\u4e00\u6837).</p>"},{"location":"Games101/lecture7/#light-falloff","title":"Light Falloff","text":"<p>\u6211\u4eec\u5047\u8bbe\u5149\u6cbf\u7740\u56db\u9762\u516b\u65b9\u5747\u5300\u4f20\u64ad, \u7531\u4e8e\u80fd\u91cf\u5b88\u6052, \u6bcf\u4e00\u65f6\u523b\u7684\u7403\u58f3\u4e0a\u5e26\u6709\u7684\u80fd\u91cf\u90fd\u662f\u4e00\u6837\u7684, \u90a3\u4e48\u5355\u4f4d\u9762\u79ef\u7684\u5149\u7167\u5f3a\u5ea6\u5c31\u4e0e\u7403\u58f3\u534a\u5f84\u7684\u5e73\u65b9(\u7403\u58f3\u9762\u79ef)\u6210\u53cd\u6bd4.</p> <p>\u7efc\u5408Lambert's Law\u548cLight Falloff, \u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u6f2b\u53cd\u5c04\u4e0b\u7684\u7740\u8272\u516c\u5f0f(Diffusion Term):</p> \\[ L_d = k_d(I/r^2)max(0,\\hat{n}\\cdot \\hat{l}) \\] <p></p> <p>\\(k_d\\)\u7684\u5b58\u5728\u662f\u56e0\u4e3a\u6211\u4eec\u5047\u8bbe\u8868\u9762\u4f1a\u5438\u6536\u4e00\u4e9b\u80fd\u91cf, \u5047\u5982\\(k_d\\)\u4e3a1, \u4ee3\u8868\u8868\u9762\u5b8c\u5168\u6ca1\u6709\u5438\u6536\u80fd\u91cf, \u4e3a0\u5219\u4ee3\u8868\u5438\u6536\u4e86\u6240\u6709\u80fd\u91cf.</p> <p>The impact of \\(k_d\\)</p> <p> \u53ef\u4ee5\u770b\u5230, \\(k_d\\)\u8d8a\u5927, \u53cd\u5c04\u7684\u5149\u5f3a\u8d8a\u5927, \u5373\u8868\u9762\u8d8a\u4eae.</p>"},{"location":"Games101/lecture7/#specular-reflection","title":"Specular Reflection","text":"<p>Intensity depends on view direction.</p> <p>Bright near mirror reflection direction. \u955c\u9762\u53cd\u5c04\u7684\u7279\u70b9\u662f\u5b58\u5728\u7740\u4e00\u4e2a\u53cd\u5c04\u65b9\u5411, \u53ea\u6709\u5728\u8be5\u53cd\u5c04\u65b9\u5411\u9644\u8fd1\u7684\u89c2\u6d4b\u65b9\u5411\u624d\u80fd\u770b\u5230\u9ad8\u5149.</p> <p></p> <p>\u6211\u4eec\u4e00\u822c\u4f7f\u7528half vector\u6765\u8ba1\u7b97\u89c2\u6d4b\u65b9\u5411\u4e0e\u5165\u5c04\u65b9\u5411\u7684\u89d2\u5e73\u5206\u7ebf\u65b9\u5411\u5411\u91cf.</p> <p>\u8ba1\u7b97half vector\u4e0e\u6cd5\u7ebf\u7684\u70b9\u79ef\u800c\u4e0d\u662f\u8ba1\u7b97\u53cd\u5c04\u5149\u7ebf\u548c\u89c2\u6d4b\u65b9\u5411\u7684\u70b9\u79ef, \u8fd9\u662f\u56e0\u4e3a\u524d\u8005\u8ba1\u7b97\u4e0a\u66f4\u52a0\u7b80\u4fbf.</p> <p>\u5f97\u5230Specular Term\u5982\u4e0b:</p> \\[ L_s = k_s(I/r^2)max(0,\\hat{h}\\cdot \\hat{n})^p \\] <p>\u540c\u7406, \u8fd9\u91cc\u7684\\(k_s\\)\u4e5f\u662f\u8868\u793a\u5438\u6536\u80fd\u91cf\u7684\u6bd4\u4f8b.</p> <p>why is there a power \\(p\\)?</p> <p> \u6211\u4eec\u5e0c\u671b\u53ea\u6709\u5728\u89c2\u6d4b\u65b9\u5411\u4e0e\u53cd\u5c04\u65b9\u5411\u975e\u5e38\u63a5\u8fd1\u7684\u65f6\u5019, \u5149\u7684\u5f3a\u5ea6\u624d\u5927, \u800c\u5f53\u89c2\u6d4b\u65b9\u5411\u4e0e\u53cd\u5c04\u65b9\u5411\u76f8\u5dee\u8f83\u5927\u7684\u65f6\u5019, \u5149\u7684\u5f3a\u5ea6\u5c31\u975e\u5e38\u5c0f. \u4f46\u662f\u5982\u679c\u4e0d\u7ed9\\(cos\\alpha\\)\u9879\u52a0\u4e0a\u6307\u6570\u7684\u8bdd, \u53ef\u4ee5\u770b\u5230\u5c31\u7b97\u662f\u76f8\u5dee\u4e8645\u5ea6, \u89c2\u6d4b\u5230\u7684\u4eae\u5ea6\u8fd8\u662f\u8f83\u5927. \u6240\u4ee5\u6211\u4eec\u9700\u8981\u7ed9\\(cos\\alpha\\)\u9879\u52a0\u4e0a\u4e00\u4e2a\u6307\u6570, \u4f7f\u5f97\u5f53\u89d2\u5ea6\u76f8\u5dee\u8f83\u5927\u7684\u65f6\u5019, \u5149\u7684\u5f3a\u5ea6\u8fc5\u901f\u4e0b\u964d.</p> Tip <p> \u5c06Diffusion Term\u548cSpecular Term\u7ed3\u5408\u8d77\u6765, \u6211\u4eec\u89c2\u5bdf\\(k_s\\)\u548c\\(p\\)\u53d8\u5316\u5bf9\u6e32\u67d3\u6548\u679c\u7684\u5f71\u54cd. \u53ef\u4ee5\u770b\u5230\u968f\u7740\\(p\\)\u589e\u5927, \u9ad8\u4eae\u66f4\u52a0\u96c6\u4e2d.</p>"},{"location":"Games101/lecture7/#ambient-term","title":"Ambient Term","text":"<p>\u5bf9\u73af\u5883\u5149\u7684\u5f71\u54cd, Blinn-Phong Model\u8003\u8651\u7684\u975e\u5e38\u7b80\u5355, \u5373\u7ed9\u6240\u6709\u5730\u65b9\u90fd\u52a0\u4e0a\u4e00\u4e2a\u76f8\u540c\u7684\u5e38\u91cf\u5149\u5f3a, \u4e0d\u8fc7\u8fd9\u4e2a\u5149\u5f3a\u8981\u4e58\u4ee5\u4e00\u4e2a\u7cfb\u6570\\(k_a\\).</p> <p></p> <p>\u7efc\u5408\u4e0a\u8ff0\u4e09\u4e2aTerm, \u6211\u4eec\u53ef\u4ee5\u5f97\u5230Blinn-Phong Reflective Model\u7684\u516c\u5f0f:</p> \\[ L = L_a + L_d + L_s \\] \\[ L = k_d(I/r^2)max(0,\\hat{n}\\cdot \\hat{l}) + k_s(I/r^2)max(0,\\hat{h}\\cdot \\hat{n})^p + k_aL_a \\] <p>\u53ef\u4ee5\u5927\u81f4\u6e32\u67d3\u51fa\u8fd9\u6837\u7684\u7ed3\u679c:</p> <p></p>"},{"location":"Games101/lecture7/#shading-frequencies","title":"Shading Frequencies","text":"<p>\u6211\u4eec\u521a\u521a\u8003\u8651\u7684\u90fd\u662f\u7b2c\u4e00\u4e2a\u70b9\u8fdb\u884c\u7740\u8272, \u4f46\u662f\u5b9e\u9645\u7684\u56fe\u5f62\u662f\u8fde\u7eed\u7684, \u6211\u4eec\u6709\u591a\u79cd\u624b\u6bb5\u5c06\u5176\u53d8\u4e3a\u79bb\u6563\u7684\u70b9, \u800c\u540e\u5728=\u518d\u7740\u8272.</p> <p></p>"},{"location":"Games101/lecture7/#flat-shading","title":"Flat Shading","text":"<p>\u6bcf\u4e2a\u4e09\u89d2\u5f62\u7684\u5c0f\u5207\u9762\u5f53\u505a\u4e00\u4e2a\u70b9, \u7136\u540e\u7ed9\u6574\u4e2a\u5207\u9762shade\u76f8\u540c\u7684\u989c\u8272.</p>"},{"location":"Games101/lecture7/#gouraud-shading","title":"Gouraud Shading","text":"<p>\u5bf9\u6bcf\u4e2a\u9876\u70b9\u7740\u8272, \u7136\u540e\u4e09\u89d2\u5f62\u4e0a\u7684\u989c\u8272\u7531\u5404\u4e2a\u9876\u70b9\u4e4b\u95f4\u63d2\u503c\u5f97\u5230.</p>"},{"location":"Games101/lecture7/#phong-shading","title":"Phong Shading","text":"<p>\u63d2\u503c\u7684\u5230\u4e09\u89d2\u5f62\u533a\u57df\u4e0a\u7684\u6240\u6709\u6cd5\u5411\u91cf, \u7136\u540e\u9488\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u7740\u8272.</p>"},{"location":"Games101/lecture7/#comparison","title":"Comparison","text":"<p>\u5176\u5b9e\u53ea\u8981\u6211\u4eec\u628a\u8fd9\u4e9b\u4e09\u89d2\u5f62\u5206\u5272\u5730\u8db3\u591f\u7ec6\u81f4, Flat \u548c Phong Shading\u6548\u679c\u76f8\u5dee\u65e0\u51e0.</p>"},{"location":"Games101/lecture7/#per-vertex-normal-vectors","title":"Per-Vertex Normal Vectors","text":"<ul> <li>Best to get vertex normals from the underlying geometry.</li> </ul> <p>\u6bd4\u5982\u6211\u4eec\u77e5\u9053\u9700\u8981\u6e32\u67d3\u7684\u662f\u4e2a\u7403\u4f53, \u90a3\u4e48\u6bcf\u4e2a\u9876\u70b9\u7684\u6cd5\u5411\u91cf\u81ea\u7136\u662f\u5706\u5fc3\u5230\u9876\u70b9\u7684\u8fde\u7ebf\u65b9\u5411. </p> <ul> <li>Otherwise have to infer vertex normals from triangle faces.</li> </ul> <p></p> <p>\u6700\u7b80\u5355\u7684\u505a\u6cd5\u5c31\u662f\u5bf9\u8be5\u9876\u70b9\u6240\u5728\u7684\u51e0\u4e2a\u4e09\u89d2\u5f62\u9762\u7684\u6cd5\u5411\u91cf\u6c42\u5e73\u5747, \u518d\u5f52\u4e00\u5316:</p> \\[ N_v = \\frac{\\sum_i N_i}{||\\sum_i N_i||} \\] <p>\u5f53\u7136\u6709\u53ef\u80fd\u6709\u4e9b\u4e09\u89d2\u5f62\u9762\u6bd4\u8f83\u5927, \u5bf9\u9876\u70b9\u7684\u6cd5\u5411\u91cf\u5f71\u54cd\u4f1a\u76f8\u5bf9\u8f83\u5927, \u6240\u4ee5\u4e5f\u53ef\u4ee5\u5bf9\u51e0\u4e2a\u6cd5\u5411\u91cf\u52a0\u6743\u6c42\u5e73\u5747.</p>"},{"location":"Games101/lecture7/#per-pixel-normal-vectors","title":"Per-Pixel Normal Vectors","text":"<p>\u7531\u51e0\u4e2a\u9876\u70b9\u4e4b\u95f4\u7684normal vectors\u63d2\u503c\u5f97\u5230, \u5177\u4f53\u63d2\u503c\u65b9\u6cd5\u540e\u6587\u4ecb\u7ecd.</p>"},{"location":"Games101/lecture7/#interpolation-across-triangles-barycentric-coordinates","title":"Interpolation Across Triangles: Barycentric Coordinates","text":"<p>\u8003\u8651\u8fd9\u6837\u7684\u60c5\u666f: \u5df2\u77e5\u4e09\u89d2\u5f62\u4e09\u4e2a\u9876\u70b9\u7684\u67d0\u79cd\u5c5e\u6027, \u6211\u4eec\u5e0c\u671b\u5f97\u5230\u4e09\u89d2\u5f62\u5185\u90e8\u4e00\u4e2a\u70b9\u7684\u8be5\u5c5e\u6027. \u6211\u4eec\u9700\u8981\u4f7f\u7528\u91cd\u5fc3\u5750\u6807\u8fdb\u884c\u63d2\u503c.</p>"},{"location":"Games101/lecture7/#barycentric-coordinates","title":"Barycentric Coordinates","text":"<p>\u5982\u679c\u4efb\u610f\u4e00\u70b9\u6ee1\u8db3\\(\\alpha + \\beta + \\gamma = 1\\), \u90a3\u4e48\u8fd9\u4e2a\u70b9\u5c31\u5728\u4e09\u89d2\u5f62\u6240\u5728\u7684\u5e73\u9762\u4e0a, \u540c\u65f6, \u5982\u679c\\(\\alpha, \\beta, \\gamma\\)\u90fd\u5927\u4e8e0, \u90a3\u4e48\u8fd9\u4e2a\u70b9\u5c31\u5728\u4e09\u89d2\u5f62\u5185\u90e8.</p> ways to calculate the barycentric coordinates <p> \u53ef\u4ee5\u7528\u9876\u70b9\u6240\u5bf9\u7684\u9762\u79ef\u6bd4\u8ba1\u7b97  \u53ef\u4ee5\u76f4\u63a5\u4ee3\u516c\u5f0f</p> <p>\u7279\u522b\u5730, \u91cd\u5fc3\u5bf9\u5e94\u7684\u91cd\u5fc3\u5750\u6807\u4e3a\\((\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})\\).</p>"},{"location":"Games101/lecture7/#interpolation","title":"Interpolation","text":"<p>\u6211\u4eec\u76f4\u63a5\u5229\u7528\\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\)\u4e09\u4e2a\u53c2\u6570\u5bf9\u4e09\u4e2a\u9876\u70b9\u7684\u67d0\u4e2a\u5c5e\u6027\u52a0\u6743\u5e73\u5747\u5373\u53ef.</p> <p>barycentric coordinates are not invariant under projection</p> <p>\u91cd\u5fc3\u5750\u6807\u4e0d\u5177\u6709\u6295\u5f71\u4e0d\u53d8\u6027, \u6240\u4ee5\u8981\u6ce8\u610f\u8ba1\u7b97\u91cd\u5fc3\u5750\u6807\u8fdb\u884c\u63d2\u503c\u7684\u65f6\u5019, \u4e00\u5b9a\u8981\u8003\u8651\u6e05\u695a\u6b64\u65f6\u5bf9\u5e94\u7684\u4e09\u89d2\u5f62\u5f62\u6001. \u6bd4\u5982, \u6211\u4eec\u60f3\u5bf9\u4e00\u4e2a\u4e09\u89d2\u5f62\u5185\u90e8\u7684\u70b9\u63d2\u503c\u8ba1\u7b97\u6df1\u5ea6(depth), \u90a3\u4e48\u6b64\u65f6\u5c31\u8981\u6c42\u8be5\u70b9\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e09\u89d2\u5f62\u7684\u91cd\u5fc3\u5750\u6807, \u800c\u4e0d\u80fd\u6c42\u5df2\u7ecf\u6295\u5f71\u5728\u4e8c\u7ef4\u5e73\u9762\u4e4b\u540e\u7684\u91cd\u5fc3\u5750\u6807.</p>"},{"location":"Games101/lecture7/#texture-mapping","title":"Texture Mapping","text":"<p>\u5728Blinn-Phong\u6a21\u578b\u4e2d, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\\(k_d\\)\u6765\u8868\u793a\u8868\u9762\u7684\u7eb9\u7406\u7279\u5f81, \u6bd4\u5982\u4e0a\u9762\u8fd9\u5f20\u56fe, \u7403\u548c\u5730\u677f\u90fd\u6709\u7740\u81ea\u5df1\u7684\u7eb9\u7406.  \u9700\u8981\u6ce8\u610f\u7684\u662f, \\(k_d\\)\u662f\u4e09\u5143\u5411\u91cf, \u8868\u793a\u4e09\u4e2a\u989c\u8272\u901a\u9053, shading\u516c\u5f0f\u540e\u9762\u6240\u4e58\u7684\u53ef\u4ee5\u7406\u89e3\u4e3a\u53ea\u662f\u4eae\u5ea6.</p> <p>\u5047\u8bbe, \u8bbe\u8ba1\u5e08\u9884\u5148\u4e3a\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4efd\u7eb9\u7406\u56fe\u6837(\u4e00\u5f202D\u56fe\u50cf), \u5e76\u4e14\u6807\u6ce8\u4e863D\u6a21\u578b\u4e0a\u7684\u6bcf\u4e2a\u4e09\u89d2\u5f62\u5bf9\u5e94\u76842D\u56fe\u6837\u4e0a\u7684\u4e09\u89d2\u5f62.</p> <p></p> <p>Each triangle vertex is assigned a texture coordinate \\((u, v)\\), which is a 2D point in \\([0,1] \\times [0,1]\\).</p> <p>\u90a3\u4e48\u6211\u4eec\u5c31\u53ef\u4ee5\u6839\u636e\u7740\u8272\u70b9\u57283D\u6a21\u578b\u4e0a\u7684\u4f4d\u7f6e, \u67e5\u8be2\u5230\u5176\u57282D\u56fe\u6837\u4e0a\u7684\u4f4d\u7f6e, \u7136\u540e\u6839\u636e\u8fd9\u4e2a\u4f4d\u7f6e\u6765\u67e5\u8be2\u7eb9\u7406\u7279\u5f81.</p> <p>\u4e00\u5f20\u7eb9\u7406\u56fe\u6837\u53ef\u80fd\u5728\u4e09\u7ef4\u6a21\u578b\u4e2d\u53cd\u590d\u51fa\u73b0, \u6240\u4ee5\u6211\u4eec\u5e0c\u671b\u7eb9\u7406\u7684\u8bbe\u8ba1\u662f\u53ef\u4ee5\u81ea\u7136\u62fc\u63a5\u7684, \u5373\u5de6\u8fb9\u4e0e\u53f3\u8fb9, \u4e0a\u8fb9\u4e0e\u4e0b\u8fb9.</p> <p></p> <p>\u4e0b\u9762, \u6211\u4eec\u4ecb\u7ecd\u51e0\u79cd\u5c06\u7eb9\u7406\u6620\u5c04\u7528\u4e8eshading\u7684\u7b97\u6cd5</p>"},{"location":"Games101/lecture7/#simple-texture-mapping-diffuse-color","title":"Simple Texture Mapping: Diffuse Color","text":"<p>\u8fd9\u6837\u7684\u505a\u6cd5\u76f4\u63a5\u5c06\u56fe\u50cf\u4e0a\u7684\u70b9\u6620\u5c04\u5230\u7eb9\u7406\u4e0a, \u91c7\u6837, \u83b7\u5f97\\(k_d\\), \u770b\u4e0a\u53bb\u5f88\u7b80\u5355, \u4f46\u662f\u5b9e\u9645\u4e2d\u4f1a\u9047\u5230\u5f88\u591a\u95ee\u9898.</p>"},{"location":"Games101/lecture7/#texture-too-small","title":"Texture too Small","text":"<p>\u60f3\u8c61\u4e00\u4e0b, \u5982\u679c\u6211\u4eec\u5e0c\u671b\u6e32\u67d3\u4e00\u9762\u5899, \u73b0\u5728\u6709\u4e00\u4e2a\u5899\u7eb8\u7684\u7eb9\u7406, \u4f46\u662f\u5899\u7684\u5206\u8fa8\u7387\u4e3a\\(1024 \\times 1024\\), \u800c\u7eb9\u7406\u7684\u5206\u8fa8\u7387\u5c0f\u5f97\u591a, \u90a3\u4e48\u5c06\u8fd9\u6837\u7684\u7eb9\u7406\u6dfb\u52a0\u5230\u5899\u4e0a, \u5c31\u4f1a\u4f7f\u5f97\u5899\u9762\u770b\u8d77\u6765\u5f88\u6a21\u7cca.</p> <p></p> <p>\u4e0d\u8fc7\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u8f83\u597d\u5730\u901a\u8fc7\u63d2\u503c\u89e3\u51b3, \u6700\u5de6\u4fa7\u7684\u56fe\u50cf\u662f\u4e0d\u505a\u5904\u7406, \u76f4\u63a5\u91c7\u6837\u5f97\u5230\u7684\u7ed3\u679c, \u53ef\u4ee5\u770b\u51fa\u6709\u5f88\u591a\u952f\u9f7f, \u4e2d\u95f4\u548c\u53f3\u8fb9\u7684\u56fe\u50cf\u5206\u522b\u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c(\u8003\u8651\u7eb9\u7406\u4e0a\u90bb\u8fd1\u7684\u56db\u4e2atexel)\u548c\u4e09\u6b21\u63d2\u503c(\u8003\u8651\u7eb9\u7406\u4e0a\u90bb\u8fd1\u768416\u4e2atexel), \u6548\u679c\u90fd\u4e0d\u9519, \u4f46\u662f\u4e09\u6b21\u63d2\u503c\u7684\u6548\u679c\u66f4\u597d, \u5374\u5904\u7406\u590d\u6742\u5ea6\u66f4\u9ad8.</p>"},{"location":"Games101/lecture7/#texture-too-large","title":"Texture too Large","text":"<p>\u4e3a\u4ec0\u4e48\u4f1a\u51fa\u73b0\u4e0a\u56fe\u7684\u8fd9\u79cd\u60c5\u51b5?  \u56e0\u4e3a\u4e00\u4e2ascreen\u4e0a\u7684pixel, \u5bf9\u5e94\u7740\u7eb9\u7406\u4e0a\u4e00\u5927\u7247texel, \u6211\u4eec\u76f4\u63a5\u91c7\u6837\u7684\u8bdd\u4f1a\u53d6\u5f97\u90a3\u4e48\u4e00\u5927\u7247\u7684\u4e2d\u5fc3texel, \u81ea\u7136\u4e0d\u80fd\u5f88\u597d\u5730\u4ee3\u8868\u4e00\u5927\u7247texel\u7684\u4fe1\u606f. </p> <p></p>"},{"location":"Games101/lecture7/#super-sampling","title":"Super Sampling","text":"<p>\u7a76\u5176\u672c\u8d28, \u8fd8\u662f\u56e0\u4e3a\u6211\u4eec\u7684\u91c7\u6837\u9891\u7387\u592a\u4f4e, \u5bfc\u81f4\u4e00\u4e2a\u91c7\u6837\u70b9\u91cc\u9762\u5305\u542b\u4e86\u592a\u591a\u4fe1\u606f. \u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u5c06\u4e00\u4e2apixel\u7ec6\u5316\u6210\u591a\u4e2a\u91c7\u6837\u70b9, \u6700\u540epixel\u7684\u989c\u8272\u5c31\u662f\u8fd9\u4e9b\u91c7\u6837\u70b9\u7684\u989c\u8272\u7684\u5e73\u5747.</p> <p></p> <p>\u8fd9\u6837\u8fdb\u884csuper sampling\u4e4b\u540e, \u6211\u4eec\u5c31\u53ef\u4ee5\u5f97\u5230\u66f4\u7cbe\u7ec6\u7684\u7eb9\u7406\u6548\u679c, \u4f46\u662f\u4ee3\u4ef7\u5c31\u662f\u8ba1\u7b97\u91cf\u66f4\u5927.</p>"},{"location":"Games101/lecture7/#mipmap","title":"Mipmap","text":"<p>\u6211\u4eec\u5728SuperSampling\u4e2d\u505a\u7684\u5176\u5b9e\u5c31\u662f\u53bb\u6a21\u62df\u4e00\u5927\u7247texel\u7684\u5e73\u5747\u503c, \u90a3\u4e48\u8003\u8651\u6709\u6ca1\u6709\u4ec0\u4e48\u7b97\u6cd5\u80fd\u591f\u8fd1\u4f3c\u5730\u5728\u8f83\u5feb\u65f6\u95f4\u5185\u67e5\u8be2\u5230\u4e00\u7247texel\u7684\u5e73\u5747\u503c\u5462? Mipmap -- Allowing (fast, approx, square) range queries.</p> <p>\u6211\u4eec\u5148\u8981\u5bf9\u7eb9\u7406\u56fe\u6837\u505a\u4e00\u4e9b\u9884\u5904\u7406, \u65b9\u4fbf\u540e\u7eed\u7684\u5feb\u901f\u533a\u57df\u67e5\u8be2.</p> <p></p> <p></p> <p>\u6211\u4eec\u5c06\u56fe\u50cf\u7684\u5206\u8fa8\u7387\u4e0d\u65ad\u964d\u4f4e, \u76f4\u5230\u53d8\u4e3a\u4e00\u4e2apixel \u800c\u540e\u5c06\u5176\u5806\u53e0\u6210\u91d1\u5b57\u5854\u7684\u5f62\u72b6.</p> overhead <p>\u6211\u4eec\u9700\u8981\u50a8\u5b58\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u7684\u56fe\u6837, \u6240\u4ee5\u4f1a\u6709\u4e00\u5b9a\u7684overhead, \u4f46\u5176\u5b9e\u8fd9\u4e2aoverhead\u4e0d\u5927, \u662f\u4e00\u4e2a\u7ea7\u6570\u6c42\u548c\u95ee\u9898(\u5047\u8bbe\u539f\u59cb\u56fe\u50cf\u7684\u5927\u5c0f\u4e3a\\(S\\)): $$ Total = \\sum_{i=0}^{\\inf} \\frac{S}{2^i} = 2S $$</p> <p>\u90a3\u4e48\u6709\u4e86mipmap\u4e4b\u540e\u8be5\u5982\u4f55\u83b7\u5f97\u4e00\u4e2apixel\u7684\u56fe\u6837\u4fe1\u606f\u5462?</p> <p></p> <p>\u6211\u4eec\u627e\u5230\u8be5pixel\u90bb\u8fd1\u7684\u51e0\u4e2apixel, \u5c06\u4ed6\u4eec\u5168\u90e8\u53d8\u6362\u5230texel\u5750\u6807, \u8ba1\u7b97\u5f7c\u6b64\u4e4b\u95f4\u7684\u8ddd\u79bb, \u53d6\u6700\u5927\u503c, \u5c06\u8fd9\u4e2a\u6700\u5927\u503c\u4f5c\u4e3a\u8fb9\u957f, \u6784\u5efa\u4e00\u4e2a\u6b63\u65b9\u5f62. \u7136\u540e\u5bf9\u8fd9\u4e2a\u6b63\u65b9\u5f62\u8fb9\u957f\u53d6\\(log\\), \u5c31\u53ef\u4ee5\u77e5\u9053\u8fd9\u6837\u5927\u5c0f\u7684\u6b63\u65b9\u5f62\u5bf9\u5e94\u7740mipmap\u4e2d\u54ea\u4e00\u5c42, \u800c\u540e\u5728\u8be5\u5c42\u4e0a\u91c7\u6837\u5373\u53ef\u8fd1\u4f3c\u6c42\u5f97\u8fd9\u4e00\u7247texel\u7684\u5e73\u5747\u503c.</p> <p>Trilinear</p> <p> \u5b9e\u9645\u4e0a\u7531\u4e8e\u8ba1\u7b97\u51fa\u6765\u7684\\(D\\)\u5e76\u975e\u662f\u4e00\u4e2a\u6574\u6570, \u6240\u4ee5\u6211\u4eec\u4e5f\u53ef\u4ee5\u5728texel\u4e0a\u7684\u76f8\u90bb\u4e24\u5c42\u5206\u522b\u53cc\u7ebf\u6027\u63d2\u503c, \u7136\u540e\u5728\u4f7f\u7528\\(D\\)\u5c06\u4e24\u5c42\u7684\u7ed3\u679c\u7ebf\u6027\u63d2\u503c\u7684\u5230\u6700\u7ec8\u7684\u7ed3\u679c.</p> <p>Shading Results</p> <p> \u53ef\u4ee5\u770b\u5230, \u5373\u4f7f\u4f7f\u7528\u4e86\u4e09\u7ebf\u6027\u63d2\u503c, \u8fd8\u662f\u4f1a\u51fa\u73b0\u4e00\u4e9b\u7455\u75b5(overblur), \u8fd9\u662f\u56e0\u4e3a\u6211\u4eec\u5728mipmap\u4e2d\u505a\u4e86\u8f83\u591a\u7684\u8fd1\u4f3c, \u5bfc\u81f4\u7ed3\u679c\u4e0d\u597d.  \u6bd4\u5982, \u6211\u4eec\u5047\u8bbe\u4e86\u5e73\u5747\u533a\u57df\u53ea\u80fd\u662f\u6b63\u65b9\u5f62, \u6b63\u65b9\u5f62\u957f\u5ea6\u53d6\u6700\u5927\u503c\u7b49\u7b49.  \u5728\u5b9e\u9645\u7684\u60c5\u51b5\u4e2d, \u4e00\u4e2apixel\u5bf9\u5e94\u7684texel\u533a\u57df\u591a\u79cd\u591a\u6837, \u53ef\u80fd\u662f\u957f\u65b9\u5f62, \u659c\u7740\u7684\u56fe\u5f62\u7b49\u7b49.</p>"},{"location":"Games101/lecture7/#anistotic-filteringripmaps-ewa-filtering","title":"Anistotic Filtering(Ripmaps, EWA Filtering)","text":"<p>\u5f88\u81ea\u7136\u7684\u4e00\u4e2a\u63d0\u5347mipmap\u7684\u60f3\u6cd5\u5c31\u662f\u5c06\u957f\u65b9\u5f62\u533a\u57df\u4e5f\u8003\u8651\u8fdb\u53bb, \u8fd9\u6837\u5c31\u6709\u4e86Ripmaps.</p> <p></p> <p>\u4f46\u662fRipmap\u7684\u5b58\u50a8\u91cf\u8981\u5927\u4e8emipmap, \u53ef\u4ee5\u770b\u5230\u6700\u540e\u5927\u7ea6\u662f\u539f\u59cb\u56fe\u50cf\u5b58\u50a8\u91cf\u7684\u56db\u500d.</p> <p>EWA filtering \u662f\u5229\u7528\u4e00\u7cfb\u5217\u5927\u5c0f\u4e0d\u7b49\u7684\u692d\u5706\u53d6\u8fd1\u4f3c\u56fe\u5f62, \u53ef\u4ee5\u89e3\u51b3\u4e0d\u89c4\u5219\u533a\u57df\u7684\u95ee\u9898, \u7cbe\u5ea6\u66f4\u9ad8, \u4f46\u662f\u663e\u7136\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5b58\u50a8\u7a7a\u95f4\u7684\u9700\u6c42\u66f4\u5927.</p>"},{"location":"Games101/pipeline/","title":"Homework 0 - 3 &amp; Graphics Pipeline(Real-time Rendering)","text":"<p>\u5b66\u4e60\u5b8c\u7b2c\u4e00\u7ae0\u5185\u5bb9\u4e4b\u540e, \u505a\u4e86\u4e00\u4e0bhw0-3, \u5b9e\u73b0\u4e86\u4e00\u4e2a\u975e\u5e38\u57fa\u672c\u7684\u56fe\u5f62\u5b66\u6e32\u67d3pipeline, \u5bf9\u524d\u9762\u5b66\u4e60\u4e2d\u9047\u5230\u7684\u95ee\u9898\u4e5f\u6709\u4e86\u8f83\u4e3a\u6df1\u5165\u7684\u7406\u89e3, \u5728\u8fd9\u91cc\u7ed3\u5408 Graphics Pipeline\u505a\u4e00\u4e2a\u5c0f\u7ed3.</p>"},{"location":"Games101/pipeline/#graphics-pipeline","title":"Graphics Pipeline","text":"<ol> <li> <p>\u9996\u5148, \u6211\u4eec\u5c06\u6211\u4eec\u5e0c\u671b\u8868\u8fbe\u76843D\u7269\u4f53\u8868\u9762\u5206\u5272\u6210\u5f88\u591a\u4e2a\u4e09\u89d2\u5f62, \u8bb2\u8fd9\u4e9b\u4e09\u89d2\u5f62\u7684\u9876\u70b9\u4f5c\u4e3a\u8f93\u5165.</p> </li> <li> <p>\u63a5\u7740, \u6211\u4eec\u4f7f\u7528\u4e4b\u524d\u5b66\u4e60\u8fc7\u7684MVP(Model View Projection)\u53d8\u6362, \u5c06\u8fd9\u4e9b\u9876\u70b9\u4ece\u6a21\u578b\u7a7a\u95f4\u53d8\u6362\u5230\u5c4f\u5e55\u7a7a\u95f4.</p> </li> <li> <p>\u771f\u5b9e\u7684\u4e09\u89d2\u5f62\u662f\u8fde\u7eed\u7684, \u6211\u4eec\u5e0c\u671b\u5c06\u5176\u6e32\u67d3\u5728\u79bb\u6563\u7684\u50cf\u7d20\u5c4f\u5e55\u4e0a, \u56e0\u6b64\u6211\u4eec\u9700\u8981\u8fdb\u884c\u5149\u6805\u5316, \u5c06\u4e09\u89d2\u5f62\u6295\u5f71\u5230\u5c4f\u5e55\u4e0a, \u5e76\u786e\u5b9a\u54ea\u4e9b\u50cf\u7d20\u88ab\u4e09\u89d2\u5f62\u8986\u76d6, \u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528\u5982MSAA\u7684\u624b\u6bb5\u63d0\u5347\u56fe\u50cf\u8d28\u91cf. \u540c\u65f6, \u7a7a\u95f4\u4e2d\u53ef\u80fd\u6709\u591a\u4e2a\u70b9\u4f1a\u6295\u5f71\u5230\u540c\u4e00\u4e2a\u50cf\u7d20\u4e0a, \u6211\u4eec\u9700\u8981\u51b3\u5b9a\u4f7f\u7528\u54ea\u4e2a\u70b9\u7684\u989c\u8272, \u8fd9\u91cc\u5c31\u9700\u8981\u7528\u5230z-buffer.</p> </li> <li> <p>\u63a5\u4e0b\u6765, \u5bf9\u4e8e\u6bcf\u4e2a\u50cf\u7d20, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528Bing-Phong\u7b49\u7740\u8272\u6a21\u578b\u5bf9\u5176\u8fdb\u884c\u7740\u8272, \u5f53\u7136\u4e5f\u53ef\u4ee5\u4f7f\u7528\u73b0\u6210\u7684\u7eb9\u7406\u5bf9\u5176\u7740\u8272.</p> </li> </ol>"},{"location":"Games101/pipeline/#problems-solved","title":"Problems Solved","text":"<ul> <li>\u6211\u4eec\u5047\u8bbe\u76f8\u673a\u653e\u5728\u5750\u6807\u7cfb\u539f\u70b9, \u770b\u5411-z\u65b9\u5411, \u90a3\u4e48\u6211\u4eec\u7684\u573a\u666f\u7a7a\u95f4\u5c31\u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a\u4ee5z\u8f74\u4e3a\u4e2d\u5fc3\u7684\u7a7a\u95f4(\u88abz\u8f74\u4e32\u8d77\u6765), \u5982\u679c\u662f\u900f\u89c6\u6295\u5f71\u7684\u60c5\u5f62, \u573a\u666f\u7684\u5f62\u72b6\u5e94\u5f53\u4e3a\u4e00\u4e2a\u68f1\u53f0, \u5982\u679c\u628a\u5b83\u8865\u5168\u6210\u68f1\u9525\u7684\u8bdd, \u9876\u70b9\u4e3a\u539f\u70b9(\u76f8\u673a\u4f4d\u7f6e). \u800c\u540e\u6211\u4eec\u5c06\u68f1\u53f0\u538b\u7f29\u4e3a\u4e00\u4e2a\u7acb\u65b9\u4f53, \u5728\u5e73\u79fb\u5230\u539f\u70b9, \u653e\u7f29\u5230[-1,1]\u7684\u8303\u56f4, \u8fd9\u6837\u5c31\u5f97\u5230\u4e86\u6211\u4eec\u5c4f\u5e55\u7a7a\u95f4, \u5c4f\u5e55\u7a7a\u95f4\u901a\u8fc7z-buffer\u8fdb\u884c\u6df1\u5ea6\u6d4b\u8bd5, \u786e\u5b9a\u54ea\u4e9b\u50cf\u7d20\u88ab\u8986\u76d6, \u5e76\u8fdb\u884c\u7740\u8272.</li> </ul> <p>\u6240\u4ee5\u5176\u5b9e\u5f53\u76f8\u673a\u4f4d\u7f6e\u6446\u597d\u4e4b\u540e, \u76f8\u673a\u4e2d\u5fc3, \u8fd1\u5e73\u9762\u4e2d\u5fc3, \u8fdc\u5e73\u9762\u4e2d\u5fc3\u90fd\u5728z\u8f74\u4e0a.</p> <ul> <li>\u8001\u5e08\u4e0a\u8bfe\u65f6\u5019\u63a8\u5bfc\u6574\u5957Pipeline\u65f6, z\u65b9\u5411\u7684\u7b26\u53f7\u6709\u70b9\u6df7\u4e71, \u5728\u8fd9\u91cc\u660e\u786e\u4e00\u4e0b.</li> </ul> <p>\u4e00\u5f00\u59cb\u7684\u76f8\u673a\u662f\u671d\u5411z\u8f74\u8d1f\u65b9\u5411\u7684, \u6240\u4ee5\u4f20\u5165\u7684\u8fd1\u5e73\u9762\u53c2\u6570\\(n\\)\u4ee5\u53ca\u8fdc\u5e73\u9762\u53c2\u6570\\(f\\)\u5e94\u5f53\u4fdd\u8bc1\u662f\u8d1f\u6570, \u4ee3\u7801\u6846\u67b6\u4e2d\u4e3a\u6b63\u6570, \u9700\u8981\u53d6\u76f8\u53cd\u6570. \u7ecf\u8fc7\u900f\u89c6\u6295\u5f71\u4e4b\u540e, \u9700\u8981\u8fdb\u884cz-buffer, \u6b64\u65f6z\u503c\u8d8a\u5927\u5bf9\u5e94\u6df1\u5ea6\u8d8a\u5927, \u90a3\u4e48z\u5e94\u5f53\u662f\u6b63\u6570, \u5373\u9700\u8981\u6211\u4eec\u518d\u6b21\u5c06z\u53d6\u76f8\u53cd\u6570, \u5373\u7ed9\u6295\u5f71\u77e9\u9635\u518d\u5de6\u4e58\u4e00\u4e2a\u53d8\u6362\u77e9\u9635\u5373\u53ef:</p> \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] <p>\u6700\u540e, \u6709\u4e00\u7bc7\u5199\u7684\u5f88\u597d\u7684blog, \u57fa\u672ccover\u4e86hw03\u4e2dPipeline\u7684\u6240\u6709\u90e8\u5206.</p>"},{"location":"papers/3DGS/","title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering","text":"<p>Paper</p>"},{"location":"papers/3DGS/#prliminaries","title":"Prliminaries","text":""},{"location":"papers/3DGS/#3d-gaussian","title":"3D Gaussian","text":"<p>Gaussians are defined by a full 3D covariance matrix \\(\\Sigma\\) defined in world space centered at point (mean) \\(\\mu\\).</p> \\[ p(xa, x2, ..., x_n) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}\\cdot e^{-\\frac{1}{2}(xa-\\mu_1)^T\\Sigma^{-1}(xa-\\mu_1)} \\] <p>Gaussian \u4e2d\u7684\u534f\u65b9\u5dee\u77e9\u9635\u662f\u4e00\u4e2a\u6b63\u5b9a\u77e9\u9635, \u4e00\u5b9a\u53ef\u4ee5\u8fdb\u884c\u5bf9\u89d2\u5316, \u539f\u6587\u4e2d\u5c06\u5176\u8868\u793a\u4e3a:</p> \\[ \\Sigma = RSS^TR^T \\] <p>\u5176\u4e2d, \\(R\\)\u662f\u4e00\u4e2a\u65cb\u8f6c\u77e9\u9635, \u901a\u8fc7\u4e00\u4e2a\u56db\u5143\u6570\u8868\u793a, \\(S\\)\u662f\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635, \u7531\u5bf9\u89d2\u7ebf\u4e0a\u7684\u4e09\u4e2a\u53c2\u6570\u51b3\u5b9a, \u6240\u4ee5\u534f\u65b9\u5dee\u77e9\u9635\u5171\u6709\u4e03\u4e2a\u53c2\u6570.</p> <p>\u51e0\u4f55\u4e0a, 3D Gaussian \u5728\u7a7a\u95f4\u4e2d\u5bf9\u5e94\u4e00\u4e2a\u692d\u7403.</p>"},{"location":"papers/3DGS/#spherical-harmonics","title":"Spherical Harmonics","text":"<p>\u7c7b\u6bd4\u4e8e\u5085\u91cc\u53f6\u7ea7\u6570, \u6211\u4eec\u9009\u53d6\u4e00\u7ec4\u7403\u9762\u51fd\u6570\u4f5c\u4e3a\u57fa\u51fd\u6570, \u8bd5\u56fe\u62df\u5408\u4e00\u4e2a\u590d\u6742\u7684\u8868\u9762.</p> <p></p> <p>\u5f53\u6211\u4eec\u5141\u8bb8\u9009\u53d6\u7684\u57fa\u51fd\u6570\u4e2a\u6570\u8d8a\u591a, \u6211\u4eec\u7684\u62df\u5408\u6548\u679c\u5c31\u8d8a\u597d.</p> <p></p> <ul> <li>\u7403\u8c10\u51fd\u6570\u7528\u4e8e\u8868\u8fbe\u7a7a\u95f4\u4e2d\u67d0\u70b9\u7684\u5149\u7167\u6a21\u578b.</li> <li>\u5149\u7167\u51fd\u6570\\(C(\\theta, \\phi)\\)\u53ef\u4ee5\u8868\u793a\u4e3a\u7403\u8c10\u51fd\u6570\u7684\u52a0\u6743\u7ebf\u6027\u7ec4\u5408\uff0c\u5982\u4e0b\uff0c\u67d0\u4e00\u4e2a\u4f4d\u7f6e\u9ad8\u65af\u7403\u7684\u51fd\u6570\uff0c\u8f93\u5165\u4e3a\u89d2\u5ea6\uff0c\u8f93\u51fa\u4e3a\u8fd9\u4e2a\u89d2\u5ea6\u7684\u989c\u8272:</li> </ul> \\[ C(\\theta, \\phi) = \\sum_{l=0}^L \\sum_{m=-l}^l a_{l}^m Y_{l}^m(\\theta, \\phi) \\] <p>\u5176\u4e2d, \\(Y_l^m(\\theta, \\phi)\\)\u4e3a\u7403\u8c10\u51fd\u6570, \\(a_l^m\\)\u4e3a\u7cfb\u6570, \u53c2\u6570\\(l\\)\u548c\\(m\\)\u5206\u522b\u4e3a\u9636\u548c\u6b21. \u9636\u63cf\u8ff0\u4e86\u51fd\u6570\u5728\u7403\u9762\u4e0a\u7684\u6ce2\u7eb9\u6570\u91cf, \u6b21\u4ee3\u8868\u4e86\u5728\u4e00\u5b9a\u9636\u7684\u524d\u63d0\u4e0b\u51fd\u6570\u65b9\u4f4d\u89d2\u7684\u53d8\u5316.</p> <ul> <li> <p>m\u7531J\u51b3\u5b9a\uff0c\u5982J=3, m=-3,-2,-1,0,1,2,3, J =2 m = -2,-1,0,1,2 , \u5bf9\u5e947+5+3+1=16\u4e2a\u7403\u8c10\u51fd\u6570\uff0c\u6240\u4ee5\u670916\u4e2a\u7cfb\u6570a,RGB\u5206\u522b\u5bf9\u5e94\u4e00\u4e2a\u7403\u8c10\u51fd\u6570\u7ebf\u6027\u7ec4\u5408\uff0c\u6240\u4ee5\u5149\u7167\u6a21\u578b\u4e00\u517116*3=48\u4e2a\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u5c31\u662f\u9700\u8981\u4f18\u5316\u7684\u53d8\u91cf.</p> </li> <li> <p>\u8f93\u5165\\((\\theta, \\phi)\\)\uff0c\u786e\u5b9aJ,\u90a3\u4e48\u7403\u8c10\u51fd\u6570\u5c31\u662f\u56fa\u5b9a\u7684\uff0c\u5982\u4e0b\uff0c\u5e26\u5165\\((\\theta, \\phi)\\)\uff0c\u90a3\u4e48\u7403\u8c10\u51fd\u6570\u5c31\u662f\u4e00\u4e2a\u4e2a\u5b9e\u6570.</p> </li> </ul>"},{"location":"papers/3DGS/#perspective-projection","title":"Perspective Projection","text":"<p>\u6211\u4eec\u5c06\u4e16\u754c\u5750\u6807\u8f6c\u6362\u5230\u76f8\u673a\u5750\u6807\\((x_e, y_e, z_e)\\)\u4e4b\u540e, \u8fd8\u9700\u8981\u4f7f\u7528\u900f\u89c6\u6295\u5f71\u5c06\u76f8\u673a\u5750\u6807\u8f6c\u6362\u5230\u5c4f\u5e55\u5750\u6807\\((x_s, y_s, z_s)\\).</p> <p>\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u53d8\u6362\u77e9\u9635\\(M_{\\text{proj}}\\), \u4f5c\u7528\u5728\\((x_e, y_e, z_e)\\)\u4e0a, \u5f97\u5230\u5c4f\u5e55\u5750\u6807. \u5177\u4f53\u63a8\u5230\u8be6\u89c1\u8fd9\u7bc7\u535a\u5ba2.</p> <p>\u56e0\u4e3a\u53d8\u6362\u540e\u7684\u5c4f\u5e55\u5750\u6807\u662f\u4e8c\u7ef4\u7684, \u6240\u4ee5\u53d8\u6362\u540e\u7684\u5750\u6807\\(z_s\\)\u5931\u53bb\u4e86\u610f\u4e49, \u4f46\u662f\u5176\u5b9e\u6211\u4eec\u53ef\u4ee5\u4ee4\\(z_s = \\sqrt{{x_e^2 + y_e^2 + z_e^2}}\\)\u8fd9\u4e00\u5173\u7cfb\u4f7f\u5f97\u5404\u4e2a\u70b9\u5728\u53d8\u6362\u524d\u540e\u6cbfz\u8f74\u7684\u906e\u6321\u5173\u7cfb\u4e0d\u53d8.</p> <p>\u4f46\u662f\u52a0\u4e0a\u8fd9\u4e00\u9650\u5236\u4e4b\u540e\u5750\u6807\u53d8\u6362\u53d8\u6210\u4e86\u975e\u7ebf\u6027\u7684, \u6240\u4ee5\u6211\u4eec\u9700\u8981\u7528Taylor\u5c55\u5f00\u901a\u8fc7\u4e00\u9636\u8fd1\u4f3c\u6765\u62df\u5408\u8fd9\u4e00\u975e\u7ebf\u6027\u53d8\u6362.</p> <p>\u9996\u5148\u5c06\u4e16\u754c\u5750\u6807\u8f6c\u6362\u4e3a\u76f8\u673a\u5750\u6807:</p> \\[ y = p_c = W_w^c p_w + t_w^c \\] <p>\u8fd9\u662f\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362, \u53d8\u6362\u540e\u7684\u7ed3\u679c\u4ecd\u7136\u7b26\u5408\u9ad8\u65af\u5206\u5e03:</p> \\[ \\Sigma_{p_c} = W\\Sigma_{p_w}W^T \\] <p>\u63a5\u4e0b\u6765\u4ece\u76f8\u673a\u5750\u6807\u53d8\u6362\u5230\u50cf\u7d20\u5750\u6807\u7cfb, \u6211\u4eec\u4f7f\u7528\u4e00\u9636\u6cf0\u52d2\u5c55\u5f00\u8fd1\u4f3c, \u4f7f\u5f97\u7ed3\u679c\u4ecd\u4e3aGaussian:</p> \\[ z = F(p_c) \\approx F(\\mu_c) + J(\\mu_c)(p_c - \\mu_c) \\] <p>\u5176\u4e2d\\(F\\)\u4e3a\u975e\u7ebf\u6027\u53d8\u6362, \\(J\\)\u4e3a\\(F\\)\u7684\u96c5\u53ef\u6bd4\u77e9\u9635.</p> <p>\u7ecf\u8fc7\u8fd9\u6837\u7684\u4e24\u6b21\u7ebf\u6027\u53d8\u6362, \u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u5728\u50cf\u7d20\u5750\u6807\u7cfb\u4e0b\u7684\u4e8c\u7ef4Gaussian, \u540c\u65f6\u4fdd\u7559\u4e86\u5176\u8ddd\u79bb\u76f8\u673a\u7684\u6df1\u5ea6\u4fe1\u606f. </p> \\[ \\Sigma_z = J\\Sigma_{p_c}J^T = JW\\Sigma_{p_w}W^TJ^T \\] \\[ \\mu_z = F(\\mu_c) + J(\\mu_c - \\mu_c) = F(\\mu_c) = F(W_{\\mu_{p_w^c}} + t_w^c) \\]"},{"location":"papers/3DGS/#splatting-and-alpha-blending","title":"Splatting and \\(\\alpha\\) Blending","text":"<p>Splatting\u662f\u4e00\u79cd\u5149\u6805\u5316\uff08Rasterize\uff093D\u5bf9\u8c61\u7684\u65b9\u6cd5\uff0c\u5373\u5c063D\u5bf9\u8c61\u6295\u5f71\u52302D\u56fe\u5f62\u3002\u5982\u5c063D\u9ad8\u65af\u7403\uff08\u4e5f\u53ef\u4ee5\u662f\u5176\u4ed6\u56fe\u5f62\uff09\u5f80\u50cf\u7d20\u5e73\u9762\u6254\u96ea\u7403\uff0c\u5728\u50cf\u7d20\u5e73\u9762\u7684\u80fd\u91cf\u4ece\u4e2d\u5fc3\u5411\u5916\u6269\u6563\u5e76\u51cf\u5f31\u3002</p> <p>\u5149\u6805\u5316\u4e4b\u540e\u600e\u4e48\u6df7\u5408\u8fd9\u4e9b\u50cf\u7d20\u5e73\u9762\u7684\u692d\u7403\u5462\uff1f\u4f7f\u7528\u03b1 \\alpha\u03b1 blending\uff0c\u4e3b\u8981\u89e3\u51b3\u56fe\u5c42\u878d\u5408\u7684\u95ee\u9898\u3002 \u4ee5\u4e24\u5e45\u56fe\u50cf\u4e3a\u4f8b\uff0c\u56fe\u50cf\\(I\\)\u7684\u900f\u660e\u5ea6\u4e3a\\(\\alpha_1\\), \u56fe\u50cf\\(I_{bg}\\)\u7684\u900f\u660e\u5ea6\u4e3a\\(\\alpha_2\\), \u6240\u4ee5\u4e8c\u8005\u878d\u5408\u7684\u516c\u5f0f\u5982\u4e0b:</p> \\[ I_{result} = I \\cdot \\alpha_1 + I_{bg} \\cdot (1-\\alpha_2) \\] <p>\u73b0\u5728\u6269\u5c55\u5230\u591a\u5f20\u56fe, \u6211\u4eec\u6709:</p> \\[ C = \\sum_{i=1}^n c_i\\alpha_i \\prod_{j=1, j\\neq i}^n (1-\\alpha_j) \\]"},{"location":"papers/3DGS/#pipeline","title":"Pipeline","text":"<p>\u6574\u4e2a\u573a\u666f\u7684\u8868\u8fbe\u4f9d\u9760\u5f88\u591a\u4e2aGaussian, \u800c\u6bcf\u4e2aGaussian\u90fd\u7531\u4e00\u7cfb\u5217\u53c2\u6570\u63cf\u8ff0, \u5177\u4f53\u800c\u8a00, \u5728\u8fd9\u7bc7\u8bba\u6587\u4e2d\u670959\u4e2a\u53c2\u6570.</p> <p></p> <ul> <li>\u4e2d\u5fc3\\(\\mu\\): 3.</li> <li>\u534f\u65b9\u5dee\u77e9\u9635\\(\\Sigma\\): 7.</li> <li>\u7403\u8c10\u51fd\u6570\u7cfb\u6570\\(a\\): 48.</li> <li>\u900f\u660e\u5ea6\\(\\alpha\\): 1.</li> </ul> <p>\u6bcf\u4e00\u4e2aGaussian\u7684\u8fd9\u4e9b\u53c2\u6570\u4e5f\u6b63\u662f\u6211\u4eec\u901a\u8fc7Ground Truth\u548cBackProp\u8fdb\u884c\u4f18\u5316\u7684\u76ee\u6807.</p>"},{"location":"papers/3DGS/#initialization","title":"Initialization","text":"<p>\u9996\u5148\u4f7f\u7528Struct from Motion\u4ece\u56fe\u50cf\u4e2d\u4f30\u8ba1\u7a7a\u95f4\u70b9\u4f5c\u4e3a\u521d\u59cb\u573a\u666f\u7684\u63cf\u8ff0, \u6bcf\u4e2a\u70b9\u4e91\u4f4d\u7f6e\u653e\u7f6e\u4e00\u4e2aGaussian\u7403, \u4e2d\u5fc3\u70b9\u4f4d\u7f6e\u8bbe\u7f6e\u4e3a\u70b9\u4e91\u4f4d\u7f6e, \u5176\u4ed6\u4fe1\u606f\u968f\u673a\u521d\u59cb\u5316.</p>"},{"location":"papers/3DGS/#projection","title":"Projection","text":"<p>\u6839\u636e\u76f8\u673a\u5185\u5916\u53c2\u6570\u77e9\u9635, \u628aGaussian\u7403Splatting\u5230\u56fe\u50cf\u4e0a. \u5f53\u4e00\u4e2a\u50cf\u7d20\u70b9\u5728Gaussian\u7403\u7684\u6982\u7387\u4e3a99%\u7684\u8303\u56f4\u5185, \u8ba4\u4e3a\u5176\u88ab\u8be5Gaussian\u8986\u76d6. </p>"},{"location":"papers/3DGS/#diferentiable-tile-tasterizer","title":"Diferentiable Tile Tasterizer","text":"<p>\u5728\u6295\u5f71\u91cd\u53e0\u533a\u57df\u8fdb\u884c\u5149\u6805\u5316\u6e32\u67d3\uff08Differentiable Tile Rasterizer\uff09\uff0c\u4f7f\u7528\u03b1 \\alpha\u03b1 blending\uff0c\u8fd9\u662f\u786e\u5b9a\u7684\u51fd\u6570\uff0c\u4e0d\u9700\u8981\u5b66\u4e60\u3002\u628a\u8fd9\u4e9b\u9ad8\u65af\u7403\u8fdb\u884c\u6df7\u5408\uff0c\u8fc7\u7a0b\u53ef\u5fae.</p> <p>\u8fd9\u90e8\u5206\u539f\u6587\u5c06\u6574\u5f20\u56fe\u50cf\u5206\u6210\u4e86\u5f88\u591atile, \u7136\u540e\u53ef\u4ee5\u5e76\u884c\u5904\u7406.</p>"},{"location":"papers/3DGS/#optimization","title":"Optimization","text":"<p>\u635f\u5931\u51fd\u6570\u4e3a:</p> \\[ L = (1-\\lambda)L_1 + \\lambda L_{D-SSIM} \\] <p>\u5176\u4e2d:</p> <ul> <li>L1 Loss: \u9010\u4e2a\u50cf\u7d20\u6bd4\u8f83\u5dee\u5f02</li> </ul> \\[ L_1 = \\frac{1}{N}\\sum_{i=1}^N |I_i - \\hat{I_i}| \\] <ul> <li>SSIM loss\uff08\u7ed3\u6784\u76f8\u4f3c\uff09\u635f\u5931\u51fd\u6570\uff1a\u8003\u8651\u4e86\u4eae\u5ea6 (luminance)\u3001\u5bf9\u6bd4\u5ea6 (contrast) \u548c\u7ed3\u6784 (structure)\u6307\u6807\uff0c\u8fd9\u5c31\u8003\u8651\u4e86\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\uff0c\u4e00\u822c\u800c\u8a00\uff0cSSIM\u5f97\u5230\u7684\u7ed3\u679c\u4f1a\u6bd4L1\uff0cL2\u7684\u7ed3\u679c\u66f4\u6709\u7ec6\u8282\uff0cSSIM \u7684\u53d6\u503c\u8303\u56f4\u4e3a -1 \u5230 1\uff0c1 \u8868\u793a\u4e24\u5e45\u56fe\u50cf\u5b8c\u5168\u4e00\u6837\uff0c-1 \u8868\u793a\u4e24\u5e45\u56fe\u50cf\u5dee\u5f02\u6700\u5927.</li> </ul> <p>\u6bcf\u6b21\u6c42\u51faLoss\u4e4b\u540e, \u6211\u4eec\u4f7f\u7528BackProp\u6c42\u51fa\u5bf9\u4e8e\u5404\u4e2aGaussian\u7403\u53c2\u6570\u7684\u68af\u5ea6, \u7136\u540eGradient Descent\u66f4\u65b0\u53c2\u6570.</p>"},{"location":"papers/3DGS/#adaptive-density-control","title":"Adaptive Density Control","text":"<p>\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u8f83\u5927\u68af\u5ea6\uff0859\u7ef4\u5bfc\u6570\uff0c\u6a21\u957f\u5927\uff09\u7684\u9ad8\u65af\u7403\u5b58\u5728under-reconstruction\u548cover-reconstruction\u95ee\u9898:</p> <p></p> <ul> <li>under-reconstruction\u533a\u57df\u7684\u9ad8\u65af\u7403\u65b9\u5dee\u5c0f\uff0c\u8fdb\u884cclone</li> <li>over-reconstruction\u533a\u57df\u9ad8\u65af\u7403\u65b9\u5dee\u5927\uff0c\u8fdb\u884csplit</li> <li>\u6bcf\u7ecf\u8fc7\u56fa\u5b9a\u6b21\u6570\u7684\u8fed\u4ee3\u8fdb\u884c\u4e00\u6b21\u5254\u9664\u64cd\u4f5c\uff0c\u5254\u9664\u51e0\u4e4e\u900f\u660e\uff08\u900f\u660e\u5ea6\u63a5\u8fd10\uff09\u7684\u9ad8\u65af\u7403\u4ee5\u53ca\u65b9\u5dee\u8fc7\u5927\u7684\u9ad8\u65af\u7403</li> </ul>"}]}